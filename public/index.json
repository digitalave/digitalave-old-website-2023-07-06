



















































































































































































































































































































































































































































































































[{"categories":["DevOps"],"contents":"Introduction This tutorial is intended to demonstrate how to setup your 1st Kubernetes cluster on Azure Kubernetes Services (AKS). This tutorial will cover up all the steps that you need to setup complete AKS cluster. You can continue alone with me without having major knowledge of Docker, Kubernetes or Azure AKS.\nAs you already may know, Kubernetes provides a distributed infrastructure for containerized application on the Azure Cloud.\nAzure Kubernetes Service (AKS) is a Managed service which provides by Microsoft Azure. Microsoft taking care of the lot\u0026rsquo;s of things such high availability, scalability, monitoring and maintenance. By using with Azure Kubernetes Services, you can deploy production ready highly available Kubernetes cluster in few minutes.\nDeploy Production Grade Kubernetes Cluster on Azure AKS (Zero to Hero Guide) Learning Goals\u0026hellip; Create an Azure Kubernetes Service (AKS)\nCreate an Azure Container Registry (ACR)\nAttach ACR with AKS\nIntegrate AKS with Azure Active Directory (AAD) (Managing AKS Cluster Using Azure AD Users/Groups)\nConfigure AKS Cluster for future Windows Node Pools\nBefore You Begin !!! I created some commands composed using windows Powershell on Visual Studio Code IDE. Therefore, Some variable assignment and next line syntax types may not be compatible on Linux bash terminals. But, You can change them very easily.\nFor Example:\nVariable assignment Powershell variable Assignment: $AKS_RESOURCE_GROUP = \u0026#34;DigitalAvenue-RSG\u0026#34; Linux Variable Assignment:\nAKS_RESOURCE_GROUP=\u0026#34;DigitalAvenue-RSG\u0026#34; Next line statement. If you are using a Linux terminal, Please replace Powershell \u0026ldquo;`\u0026rdquo; next line syntax with \u0026ldquo;\u0026quot; Linux next line syntax.\nFor ex:\nPowershell Command:\naz ad group show ` --group AKS-Admins ` --query objectId ` --output tsv Linux Bash Command:\naz ad group show \\ --group AKS-Admins \\ --query objectId \\ --output tsv I\u0026rsquo;m pretty sure you\u0026rsquo;ve got the point. okay let\u0026rsquo;s jump !!!\nDeployment Workflow Installing prerequisites tools\n(Docker Desktop, Azure CLI, KubeCTL)\nLogin to Azure / Set Subscription\nCreate Resource Group\nAzure AD User/Group Integration with AKS\nThis allows you to define who can have access to the Azure AKS\nREF: https://docs.microsoft.com/en-us/azure/aks/azure-ad-rbac Create a SSH Key\nCreate Azure Container Registry (ACR)\nCreate AKS Cluster with System(Linux) Node Pool\nDeployment Prerequisites: Azure Subscription ($200 free credit) As the initial requirement, you should have a Microsoft Azure Account. You can get a free Azure account with $200 worth credits for one year. REF: https://azure.microsoft.com/en-us/free/ Docker Desktop (Windows/Linux/Mac) Installing Docker in local machines a primary requirement for Kubernetes testing and deployments.\nREF: https://www.docker.com/products/docker-desktop Azure CLI Then, you need to install Azure CLI on your local machine. Azure CLI is available to install in Windows, Mac and Linux environments.\nREF: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli Kubectl Kubectl is the Kubernetes command line tool which allows us to execute commands against the Kubernetes cluster.\nREF: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ Before we continuing the AKS Cluster deployment, We need to complete the following prerequisites.\nSTEP 01: Install Prerequisites Tools As the 1st step I need to install following tools in order to continue the deployment process. So, I\u0026rsquo;m quickly go through with the required packages installation.\nIf they are already available on your computer, then directly jump into the STEP-02\nInstall Docker Desktop\nYou can simply download and install docker Docker Desktop from here.\nInstall Azure CLI\nThere are several ways to install Azure CLI on your computer. I\u0026rsquo;m this I\u0026rsquo;m going to install Azure CLI package using this powershell command.\nOpen up you Windows powershell as an administrator and then run the command below.\nInvoke-WebRequest -Uri https://aka.ms/installazurecliwindows -OutFile .\\AzureCLI.msi; Start-Process msiexec.exe -Wait -ArgumentList \u0026#39;/I AzureCLI.msi /quiet\u0026#39;; rm .\\AzureCLI.msi Or else, you also can install Windows MSI binary using this link.\nRef: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli Install KubeCTL There are many ways to install KubeCTL on windows. Either you can install using \u0026ldquo;curl\u0026rdquo; or using \u0026ldquo;Chocolatey\u0026rdquo; package manager.\nOption 01: Install Using Curl:\nMake sure whether the \u0026ldquo;curl\u0026rdquo; package available on your computer.\ncurl -LO https://dl.k8s.io/release/v1.21.0/bin/windows/amd64/kubectl.exe Once you downloaded, move \u0026ldquo;kubectl.exe\u0026rdquo; binary into somewhere your programs resides. And make sure to set the PATH environment variable for \u0026ldquo;kubectl.exe\u0026rdquo;\nOption 02: Installing Using Chocolaty Package Manger:\nOpen up Powershell as an administrator and simply execute the following commands.\nInstall Chocolaty:\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) choco install kubernetes-cli kubectl version --client mkidr ~/.kube cd ~/.kube New-Item config -type file Ref: https://chocolatey.org/install STEP 02: Login and Set Subscription 2.1 Login to Azure CLI Before moving further, we need to authenticate with Azure using the following command. 1st and foremost we need to login into our Azure subscription. Hit the following commend to login into Azure account.\naz login After entering the above command, you web browser will open and ask for relevant credentials for Azure Portal.\n2.2 Activate the correct subscription. Azure uses the concept of subscriptions serve as a single billing using for azure resources and services among different environments such as dev, staging and prod. If you have more than one subscription with you account, You can run the following command to list all the subscriptions associated with your Azure account and choose one for your deployment.\nList Subscriptions:\naz account list --refresh --output table Set Subscription:\nPick the subscription you want to use for creating the cluster, and set that as your default. If you only have one subscription you can ignore this step.\naz account set -s $SUBSCRIPTION_ID 3. Create a Resource Group 3.1 Choose location Cluster has to be resided And then you need to define where you going to deploy your AKS cluster. For that, you can choose a Azure data-center which is close to you. This command list all the data-centers with their location.\naz account list-locations --query \u0026#34;sort_by([].{DisplayName:displayName, Name:name}, \u0026amp;DisplayName)\u0026#34; --output table 3.2 Create the Resource group Further moving one, We need to create a separate resource group. This is where all the cluster resources going to create. Azure Resource Groups are logical collection, which can holds related Azure services and resources almost anything in Azure. This resource groups allows you to manage every relevant resources in a separate environment.\n\u0026ndash;name = Resource Group Name\n\u0026ndash;location = Azure Data-Center Location\nOkay, Let\u0026rsquo;s create a resource group. And keep in mind to create all the AKS cluster specific resources within this resource group in same region/location.\naz group create --name $AKS_RESOURCE_GROUP --location $AKS_REGION STEP 04: Create SSH Key In some cases, You may need to login into an AKS Node for the maintenance activities such as logs collections or even troubleshooting activities. If something happens in future we might require SSH credentials.\nOkay, Simply hit the following commands to create a SSH Key.\nmkdir $HOME/.ssh/aks-prod-sshkeys/ ssh-keygen -t rsa -C \u0026#34;HOSTNAME\u0026#34; -f \u0026#34;$HOME/.ssh/aks-prod-sshkeys/aksprod-id_rsa\u0026#34; Set SSH Key path into a environment variable.\n$AKS_SSH_KEY_LOCATION=\u0026#34;$HOME/.ssh/aks-prod-sshkeys/aksprod-id_rsa.pub\u0026#34; STEP 05: Integrate Azure AD with AKS (AKS-managed Azure Active Directory integration) There are numerous ways to secure your Azure Kubernetes Cluster. We need to thoroughly consider about the AKS security, since we are going to deploy a production ready cluster. In that sense, I\u0026rsquo;m going to integrate Azure Active Directory with Azure Kubernetes Services (AKS). Therefore we can authenticate, control access and authorize from Azure AD to AKS Cluster securely.\nAKS can be configured to use Azure AD for user authentication, We can can use Kubernetes Role-Based Access Control (RBAC) to limit access to the cluster resources.\nLimitations AKS-managed Azure AD integration can\u0026rsquo;t be disabled non-Kubernetes RBAC enabled clusters aren\u0026rsquo;t supported for AKS-managed Azure AD integration Changing the Azure AD tenant associated with AKS-managed Azure AD integration isn\u0026rsquo;t supported Changing a AKS-managed Azure AD integrated cluster to legacy AAD is not supported 5.1 Create Azure AD Group Now, I\u0026rsquo;m going to create a AKS cluster administrator group.\n$AD_AKS_ADMIN_GROUP = \u0026ldquo;aksadmins\u0026rdquo;\nFollowing command creates a group in Azure AD for the AKS cluster administrators. And assigned the group id into a variable.\naz ad group create --display-name AKS-Admins --mail-nickname $AKS_ADMIN_GROUP $AD_AKS_ADMIN_GROUP_ID=$(az ad group show --group AKS-Admins --query objectId --output tsv) Or else, you can run this single command to create admin group and get the objectID for the group at once.\n$AD_AKS_ADMIN_GROUP_ID=$(az ad group create --display-name AKS-Admins --mail-nickname $AKS_ADMIN_GROUP --query objectId -o tsv) 5.2 Create Azure AD AKS Admin User Then, I\u0026rsquo;m going to create \u0026ldquo;aksadmin1\u0026rdquo; user and assign user ID into the variable\naz ad user create --display-name \u0026#34;AKSAdmin1\u0026#34; --password \u0026#34;DAvenue@12345\u0026#34; --user-principal-name aksadmin1@DigitalAvenue105.onmicrosoft.com $AD_AKS_ADMIN1_USER_OBJECT_ID=$(az ad user show --id aksadmin1@DigitalAvenue105.onmicrosoft.com --query objectId --output tsv) Or else, you can run this single command to create admin user and get the objectID for the user at once.\n$AD_AKS_ADMIN1_USER_OBJECT_ID=$(az ad user create ` --display-name \u0026#34;AKSAdmin1\u0026#34; ` --user-principal-name aksadmin1@DigitalAvenue105.onmicrosoft.com ` --password \u0026#34;DAvenue@12345\u0026#34; ` --query objectId -o tsv) NOTE: Please make sure to replace \u0026ldquo;aksadmin1@DigitalAvenue105.onmicrosoft.com \u0026rdquo; with your AD Domain with in \u0026ldquo;user-principal-name\u0026rdquo;\n5.3 Associate \u0026ldquo;AKS-Admin\u0026rdquo; User to \u0026ldquo;AKS-Admins\u0026rdquo; Group az ad group member add --group AKS-Admin --member-id $AD_AKS_ADMIN1_USER_OBJECT_ID Note: Make sure to note down your \u0026ldquo;username\u0026rdquo; and \u0026ldquo;password\u0026rdquo;.\nUserName : aksdmin1@DigitalAvenue105.onmicrosoft.com Password : DAvenue@12345\nSTEP 06: Get Azure AD Tenant ID and Set Windows Username Password In future, You may need to run Windows based containers on your Kubernetes cluster. But, Generally, Windows containers (such as Windows Server 2019) cannot be run on Linux Nodes. So, Over the time you can add a windows node pool into your AKS cluster. So, I\u0026rsquo;m going getting ready for that as well.\n6.1 Get Azure AD Tenant ID You find your tenant ID either using Azure Portal or Azure CLI.\n$AZURE_DEFAULT_AD_TENANTID=$(az account show --query tenantId --output tsv) Or, Head-over to Azure Portal and go to Services -\u0026gt; Azure Active Directory -\u0026gt; Properties -\u0026gt; Tenant ID\n6.2 Set Username and Password for Windows Node $AKS_WINDOWS_NODE_USERNAME= \u0026ldquo;aksuser\u0026rdquo; $AKS_WINDOWS_NODE_PASSWORD= \u0026ldquo;DAvenue@123456\u0026rdquo; # Required length: [14 -123]\nSTEP 07: Create Azure Container Registry (ACR) Then, We need to create an Azure Container Registry (ACR). ACR is a private container registry which allows you to store and manage container images securely.\nNote: The name of the ACR must be globally unique. choose something not exist already. Run the following line to create an Azure Container Registry if you do not already have one\nCheck if your desired name is available\n$ACR_NAME = \u0026ldquo;DAvenueACR\u0026rdquo;\naz acr check-name -n $ACR_NAME az acr create --resource-group $AKS_RESOURCE_GROUP --name $ACR_NAME --sku Standard --location $AKS_REGION STEP 08: Create AKS Cluster With System/Linux Node Pool Now, I\u0026rsquo;m going to deploy AKS cluster with following parameters and their variables. While you are doing, make sure to assign correct parameter as I\u0026rsquo;ve done here.\nRun below command to create AKS cluster.\nHere we\u0026rsquo;ll pass following variables.\n$SUBSCRIPTION_ID = \u0026#34;9e327125-90eb-4bb9-9f0f-069d133bed59\u0026#34; $AKS_RESOURCE_GROUP = \u0026#34;DigitalAvenue-RSG\u0026#34; $AKS_CLUSTER_NAME = \u0026#34;DAvenueAKS-Cluster\u0026#34; $AKS_REGION = \u0026#34;southeastasia\u0026#34; $AKS_SSH_KEY_LOCATION=\u0026#34;$HOME/.ssh/aks-prod-sshkeys/aksprod-id_rsa.pub\u0026#34; $AKS_ADMIN_GROUP = \u0026#34;aksadmins\u0026#34; $AKS_ADMIN_USER1 = \u0026#34;aksadmin1\u0026#34; $AD_AKS_ADMIN_GROUP_ID=$(az ad group show --group AKS-Admins --query objectId --output tsv) $AD_AKS_ADMIN1_USER_OBJECT_ID=$(az ad user show --id aksadmin1@DigitalAvenue105.onmicrosoft.com --query objectId --output tsv) $AZURE_DEFAULT_AD_TENANTID=$(az account show --query tenantId --output tsv) $ACR_NAME = \u0026#34;DAvenueACR\u0026#34; $AKS_WINDOWS_NODE_USERNAME= \u0026#34;aksuser\u0026#34; $AKS_WINDOWS_NODE_PASSWORD= \u0026#34;DAvenue@123456\u0026#34; # Required length: [14 -123] $LinuxNodeVMSize = \u0026#39;Standard_B2s\u0026#39; Let\u0026rsquo;s create our AKS cluster with Azure AD enabled.\naz aks create --resource-group ${AKS_RESOURCE_GROUP} ` --name ${AKS_CLUSTER_NAME} ` --enable-managed-identity ` --ssh-key-value ${AKS_SSH_KEY_LOCATION} ` --admin-username aksnodeadmin ` --node-count 1 ` --enable-cluster-autoscaler ` --min-count 1 ` --max-count 2 ` --network-plugin azure ` --enable-azure-rbac ` --enable-aad ` --aad-admin-group-object-ids ${AD_AKS_ADMIN_GROUP_ID} ` --aad-tenant-id ${AZURE_DEFAULT_AD_TENANTID} ` --windows-admin-password ${AKS_WINDOWS_NODE_PASSWORD} ` --windows-admin-username ${AKS_WINDOWS_NODE_USERNAME} ` --node-osdisk-size 30 ` --attach-acr ${ACR_NAME} ` --node-vm-size ${LinuxNodeVMSize} ` --nodepool-labels nodepool-type=system nodepoolos=linux app=davenue-apps ` --nodepool-name systempool ` --nodepool-tags nodepool-type=system nodepoolos=linux app=davenue-apps ` --enable-ahub ` --location ${AKS_REGION} \u0026ndash;enable-aad = AKS cluster with managed Azure AD integration - enable administrative access to Azure AD Group \u0026ndash;enable-azure-rbac = Azure RBAC for Kubernetes Authorization\nIMG1\nAKS cluster creation will take up-to 5 - 10 minutes. Wait until it completed. Once you have completed all the above step, you can see numerous resources had been created on your AKS resource group.\nIMG2\nIMG8\nSTEP 09: Create AKS Role Assignment To Azure AD User / Group In this step, I will assign \u0026ldquo;AKS Cluster Admin Role\u0026rdquo; into previously created AKS-Admins group. (STEP 3.3)\nThere are two build-in Azure RBAC roles available for AKS.\nAKS Cluster Admin Role AKS Cluster User Role Refer fore more info: https://docs.microsoft.com/en-us/azure/aks/control-kubeconfig-access#available-cluster-roles-permissions In this case I\u0026rsquo;m allowing super-user access to \u0026ldquo;AKS-Admins\u0026rdquo; group. Therefore, Users who are in the \u0026ldquo;AKS-Admins\u0026rdquo; group will gain full control over every resource in the cluster and in all namespaces You can change it according your requirement. And this Roles assignments scoped to the entire AKS cluster.\nWe need to have Azure AD group ID and AKS Cluster ID, In order to assign this cluster admin role to the Azure AD group.\nAlready We got the AKS-Admins group object ID in the step 3.1.\n$AD_AKS_ADMIN_GROUP_ID=$(az ad group show --group AKS-Admins --query objectId --output tsv) Get your AKS Resource ID $AKS_ID=$(az aks show -g $AKS_RESOURCE_GROUP -n ${AKS_CLUSTER_NAME} --query id -o tsv) Here you can assign Cluster Admin Role to your user or whole group.\nAssign the \u0026lsquo;Cluster Admin\u0026rsquo; role to the User az role assignment create ` --assignee $ACCOUNT_ID ` --scope $AKS_ID ` --role \u0026#34;Azure Kubernetes Service RBAC Admin\u0026#34; Or else,\nAssign the \u0026lsquo;Cluster Admin\u0026rsquo; role to the User az role assignment create --role \u0026#34;Azure Kubernetes Service RBAC Admin\u0026#34; ` --assignee-object-id ${AD_AKS_ADMIN_GROUP_ID} ` --assignee-principal-type Group ` --scope $AKS_ID \u0026ndash;assignee-principal-type = {Group, ServicePrincipal, User}\nIMG9\nSTEP 10: Configure Credentials and Access Azure AD Enabled AKS Cluster In order to authenticate with your AKS cluster, You need to download the kubeconfig file into your local machine.\naz aks get-credentials --resource-group $AKS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME --admin IMG3\nSTEP 11: Verification Now, The cluster creation is almost completed. But, We can verify it using following commands.\nAs the first step, I\u0026rsquo;m going to check whether out Azure AD authentication is working. Simply, hit the following command on your terminal\nIMG4\nGet AKS Nodes kubectl get nodes It will ask you to verify your account activity with device login access token. Please follow the command output and open the URL and paste your TOKEN. Output will look like this once you authenticated successfully.\nPS C:\\Users\\dimui\\deployments\u0026gt; kubectl get nodes To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code SU3BKRKYT to authenticate. Error from server (Forbidden): nodes is forbidden: User \u0026#34;f8b99861-f202-448e-9cb9-d147630c2786\u0026#34; cannot list resource \u0026#34;nodes\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope Cluster Info kubectl cluster-info Now your production-ready AKS Cluster is ready to deploy Kubernetes applications. I will cover-up following areas in future tutorials.\nWriting Kubernetes Manifest Step by Step Azure DNS Zone configuration for AKS Kubernetes External DNS and Ingress Configuration Ingress SSL configuration with Let\u0026rsquo;s Encrypt. A Final Thought !!! I hope you learn something new today. If you feel this tutorial worthwhile, Please visit my YouTube channel and hit the Subscribe and keep me posted.\nGood Luck \u0026amp; Stay Safe !!!\n","date":"July 25, 2021","image":null,"permalink":"/blog/2021-07-25-deploy-aks-kubernetes-cluster-on-azure-cloud/","title":"Deploy Production Grade Kubernetes Cluster on Azure AKS"},{"categories":["DevOps"],"contents":"Getting Started With Docker - Quick Start Guide Docker Engine Platform as a Service (PaaS) Cloud platform service. Allows you to manage its application and data. Cloud service provider manages servers, storage, networking, and computing power. Provides OS level virtualization platform The OS kernel allows multiple isolated instances. Components of the Docker Engine Docker Daemon: Brain of the Docker Persistent background process ( such as our normal system service, which runs continuously.) Process named “dockerd” listens for Docker API requests and manages Docker objects. (create docker images, run docker containers, route traffic among Docker containers. Manages Docker Objects. Docker images Docker Containers Docker Networking Docker Volumes Docker Engine REST API: Docker API used to keep interaction with Docker Daemon. Communication performed using HTTP requests. Application Programming Interface (API) This API communicate using UNIX Socket (unix:///var/run/docker.sock) Docker CLI Command Line Interface. This user interface allows the user to manage Docker objects and communicate to Docker daemon. Docker Engine - Birds Eye View Docker Architecture Docker architecture consist of 3 main components Docker Client Docker Host Docker Registry Docker Client - This is where we are doing stuff Enables us to interact with Docker daemon Docker Clients typically reside on the same host where Docker daemon is running. But in some cases, it can run on a remote host as well. Docker client provides us to command line interface to issue command to Docker daemon (docker build, docker run.. ) Docker Host Provides a complete infrastructure to execute and run applications Main control center Docker daemon pulls images from docker registry and build container images and run application as we requested by Docker CLI/ Docker Client Docker Objects There are numerous objects available. One or more objects required to run an application in a container.\nImages Containers Networking Storage 3.1. Docker Images Each of the files in a Docker image consist it’s own layer. Use to build containers Use to store and ship containers Read only binaries snapshot or template Consist of File system and required libraries to run a specific application environment Shareable using public / private registry Consists a series of layers and each command composed to a read-only layer. 3.2. Docker Containers Encapsulated environments which can run applications Running instance of a Docker image 3.3. Docker Networking Bridge network Overlay network Macvlan network Null 3.4. Docker Storage Data Volume Support persistent data storage. (data will not loss even after the container restart/delete) Data volumes resides on the host file system Container data can accessible from host-file system Scenarios - Running state full applications Data Volume Container This volume type can share among multiple containers This is independent from application container - shared volumes Scenarios - sharing same configuration among multiple containers Directory Mount Directly mount the host\u0026rsquo;s local directory into a container. Any directory on the host can be mounted as a source for the container volumes. Scenarios - mounting hosts .ssh folder, .bash_profile file, .kubectl folder into container to share the same host configuration to the container. Storage Plugins Allows to connect other storage types such as data LUNs, SAN storages, NFS, SAMBA There are number of storage plugins available Azure File Storage, EMC Docker Registry This is where docker images can store There may be private and public registries Creating a Dockerfile Command Usage FROM To specify the parent image. WORKDIR To set the working directory to execute the command within the directory RUN To install packages that are need to run specific application COPY To copy over files or directories from a source to destination ADD Same as COPY command. But, also able to handle remote URLs and unpack compressed files. ENTRYPOINT Command that will always be executed when the container starts. If not specified, the default is /bin/sh -c CMD Arguments passed to the entrypoint. If ENTRYPOINT is not set (defaults to /bin/sh -c), the CMD will be the commands the container executes. EXPOSE To define which port through which to access your container application. Sample Dockerfile\n#Download base image ubuntu 20.04 FROM ubuntu:latest USER root # labels about the custom image LABEL maintainer=\u0026#34;NGINX Sample WebServer\u0026#34; LABEL maintainer=\u0026#34;dimuit86@gmail.com\u0026#34; LABEL description=\u0026#34;This is custom Docker Image for \\ DigitalAvenue DevOps.\u0026#34; # Update the image to the latest packages \\ # Install Nginx RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install nginx -y \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* \u0026amp;\u0026amp; \\ apt clean RUN rm -rf /var/www/html/*.html COPY ./index.html /var/www/html/index.html # Volume configuration VOLUME [\u0026#34;/var/log/nginx\u0026#34;, \u0026#34;/var/www/html\u0026#34;] # Expose Port for the Application EXPOSE 80 443 # Last is the actual command to start up NGINX within our Container CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] Dockerfile Definitions FROM : parent image/ base image use to create a container with a specific application LABEL : Define a description, ownership, authority RUN : Specific command to make changes to base image. Install new applications, packages, libraries Adding new users Changing file permissions and ownership USER : Defines the username that all the commands will be run as the mentioned user. WORKDIR : Defines default working directory where all the commands are going to be executed. WORKDIR comes before “ENTRYPOINT” OR “CMD” CMD : This command runs when the container starts ENTRYPOINT : This is where the default application is going to start when the container is created. Practical 01: Containerize NGINX Web Application In this demonstration, we are going to spin up a simple Nginx website using Docker Ubuntu base image. And we’ll copy custom “nginx.conf” configuration files and “index.html” files into the docker container.\nSTEP 01: Create a “index.html” \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Hello, Nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, Digital Avenue!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;We have just configured our Nginx web server on Ubuntu Server!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; STEP 02: Create Custom “nginx.conf” file user www-data; worker_processes auto; pid /run/nginx.pid; include /etc/nginx/modules-enabled/*.conf; events { worker_connections 768; } http { sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; gzip on; include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } STEP 03: Create a Dockerfile #Download base image ubuntu 20.04 FROM ubuntu:latest # labels about the custom image LABEL maintainer=\u0026#34;NGINX Sample WebServer\u0026#34; LABEL maintainer=\u0026#34;dimuit86@gmail.com\u0026#34; LABEL description=\u0026#34;This is custom Docker Image for \\ DigitalAvenue DevOps.\u0026#34; # Update the image to the latest packages \\ # Install Nginx RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install nginx -y \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* \u0026amp;\u0026amp; \\ apt clean RUN rm -rf /etc/nginx/nginx.conf RUN rm -rf /var/www/html/*.html COPY ./nginx.conf /etc/nginx/nginx.conf COPY ./index.html /var/www/html/index.html # Volume configuration VOLUME [\u0026#34;/var/log/nginx\u0026#34;, \u0026#34;/var/www/html\u0026#34;] # Expose Port for the Application EXPOSE 80 443 # Last is the actual command to start up NGINX within our Container CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] docker build -t nginx-web:v1 .\ndocker run -d -p 8080:80 \u0026ndash;name nginx-web nginx-web:v1\nhttp://localhost:8080\nBuild a Docker Image:\ndocker build -t : .\nRun Docker Container\ndocker run -d -p 8080:80 \u0026ndash;name :\nInspect Docker Container\ndocker container ls\ndocker container \u0026lt;CONTAINER-NAME/ID\u0026gt;\nLog into Docker Container\ndocker run -it \u0026lt;CONTAINER-NAME/ID\u0026gt; Accessing Docker Volumes:\nDocker Volume Location:\nLinux : /var/lib/docker/volumes/\nWindows WSL: Windows Subsystem for Linux (WSL2)\nCtrl + R = “\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes”\nPractical 02: Containerized Node Application - Multi Stage Docker Images # ================================================================ # B U I L D E R S T A G E # ================================================================ FROM node:14-alpine AS builder LABEL Name=\u0026#34;Node.js Demo App - MULTI STAGE\u0026#34; LABEL maintainer=\u0026#34;dimuit86@gmail.com\u0026#34; LABEL description=\u0026#34;This is custom Docker Image for \\ DigitalAvenue DevOps.\u0026#34; ENV NODE_ENV production # Create app directory WORKDIR /usr/src/app WORKDIR /app # Copy package.json and .lock file # from host to docker container COPY package.json package-lock.json ./ # Installing dependencies # RUN npm install # For Production usage RUN npm ci --only=production # Bundle app source COPY . . # ================================================================ # D E P L O Y S T A G E # ================================================================ FROM nginx:1.19-alpine AS server # Copy custom nginx.con file COPY ./nginx.conf /etc/nginx/conf.d/default.conf # Copy build output from build stage COPY --from=builder /usr/src/app /usr/share/nginx/html # Open port 80 EXPOSE 80 # Start nginx from backgrood CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] docker build -t nodejs-web:v1 .\n","date":"July 14, 2021","image":"https://digitalavenue.dev/img/post-imgs/docker-getting-started/docker_hue0eede4efebdbcdf908708cf1d9e806b_244602_650x0_resize_q90_box.jpg","permalink":"/blog/2021-07-14-getting-started-with-docker/","title":"Getting Started With Docker - Quick Start Guide "},{"categories":["DevOps"],"contents":"Prerequisites: Azure CLI\nhttps://docs.microsoft.com/en-us/cli/azure/install-azure-cli 1. Run the Azure CLI with the az command. 1.1 Run the login command. az login Login in the browser with the azure account.\n2. Activate the correct subscription. Azure uses the concept of subscriptions to manage spending. You can get a list of subscriptions your account has access to by running:\naz account list --refresh --output table Choose correct subscription (If it has more than one)\nSubscriptionId : cfg55fb3-ck7f-4342-6a1a-f934d2254ca7\nName : DigitalAvenue azure subscription\n2.1 Select Subscription Pick the subscription you want to use for creating the cluster, and set that as your default. If you only have one subscription you can ignore this step.\naz account set -s \u0026lt;YOUR-CHOSEN-SUBSCRIPTION-NAME\u0026gt; az account set -s cfg55fb3-ck7f-4342-6a1a-f934d2254ca7 3. Create a Resource Group 3.1 Choose location Cluster has to be resided az account list-locations --query \u0026#39;[].name\u0026#39; OR\naz account list-locations --query \u0026#34;sort_by([].{DisplayName:displayName, Name:name}, \u0026amp;DisplayName)\u0026#34; --output table List location from above command\n3.2 Create the Resource group az group create --name DigitalAvenueRSG --location uksouth For eg:\naz group create --name \u0026lt;RESOURCE_GROUP_NAME\u0026gt; --location \u0026lt;AZ Location\u0026gt; 4. Create AKS cluster 4.1 Generate ssh key pair ssh-keygen -f ssh-key-\u0026lt;CLUSTER-NAME\u0026gt; For eg:\nssh-keygen -f ssh-key-DigitalAvenue 4.2 Download and install kubectl and kubelogin az aks install-cli Note: Follow the guidelines with the command output\n4.3 Deploy K8S Cluster on AKS Choosing VM Type:\nReferences:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/sizes-b-series-burstable https://azure.microsoft.com/en-us/pricing/details/virtual-machines/ubuntu-advantage-standard/ This process may take few minutes. Please wait until it completed.\naz aks create --name DigitalAvenue-Cluster --resource-group DigitalAvenueRSG --ssh-key-value .\\ssh-key-DigitalAvenue.pub --node-count 1 --vm-set-type VirtualMachineScaleSets --load-balancer-sku standard --enable-cluster-autoscaler --min-count 1 --max-count 2 --node-vm-size Standard_B4ms --output table For eg:\naz aks create --name \u0026lt;CLUSTER-NAME\u0026gt; \\ --resource-group \u0026lt;RESOURCE_GROUP_NAME\u0026gt; \\ --ssh-key-value ssh-key-\u0026lt;CLUSTER-NAME\u0026gt;.pub \\ --node-count 1 \\ --vm-set-type VirtualMachineScaleSets \\ --load-balancer-sku standard \\ --enable-cluster-autoscaler \\ --min-count 1 \\ --max-count 2 \\ --node-vm-size \u0026lt;VM Type\u0026gt; \\ --output table 5. Connect to cluster 5.1 Connect to AKS cluster using following command (Authentication) az aks get-credentials --resource-group \u0026lt;Resource-Group-Name\u0026gt; --name \u0026lt;AKS-Cluster-Name\u0026gt; For eg:\naz aks get-credentials --resource-group DigitalAvenueRSG --name DigitalAvenue-Cluster 5.2 Verification\nkubectl get nodes This command will returns the created cluster node name and status\nDeploying SQL Server Container in Kubernetes with AKS 6. Manage AKS Cluster Available Features:\nHigh Availability Data Persistency - (Even after a DB crash event) Resiliancy - If SQL Server fails, kubernetes automatically recreate DB in a new pod 6.1 Create a namespace vim DigitalAvenue-ns.yaml apiVersion: v1 kind: Namespace metadata: name: DigitalAvenue kubectl apply -f DigitalAvenue-ns.yaml 6.2 Create SA Password - Secret Password For SQL SA manages sensitive information such as DB passwords and configurations.\nProvide secure passwrd before creating a SA secret\nkubectl create secret generic mssql --from-literal=SA_PASSWORD=\u0026#34;\u0026lt;Secure-Password\u0026gt;\u0026#34; -n \u0026lt;NameSpace\u0026gt; For eg:\nkubectl create secret generic mssql --from-literal=SA_PASSWORD=\u0026#34;PaSsW@rD\u0026#34; -n DigitalAvenue 8. Create a Persistent Storage 8.1 Create \u0026ldquo;PersistentVolumeClaim\u0026rdquo; and \u0026ldquo;PersistentVolume\u0026rdquo; vim db-pvc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: azure-disk namespace: DigitalAvenue provisioner: kubernetes.io/azure-disk parameters: storageaccounttype: Standard_LRS kind: Managed --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: mssql-data namespace: DigitalAvenue annotations: volume.beta.kubernetes.io/storage-class: azure-disk spec: accessModes: - ReadWriteOnce resources: requests: storage: 15Gi Create PersistentVolumeClaim\nkubectl apply -f .\\db-pvc.yaml -n DigitalAvenue verify and check status\nkubectl get pv,pvc -n DigitalAvenue kubectl describe pvc mssql-data -n DigitalAvenue 9. Create the deployment MSSQL Container based on following Docker image: https://hub.docker.com/_/microsoft-mssql-server 9.1 Create Kubernetes manifest for the MSSQL deployment. vim db-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mssql-deployment spec: replicas: 1 selector: matchLabels: app: mssql template: metadata: labels: app: mssql spec: terminationGracePeriodSeconds: 30 hostname: mssqlinst securityContext: fsGroup: 10001 containers: - name: mssql image: mcr.microsoft.com/mssql/server:2019-latest ports: - containerPort: 1433 env: - name: MSSQL_PID value: \u0026#34;Developer\u0026#34; - name: ACCEPT_EULA value: \u0026#34;Y\u0026#34; - name: SA_PASSWORD valueFrom: secretKeyRef: name: mssql key: SA_PASSWORD volumeMounts: - name: mssqldb mountPath: /var/opt/mssql volumes: - name: mssqldb persistentVolumeClaim: claimName: mssql-data --- apiVersion: v1 kind: Service metadata: name: mssql-deployment spec: selector: app: mssql ports: - protocol: TCP port: 1433 targetPort: 1433 type: LoadBalancer Environment variables MSSQL_PID: Edition : Developer (Enterprise, Standard, or Express)\nACCEPT_EULA: License Agreement : Y\nSA_PASSWORD: MSSQL Password Secret : [Added at the step 6.1 ]\nService Network Service: loadbalancer\nApply deployment\nkubectl apply -f db-deployment.yaml -n DigitalAvenue 9.2 Verify the Deployment kubectl get pv,pvc,pod,deployment,svc,secret -n DigitalAvenue Execute below command to get more information about the cluster. It will automatically redirect you to Azure Portal.\naz aks browse --resource-group \u0026lt;MyResourceGroup\u0026gt; --name \u0026lt;MyKubernetesClustername\u0026gt; az aks browse --resource-group DigitalAvenueRSG --name DigitalAvenue-Cluster 9.3 Connect to the SQL Server instance Using \u0026ldquo;sqlcmd\u0026rdquo; CLI\nsqlcmd -S \u0026lt;External IP Address\u0026gt; -U sa -P \u0026#34;PaSsW@rD\u0026#34; ","date":"April 17, 2021","image":"https://digitalavenue.dev/img/post-imgs/sql-on-aks/MSSQL-On-AKS_hufe6d0ff729988c2e70ad894d656fab5a_489388_650x0_resize_q90_box.jpg","permalink":"/blog/2021-04-17-run-microsoft-sql-on-kubernetes-aks/","title":"How To Run Microsoft SQL Server On Kubernetes - Azure Kubernetes Service"},{"categories":["DevOps"],"contents":"Jenkins is the most commonly used and popular open-source CI/CD tool used by many famous companies worldwide. We don\u0026rsquo;t need to have a second thought of it since the biggest companies like Facebook, Udemy, NetFlix, LinkedIn and many more companies using Jenkins with confidence.\nWhen your Jenkins running on the traditional method (on a VM, bare metal server, single container) code base growing accordingly and your code build jobs increase, You also might be running numerous build jobs in parallel. This will lead to redundant resource utilizations and slowness of your delivery pipelines and build/deploy jobs.\nHow to Setup Jenkins on Kubernetes with Scalable Master Slave Setup So, Is there any way to overcome this slowness? Yes, For sure\u0026hellip; Here\u0026rsquo;s the way. This tutorial will walk you through the setup scalable Jenkins server on the Kubernetes cluster using a set of Kubernetes deployment manifest YAML. The use of Kubernetes YAML files will help you track, edit, modify changes and reuse deployments as much as you want.\nJenkins Scalability: Scalability can be defined as the system\u0026rsquo;s ability to expand its capabilities to handle an additional load as required.\nJenkins scaling is based on the Master-Slave model. This means You will have several Jenkins agent instances (Salves) and one Master Jenkins instance responsible for distributing jobs among Jenkins slaves. Jenkins Slaves were doing the jobs when Jenkins Master triggered the build/deploy pipeline.\nSounds great! Right?\nHave a look at this image, and it will feed more idea into your mind.\nYou Got Benefits !!! Multi-Tasking: You can run many more build jobs in parallel Self-Healing: Replacing corrupted Jenkins instances automatically. Cost Saving: Spinning up and removing slaves dynamically based on need. Even Load Distribution: Spreading the load across different physical machines/ VMs / Nodes when required. By spinning up Jenkins on Kubernetes, you will get the power of infinity stones of the Kubernetes. :)\nSTEP 01: Create a Namespace for the Jenkins Deployment Kubernetes namespace provides an additional layer of isolation for your Jenkins deployment. It allows you more control over the CI/CD environment.\nCreate a file named with Jenkins-ns. yaml and define your namespace name here.\nvim jenkins-ns.yaml apiVersion: v1 kind: Namespace metadata: name: jenkins Apply changes to the Kubernetes cluster\nkubectl apply -f jenkins-ns.yaml Use the following command to list all existing namespaces\nSTEP 02: Create Persistent Volume Creating a persistence volume is essential since all of your Jenkins jobs, plugins, configurations should be persisted. If one pod dies, then another new pod can continue with persistent data from your volume.\nOk, Then Let\u0026rsquo;s create a Persistent Volume\nCreate a new file named with \u0026ldquo;jenkins-pv.yaml\u0026rdquo; and paste the below content into your file.\nvim jenkins-deployment.yaml apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-home-pv namespace: jenkins labels: usage: jenkins-shared-deployement spec: storageClassName: default # managed-premium capacity: storage: 5Gi # Change Me accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026#34;/var/jenkins_home\u0026#34; Let\u0026rsquo;s apply the change to the Kubernetes cluster. (Make sure to append the \u0026ldquo;-n jenkins\u0026rdquo; option)\nkubectl apply -f jenkins-pv.yaml -n jenkins STEP 03: Create a PersistentVolumeClaim \u0026ldquo;PersistentVolumeClaim\u0026rdquo; request for storage with a specific size and access mode. In this case, I\u0026rsquo;m going to claim 5GB of storage to my \u0026ldquo;Jenkins_Home.\u0026rdquo;\nNow, Create another file named \u0026ldquo;jenkins-pvc.yaml\u0026rdquo; and paste the below content.\nNote: Make sure to match the selector labels If you plan to change the manifest as you like. (Ex: usage: Jenkins-shared-deployment)\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-home-pvc namespace: jenkins labels: app: jenkins spec: accessModes: - ReadWriteOnce storageClassName: default # managed-premium resources: requests: storage: 5Gi # Change Me Then, apply changes to the cluster.\nkubectl apply -f jenkins-pvc.yaml -n jenkins STEP 04: Create a Deployment Manifest Here, I\u0026rsquo;m using Jenkins LTS docker image and expose port 8080 for the web access and port 50000 for the Jenkins JNLP service, which use to access Jenkins workers, respectively.\nLet\u0026rsquo;s create a Kubernetes deployment manifest for the Jenkins server. Here I\u0026rsquo;m using Jenkins LTS image and exposing port number 8080 and 50000 for web access and inter master agent communications.\nNow, create a file named \u0026ldquo;jenkins-deployemt.yaml\u0026rdquo; and paste the below content.\napiVersion: apps/v1 kind: Deployment metadata: name: jenkins-master namespace: jenkins labels: app: jenkins spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 selector: matchLabels: app: jenkins template: metadata: labels: app: jenkins spec: securityContext: # Set runAsUser to 1000 to let Jenkins run as non-root user \u0026#39;jenkins\u0026#39; which exists in \u0026#39;jenkins/jenkins\u0026#39; docker image. runAsUser: 0 fsGroup: 1000 containers: - name: jenkins-master # https://github.com/jenkinsci/docker image: jenkins/jenkins:lts #jenkinsci/jenkins:2.150.1 imagePullPolicy: IfNotPresent env: - name: JENKINS_HOME value: /var/jenkins_home - name: JAVA_OPTS value: -Djenkins.install.runSetupWizard=false - name: JAVA_OPTS value: \u0026#34;-Xmx8192m\u0026#34; # - name: COPY_REFERENCE_FILE_LOG # value: $JENKINS_HOME/copy_reference_file.log ports: - name: jenkins-port # Access Port for UI containerPort: 8080 - name: jnlp-port # Inter agent communication via docker daemon containerPort: 50000 resources: # Resource limitations requests: memory: \u0026#34;256Mi\u0026#34; # Change Me cpu: \u0026#34;50m\u0026#34; limits: memory: \u0026#34;4Gi\u0026#34; # Change Me cpu: \u0026#34;2000m\u0026#34; volumeMounts: - name: jenkins-home-volume mountPath: \u0026#34;/var/jenkins_home\u0026#34; volumes: - name: jenkins-home-volume persistentVolumeClaim: claimName: jenkins-home-pvc Then apply changes to the cluster by executing the following command.\nkubectl apply -f jenkins-deployment.yaml -n jenkins STEP 05: Create a Service Manifest Now, We have deployed Jenkins on Kubernetes. But, It is still not accessible.\nServices in Kubernetes use to expose applications running on Kubernetes pods. Now we need to grant access to the Jenkins via Kubernetes service. So, We need to define a service for the Jenkins server. Here, we expose port 8080 and 50000.\nOk, Let\u0026rsquo;s create a file named \u0026ldquo;jenkins-svc.yaml\u0026rdquo; and paste the content below.\napiVersion: v1 kind: Service metadata: name: jenkins-ui-service namespace: jenkins spec: type: ClusterIP # NodePort, LoadBalancer ports: - protocol: TCP port: 8080 targetPort: 8080 # nodePort: 30100 name: ui selector: app: jenkins --- apiVersion: v1 kind: Service metadata: name: jenkins-jnlp-service namespace: jenkins spec: type: ClusterIP # NodePort, LoadBalancer ports: - port: 50000 targetPort: 50000 selector: app: jenkins And then apply the changes to the cluster.\nkubectl apply -f jenkins-svc.yaml -n jenkins Now you can access the Jenkins Dashboard.\nSTEP 06: Port-Forwarding (Optional) As you can see in the service manifest, I have used \u0026ldquo;ClusterIP\u0026rdquo; as my network service. So, I need to port-forward Jenkins-UI service to access through my (localhost) computer.\nIf you plan to deploy on Cloud service, you will get many more options such as ingress controllers. We discuss them in the next tutorial.\nFor now, I\u0026rsquo;m going to port forward Jenkins-UI (8080/TCP) to my localhost as I\u0026rsquo;m deploying Jenkins on Minikube.\nGet the Jenkins-UI service details.\ndimuthu@srv01:~/jenkins-k8s$ kubectl get svc -n jenkins NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jenkins-jnlp-service ClusterIP 10.106.232.10 \u0026lt;none\u0026gt; 50000/TCP 10m jenkins-ui-service ClusterIP 10.98.225.50 \u0026lt;none\u0026gt; 8080/TCP 10m Now, I\u0026rsquo;m going to forward my port 8080 to my local machine. Simply run the following command.\ndimuthu@srv01:~/jenkins-k8s$ kubectl port-forward service/jenkins-ui-service 8080:8080 -n jenkins Forwarding from 127.0.0.1:8080 -\u0026gt; 8080 Forwarding from [::1]:8080 -\u0026gt; 8080 STEP 05: Access Jenkins Server Now, You can access your Jenkins server through the web browser.\nhttp://\u0026lt;YOUR-HOST-NAME-OR-IP\u0026gt;:8080/\nFor the 1st time, Jenkins will prompt to enter an unlock password. You can get the password by looking at the logs for Jenkins deployment or pod.\nGet the name of the deployment.\nkubectl get deployments -n jenkins Look for the initial password in the deployment logs. To get the initial password, You need to execute the following command on your kubectl terminal and copy the output password and paste it into the Administrator password text box.\nkubectl logs deployment/jenkins-master -n jenkins 706f68110c5b4b3a8846c1208f0c1c7c\nLog output will look like below.\ndimuthu@srv01:~/jenkins-k8s$ kubectl logs jenkins-master-d654bbdf5-5bsfz -n jenkins Running from: /usr/share/jenkins/jenkins.war webroot: EnvVars.masterEnvVars.get(\u0026#34;JENKINS_HOME\u0026#34;) 2021-04-14 18:48:28.430+0000 [id=1] INFO org.eclipse.jetty.util.log.Log#initialized: Logging initialized @894ms to org.eclipse.jetty.util.log.JavaUtilLog ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: 706f68110c5b4b3a8846c1208f0c1c7c This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* Troubleshooting: If the \u0026quot;initialAdminPassword\u0026quot; not available, You may have to re-deploy the jenkins and try again.\nREF: Admin Password Not Available\nSTEP 07: Install Plugins In this section, You will need to install Jenkins plugins according to how you will use them. You can choose either option.\nIn this case, I\u0026rsquo;m choosing the \u0026ldquo;select plugins to install option.\u0026rdquo;\nOnce you choose the required plugins, click next. This plugin installation may take considerable time. Please wait until completed.\nSTEP 08: Create First Admin User Next, You have to provide your \u0026ldquo;name\u0026rdquo;, username\u0026quot;, \u0026ldquo;password\u0026rdquo;, and \u0026ldquo;email\u0026rdquo; for the Jenkins admin user.\nNext, You need to provide your FQDN or IP address. By default, the IP address will automatically load into the \u0026ldquo;instance URL\u0026rdquo; section.\nProvide a domain name as required. (optional)\nNow we have deployed our Jenkins server on a Kubernetes cluster. Now we can use this Jenkins server as usual. But, We also can set up Jenkins Slave agent to our Jenkins Master.\nWe will discuss how to set up Jenkins-Slaves in the next tutorial. You can visit the next session from this link.\nIn this lesson, you learned\u0026hellip;\nHow to setup how to deploy a Kubernetes cluster How to use persistent volumes, attaching services to the deployment. I hope you learn something new from this tutorial. If you facing any difficulties, please comment below. I\u0026rsquo;ll regularly jump into comments.\n","date":"April 16, 2021","image":"https://digitalavenue.dev/img/post-imgs/jenkins-k8s-deploy/Jenkins-on-kubernetes_huf054e7e907ad73843be3a97f4ff1efde_675328_650x0_resize_q90_box.jpg","permalink":"/blog/2021-04-16-how-to-setup-jenkins-on-kubernetes/","title":"How To Setup Jenkins Server on Kubernetes - Master Slave Setup"},{"categories":["DevOps"],"contents":"Managing Stateful Applications So far, We were discussing about the stateless applications. But, How are We going to take of our Stateful applications that need to persist their data on a disk even after a reboot. Imagine a front-end application which has a database.\nKubernetes offers many kinds of objects to manage Stateful applications using \u0026ldquo;StatefulSet\u0026rdquo;, \u0026ldquo;StorageClass\u0026rdquo;, \u0026ldquo;PersistentVolumeClaim\u0026rdquo; and \u0026ldquo;PersistentVolumes\u0026rdquo;\nStateful Sets When you running a Stateful application using dynamic volume provisioning, Kubernetes ensure that storage was mounted at a volume to the pod. We need to mention our Kubernetes service in the \u0026ldquo;StatefulSet\u0026rdquo; manifest. That\u0026rsquo;s why we need to define the \u0026ldquo;serviceName\u0026rdquo; in the \u0026ldquo;StatefulSet\u0026rdquo; spec section.\nA StatefulSet also requires a \u0026ldquo;PersistentVolume\u0026rdquo; which can provisioned either manually or through an automatic provisioned through a storage class. This is why we mention \u0026ldquo;VolumeClaimTemplate\u0026rdquo; in the \u0026ldquo;StatefulSet\u0026rdquo; manifest.\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \u0026#34;nginx\u0026#34; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;my-storage-class\u0026#34; resources: requests: storage: 1Gi Persistent Volume (PV) Persistent volumes are storage resources such as disks and file systems. They can provisioned either manually by administrator or dynamic provisioning using Storage Classes. The we can mount these disks into containers running inside pods to ensure the data persistence.\nKubernetes has operational activities such as Provisioning, Configuring and attaching. These relationships can be mapped as shown below.\nStorage Activity Storage Primitive Provisioning Persistent Volume Configuring Storage Class Attaching Persistent Volume Claim Static Provisioning In static provisioning, The administrator needs to create volumes manually and supply those volume information into a persistent volume manifest. In order to use these Volumes needs be claimed.\nIMG5\nIn this manifest shows how we can provision our persistent volume using Azure Disk. Also we need to include persistent volume claim template.\n--- apiVersion: v1 kind: PersistentVolume metadata: name: nginx-pv labels: usage: nginxdisk spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce azureDisk: kind: Managed diskName: myAKSDisk diskURI: /subscriptions/\u0026lt;subscriptionID\u0026gt;/resourceGroups/myRG/providers/Microsoft.Compute/disks/myAzureDisk --- apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \u0026#34;nginx\u0026#34; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi selector: matchLabels: usage: nginxdisk Dynamic Volume Provisioning Even though the static provisioning is a good option, in some cases we may have to shift our workload into a different cloud provider. Then we may have to change our volume information in the manifest file according to the newly provisioned volumes. So\u0026hellip; what if you have hundreds or thousands of containers to mount in this way? So, this may not be the good option at all.\nTo ensure a more dynamic and platform independent solution, Here we would create a storage class object which define the kind of object such as Disk, SSD, NFS Share, Block Storage, etc. finally, the cloud provider is going to provision it for us. Then PersistentVolumeClaim use this storage class to tell the cloud provider to provision a storage. Finally, Kubernetes mount the PersitentVolumeClaim (PVC) as a volume to the Pod.\nStorage Class A Storage Class object defines the kind of object which needs provisioning ( Disks, SSD, Block Storage, NFS\u0026hellip;). Then, the cloud provider is going to provision the volume. In here storage class name should be platform independent ( Fast, Disk, SSD\u0026hellip;)\nIn this example, Here we provisioning a Azure Managed Premium Storage in a \u0026ldquo;fast\u0026rdquo; Storage Class.\n--- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast provisioner: kubernetes.io/azure-disk reclaimPolicy: Retain parameters: storageaccounttype: Premium_LRS kind: Managed Persistent Volume Claim (PVC) Persistent Volume Claim refers the storage classes dynamically. It can provision volumes as required. In this example, storage class is referring as fast storage class to provision an Azure disk of 5GB. Then, volumes can mount to the container using \u0026ldquo;PersistentVolumeClaim\u0026rdquo;\n--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: azure-managed-disk spec: accessModes: - ReadWriteOnce storageClassName: fast resources: requests: storage: 5Gi Dynamic Provisioning On a StatefulSet When we at the manifest carefully. You can see there is no singple place that we mentioned a cloud provider. In this way, we can make storage provisioning is more portable and independent from cloud providers.\n--- apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \u0026#34;nginx\u0026#34; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;fast\u0026#34; resources: requests: storage: 5Gi DaemonSet Daemons are background processes that are specialized to perform specific admin tasks such as collect logs, monitoring and more.\nKubernetes object called \u0026ldquo;DaemonSets\u0026rdquo;\u0026quot; are using special use cases such as monitoring node health, shipping logs, and spinning up cluster storage daemons.\nIn this DaemonSet manifest shows which runs FluentD logging app within all nodes of the Kubernetes cluster.\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can\u0026#39;t run pods - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers Exposing Kubernetes Applications In this section, I\u0026rsquo;m going to discuss the multiple ways to expose Kubernetes objects to internal or external clients through the Kubernetes services and Ingress resources.\nServices Pods are ephemeral objects. Which means, Pod\u0026rsquo;s lifespan is end with restarts. Every Pod has an IP address. If one unhealthy pod dies, Kubernetes replace it with fresh pod with different IP address. This leads to a problem as we cannot use the same IP address which was in previous pod.\nTo overcome this, Kubernetes provides a built-in service discovery mechanism called CoreDNS. CoreDNS can keep track of the IP addresses of the Pods. This CoreDNS service maintain a DNS register which helps to provide internal Load Balancing and DNS Resolution. Therefore, We don\u0026rsquo;t need have to worry about the dynamic IP addresses of Pods.\nBased on this mechanism we can expose our application in three ways.\nClusterIP NodePort LoadBalancer ClusterIP Services ClusterIP service is the default service type. Let\u0026rsquo;s say something we are running a database in a container within Kubernetes cluster. And we don\u0026rsquo;t need to expose our database pod outside the Kubernetes cluster. In here ClusterIP service is discoverable only within the Kubernetes cluster.\nBut any pods within the cluster can access the database using its \u0026ldquo;ServiceName:Port\u0026rdquo; which can routed traffic into the correct pod through dynamic service discovery. And also we don\u0026rsquo;t need to worry about pod\u0026rsquo;s IP.\napiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 NodePort Services A NodePort service exposes your Pods into node\u0026rsquo;s static port called the NodePort. This NodePort can discoverable from any of Kubernetes nodes. If your nodes reachable from the internet, external clients would able to access your pod on any \u0026ldquo;NodeIP:NodePort\u0026rdquo;\nThese NodePort ranges from 30000 - 32767 This mechanism is not recommended for production deployments.\nIn this example shows NodePort service expose the nginx deployment on Port 31000. If you don\u0026rsquo;t specify a port in the manifest, it pickup freely available port in the range.\napiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 31000 type: NodePort LoadBalancer Services In general, a load balancer service generates a NodePort and requests that the cloud provider automatically provision a load balancer in front of your Nodes.\nTo clarify, suppose you have a master and three worker nodes called master, node01, node02, and node03, as well as an NGINX pod running on port 80 that you want to open to outside world.\nA LoadBalancer service will first create a NodePort on any random port from the NodePort range (assume,31000), and also a Load Balancer with master:31000, node01:31000, node02:31000, and node03:31000 as the backend.\nIn a production system, a Load Balancer service is one of the ways to expose your frontend pods to the Internet inside of your internal infrastructure.\napiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: LoadBalancer ","date":"April 13, 2021","image":"https://digitalavenue.dev/img/post-imgs/k8s-objects-02/k8s-objects-explained-02_hu6f62d347f51090a3812c6dbc285b3e64_1235710_650x0_resize_q90_box.jpg","permalink":"/blog/2021-04-13-kubernetes-objects-explained-2/","title":"Most Wanted Kubernetes Objects Explained - Episode 02"},{"categories":["DevOps"],"contents":"Kubernetes is now becoming a new kid in town. Now, Kubernetes is in action, because its easy to deploy scale and manage thousands of containerized applications more quicker. Now, most of cloud providers such as AWS, Azure , and Google privde managed kubernetes services run our deployments. This article would helps anyone who steping into the kubernetes world. This will also would helpful for someone who getting ready for Certified Kubernetes Administrator exam. Now, I\u0026rsquo;ll explain main kubernetes component you need to know.\nKubernetes Pods A Pod is the fundamental building block in the Kubernetes. Usually, a pod runs only a single container. But there are some instances you need to run more than one container in the pod. Such as init containers and internal containers.\nThis example defines pod manifest for single containers --- # Digital Avenue Pod Definition File kind: Pod apiVersion: v1 metadata: name: nginx-app labels: app: nginx spec: containers: - name: nginx-app image: nginx This example defines pod manifest with \u0026ldquo;InitContainers.\u0026rdquo; --- # Digital Avenue Pod Definition File kind: Pod apiVersion: v1 metadata: name: nginx-app labels: app: nginx spec: containers: - name: nginx-app image: nginx volumeMounts: - mountPath: \u0026#34;/etc/nginx\u0026#34; name: nginx-pv-claim initContainers: - name: volume-mount-data-log image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 101:101 /etc/nginx\u0026#34;] volumeMounts: - mountPath: \u0026#34;/etc/nginx\u0026#34; name: nginx-pv-claim The above configuration shows the Nginx container with a busy-box init container. In here, the busybox container start first and making sure correct permission \u0026ldquo;UID\u0026rdquo;:101 and \u0026ldquo;GID\u0026rdquo;:101 in the \u0026ldquo;/etc/nginx\u0026rdquo; persistent volume. Then spin up the nginx container secondly.\nKubernetes Replication Controller If you deploy only a pod, It is not possible to scale and replicate. Once you delete it, it\u0026rsquo;s gone. Replication controllers maintain a minimum number of pods that are always running. To do this, we need to have an object called \u0026ldquo;ReplicationController\u0026rdquo;. However, Now Kubernetes has introduced a new object called \u0026ldquo;ReplicaSet\u0026rdquo; as a replacement for \u0026ldquo;ReplicationController.\u0026rdquo;\n# Digital Avenue Replication Controller Definition File apiVersion: v1 kind: ReplicationController metadata: name: nginx spec: replicas: 3 selector: app: nginx template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx-app image: nginx In the above Kubernetes manifest, The replicas determine the number of pods that would run at their desired state. Replication Controllers scales pods by searching on the pod labels. \u0026ldquo;Selector\u0026rdquo; labels use to match the pod labels exactly.\nKubernetes Replica Sets Replica Sets are the new replacement for the Replication Controllers. Unlike the Replication Controllers, The ReplicaSets can use set-based search notations to group pods rather than a named Key: Value pair based match. You will get more control and more dynamic selector options to use ReplicaSet.\n# Digital Avenue ReplicaSet Definition File apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx environment: dev template: metadata: name: nginx labels: app: nginx environment: dev spec: containers: - name: nginx-app image: nginx Or like this\u0026hellip;\n# Digital Avenue ReplicaSet Definition File apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: nginx spec: replicas: 3 selector: matchExpressions: - {key: app, operator: In, values: [nginx]} - {key: environment, operator: NotIn, values: [production]} template: metadata: name: nginx labels: app: nginx environment: dev spec: containers: - name: nginx-app image: nginx Kubernetes Deployments Deployment manifest is the most recommended and primarily used method to deploy Kubernetes pods. Deployment manifest replaces \u0026ldquo;Replication Controllers\u0026rdquo;. Deployment is the most powerful Kubernetes object since It\u0026rsquo;s possible to roll out and roll back changes/deployments.\nYou can use deployments when\u0026hellip;\nYou require scaling and self-healing of your pods\nIf you are running stateless applications that are don\u0026rsquo;t required to store persistently on a disk, such as applications connected with back-end services and databases.\n# Digital Avenue Deployment Definition File apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx environment: dev template: metadata: name: nginx labels: app: nginx environment: dev spec: containers: - name: nginx-app image: nginx Deployment manifest also performs the same task as ReplicaSet. Deployments manage pods employing a ReplicaSet. The \u0026ldquo;Deployment\u0026rdquo; manifest pretty much slimmer to \u0026ldquo;ReplicaSet\u0026rdquo;. The only difference is the only object type.\nScaling Deployments Scaling Deployment can be performed in declarative and imperative ways. Which means you can modify the replica section of the manifest file and run it.\n\u0026#34;kubectl apply -f \u0026lt;manifest file\u0026gt;\u0026#34; Or else, Run this command imperatively.\n\u0026#34;kubectl scale deployment nginx-deployment --replicas=10\u0026#34; But, modifying the manifest file is the most recommended way.\nAutScaling Deployments Kubernetes cloud auto-scale the deployment if the pod exceed the defined threshold. To perform this, Kubernetes uses the object called \u0026ldquo;HorizontalPodAutoScaler\u0026rdquo;. This object checks the pod metrics, and if it breaches a defined threshold, It spins up a new pod to maintain the load. So, you need to specify the minimum and the maximum number of pods.\nHorizontal Pod AutoScaler The Horizontal Pod AutoScaler ensures your Kubernetes deployment ca scale based on pre-defined metrics horizontally. Kubernetes also can manage the creation and deletion of pods based on metrics.\nYou can do this either using \u0026ldquo;kubectl\u0026rdquo; or using \u0026ldquo;HorizontalPodScaler\u0026rdquo; manifest file.\nFirst of all, we need to modify our deployment manifest to apply a resource limit to the pods.\n# Digital Avenue Deployment Definition File apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx environment: dev template: metadata: name: nginx labels: app: nginx environment: dev spec: containers: - name: nginx-app image: nginx resources: limits: cpu: 500m requests: cpu: 200m This configuration ensures the nginx container limit up-to 500 milicore of the worker node.\nUsing \u0026ldquo;HorizontalPodAutoScaler\u0026rdquo; You can auto-scale pods using more advanced metrics and custom metrics such as network utilization.\n# Digital Avenue HorizontalPodAutoscaler Definition File apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: nginx namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 - type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 10k According to the above manifest, you can see there are three kinds of metrics.\nResource Metrics: CPU, Memory Pod Level Metrics: Metrics such as \u0026ldquo;Packet-Per-Second\u0026rdquo; and other network traffic within the pod Object Level Metrics: Metrics such as \u0026ldquo;Requests-Per-Second\u0026rdquo; and other network traffic in other Kubernetes objects such as Ingress Controllers. Managing Stateful Applications So far, We were discussing stateless applications. How are We going to take care of our Stateful applications that need to persist their data on a disk even after a reboot? Imagine a front-end application that has a database.\nKubernetes offers many kinds of objects to manage Stateful applications using \u0026ldquo;StatefulSet\u0026rdquo;, \u0026ldquo;StorageClass\u0026rdquo;, \u0026ldquo;PersistentVolumeClaim\u0026rdquo; and \u0026ldquo;PersistentVolumes\u0026rdquo;\nWe will discus other most commonly used Kubernetes object in the next episode.\n","date":"March 19, 2021","image":"https://digitalavenue.dev/img/post-imgs/k8s-objects-01/kubernetes-objects-01_kubernetes_huc17349f2d83acefb99c70804456df5e1_317757_650x0_resize_q90_box.jpg","permalink":"/blog/2021-03-19-kubernetes-objects-explained-1/","title":"Most Wanted Kubernetes Objects Explained - Episode 01"},{"categories":["DevOps"],"contents":"Building Docker Images using Jenkins This tutorial will show you how to configure Jenkins to build Docker images based on a Dockerfile. These steps will require when you will use Docker with CI/CD pipelines that build your applications into Docker images and deploy into deferent environments such as dev, staging, and finally in production.\nI\u0026rsquo;m already having a Jenkins server that is running as a Docker container. If you don\u0026rsquo;t have a Jenkins server on a Docker container, You can refer to my previous article to spin up a new one.\nSTEP 01: Install Docker Plugin This plugin allows containers to be dynamically provisioned as Jenkins nodes using Docker. It is a Jenkins Cloud plugin for Docker.\nThis docker plugin aims to be able to use a Docker host to dynamically provision a docker container as a Jenkins agent node, let that run a single build, then tear-down that node, without the build process (or Jenkins job definition) requiring any awareness of docker.\nAs the first step, you need to install the Docker plugin for Jenkins. Whenever we are using docker in our Jenkins pipelines, Jenkins create a \u0026ldquo;Cloud Agent\u0026rdquo; using the \u0026ldquo;Docker\u0026rdquo; plugin. This agent will be a \u0026ldquo;Docker Container, \u0026quot; configured to communicate with our Jenkins server\u0026rsquo;s Docker Daemon.\nThe Pipelines with build jobs will use this agent container to execute Docker images to build steps. Then, the docker image can be pushed into a Docker container registry for deployment.\nFist, Head-over to Jenkins server and select \u0026ldquo;Manage jenkins\u0026rdquo;, then select the \u0026ldquo;Manage Plugin\u0026rdquo; option under \u0026ldquo;system Configuration.\u0026rdquo;\nNow, Click the \u0026ldquo;Available\u0026rdquo; tab to view all the Jenkins plugins that can be installed. Search for \u0026ldquo;Docker\u0026rdquo; within the search box. Several plugins are available named \u0026ldquo;Docker\u0026rdquo;, but choose the exact plugin which comes under the \u0026ldquo;Cloud Providers\u0026rdquo; heading. Then, click the \u0026ldquo;Install Without Restart\u0026rdquo; option and install the plugin.\nDocker Plugin For Jenkins : https://plugins.jenkins.io/docker-plugin/ STEP 02: Configure Docker Plugin Once the plugin has been configured, you can define how the Docker plugin needs to launch the Docker containers. This configuration tells the Docker plugin which Docker image to use as the agent. , Docker plugin commands the agent to use a specific Docker image. And after, Docke-Agent Container host the Docker environment to build our Docker image where we defined in the Jenkins pipeline.\nWhen the Docker build request comes from the Jenkins server, Docker-Agent starts to spin up a new container for the Jenkins pipeline task/job.\nFor the sake of simplicity, This Docker-Agent container spins up when the Jenkins build job/pipeline triggered from the Jenkins server.\nNow, Let\u0026rsquo;s head over to \u0026ldquo;Manage Jenkins\u0026rdquo; and select the \u0026ldquo;Configure System\u0026rdquo; section. Scroll to the bottom of the page, and there is a section called \u0026ldquo;Cloud\u0026rdquo;. And click on the \u0026ldquo;A separate configuration section\u0026rdquo; link.\nManage Jenkins \u0026gt; Configure System \u0026gt; [Scroll Down to Far Bottom] \u0026gt; Cloud \u0026gt; [Configure Clouds]\nThe Cloud Configuration section will be open in a separate section.\nClick on the \u0026ldquo;Add New Cloud\u0026rdquo; dropdown menu and select the option \u0026ldquo;Docker\u0026rdquo; from the list. And then click on the \u0026ldquo;Docker Cloud Details\u0026rdquo; button.\nSelect the option \u0026ldquo;Docker\u0026rdquo; from the \u0026ldquo;Configure Cloud\u0026rdquo; dropdown button. And then click on the \u0026ldquo;Docker Cloud Details\u0026rdquo; button.\nNow, You want to set the Docker-Agent Host URI. And Click \u0026ldquo;Test Connection\u0026rdquo;. Here you will get the following error as \u0026ldquo;Client sent an HTTP request to an HTTPS server\u0026rdquo;.\nTo solve this issue, we need to setup Server Credentials.\nNote: Your test should have worked If you configured Jenkins and Docker-Agent to skip the TLS and to use port 2375. (STEP 04 and 05)\nThe \u0026ldquo;Docker Host URI\u0026rdquo; is where Jenkins launches the agent container. In this scenario, I\u0026rsquo;ll use my previously configured Docker-Agent\u0026rsquo;s hostname. (Refer STEP 05)\nClick on the \u0026ldquo;Add\u0026rdquo; dropdown button and select \u0026ldquo;Jenkins\u0026rdquo;, And then we need to create \u0026ldquo;X.509 Client Certificate\u0026rdquo; from the Docker-Agent(Dind)\u0026rsquo;s \u0026ldquo;/cert\u0026rdquo; directory. We have created those certificates at STEP-04.\nTo accomplish this task, you will need the following certificates.\nClient Key = key.pem Client Certificate = cert.pem Server CA Certificate = ca.pem We can collect these by running the following docker commands. Copy and paste these certificate content into relevant fields in the \u0026ldquo;Add Credentials\u0026rdquo; section.\nsudo docker exec jenkins-dind cat /certs/client/key.pem sudo docker exec jenkins-dind cat /certs/client/cert.pem sudo docker exec jenkins-dind cat /certs/server/ca.pem Or else you can locate these file on \u0026ldquo;/var/lib/docker/volumes/jenkins-docker-certs/_data\u0026rdquo; mount directory path.\nIn the end, Your credentials will look like this.\nNow, Add your \u0026ldquo;Docker Host URI and select \u0026ldquo;Server Credentials\u0026rdquo; from the dropdown menu. Finally, \u0026ldquo;Test Connection\u0026rdquo; will get the Docker version and API version as a result. So, now we can reach the Docker daemon from the Jenkins server. Now, the Jenkins server has been configured to build Docker images.\nSTEP 03: Configure Docker Agent Templates The Docker Agent Template is the container that will be started to handle the build process.\nLet\u0026rsquo;s head-over to Manage Jenkins \u0026gt; Configure System \u0026gt; [Scroll bottom] \u0026gt; Cloud \u0026gt; Configure Cloud.\nNow, Click on \u0026ldquo;Docker Agent Template\u0026rdquo; and then \u0026ldquo;Add Docker Template\u0026rdquo;. Here, You can configure the container options. You can attach these templates when you start building docker containers.\nI\u0026rsquo;m using the \u0026ldquo;jenkins/agent:latest\u0026rdquo; image for the demonstration purpose, which has the docker client inside.\nHere, \u0026ldquo;Labels\u0026rdquo; and \u0026ldquo;Name\u0026rdquo; cam use to associate a job to a particular agent. This means that Jenkins\u0026rsquo;s labels build jobs to indicate that they should be built via the Docker agent that we are running here.\nProvide required docker command options, as seen in the image below.\nSTEP 04: Test The Setup A. Create a Build Project\nNow, Let\u0026rsquo;s move on to the Jenkins dashboard, click on \u0026ldquo;New Item\u0026rdquo;, and create \u0026ldquo;Freestyle Project\u0026rdquo;.\nSet the \u0026ldquo;Label expression\u0026rdquo; to \u0026ldquo;jenkins-agent.\u0026rdquo;\nThen, We need to add a new build step. Move down further and select \u0026ldquo;Execute Shell\u0026rdquo; from the \u0026ldquo;Build\u0026rdquo; section.\nAdd the sample \u0026ldquo;Hello World!\u0026rdquo; text inside the execution shell box. Now, Save and build your job.\nAccording to the execution commands, Jenkins will download the image, and run the container then run the job upon it. Finally, you will get an output, as seen in the image below.\nAlso, You can check weather which containers are running on the Docker-on-Docker container by using this command.\ndocker exec -it jenkins-docker docker ps\nWhile the build process, the result will be output like this.\ndimuthu@svr05:~$ sudo docker exec -it jenkins-docker docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES be7725b397e3 jenkins/agent:latest \u0026#34;/bin/sh\u0026#34; 9 seconds ago Up 9 seconds kind_carver Now, All the configuration has been completed from scratch. Now, You can define your own \u0026ldquo;Cloud Templates\u0026rdquo; and run the Docker build jobs.\nSummary In this article, you learned to Provision ephemeral Docker Containers as Jenkins Agent Node which helpful for better resource utilization and many more advantages. If you are facing issues with the implementation, please comment below. I will regularly reply here.\nHappy learning !\n","date":"March 13, 2021","image":"https://digitalavenue.dev/img/post-imgs/Build_Docker_Images_on_Jenkins/docker-image-build-jenkins_jenkins_hu261af25c84b26195d11ab820e46d11e7_357689_650x0_resize_q90_box.jpg","permalink":"/blog/2021-03-13-build-docker-images-jenkins/","title":"Build Docker Images With Docker Containers as Jenkins Build Slaves"},{"categories":["DevOps"],"contents":"Setup Jenkins On Ubuntu 20.04LTS Jenkins is an open-source continues integration and continues deployment tool. Which can use to automate application building, testing and deploying. Jenkins is the most popular automation server. Jenkins built using java, which can integrate with numerous plugins.\nPrerequisites OS Requirement: Make sure to use Ubuntu LTS versions\nHardware Requirement:\nMinimum Hardware Requirement Recommended Hardware Requirement 1GB+ Hard Disk Space 50GB+ Hard Disk Space 256 MB RAM 1GB + RAM Software Requirement:\nJava - JRE8/11 (32Bit or 64Bit Supported) Note: Older versions and Java 9, 10,12 are not supported\nJDK JRE OpenJDK 8 , OpenJRE 8 JRE 8 OpenJDK 11 , OpenJRE 11 JRE 11 STEP 01: Update OS sudo apt update -y STEP 02: Install Java Jenkins is built with Java, So we need to install the appropriate Java version. This time I\u0026rsquo;m going to use OpenJDK version 11. Now head-over to your ubuntu terminal and do the following steps.\nRun this command and pick one OpenJDK version 8 or 11 from the list\nsudo apt search openjdk sudo apt-get install openjdk-11-jdk -y dimuthu@build-svr:~$ java -version openjdk version \u0026#34;11.0.10\u0026#34; 2021-01-19 OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.20.04) OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.20.04, mixed mode, sharing) STEP 03: Add GPG Key and Repository Install GPG Trusted Key\nsudo wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add - Add Jenkins Repository\nThis step is to append the Jenkins repository in the Debian source.list\nsudo sh -c \u0026#39;echo deb https://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list\u0026#39; Once again, update OS repositories.\nsudo apt-get update -y Install Jenkins\nsudo apt-get install jenkins -y STEP 04: Enable \u0026amp; Start Jenkins Start \u0026amp; enable Jenkins service on system boot.\nsudo systemctl enable jenkins sudo systemctl start jenkins Optionally, Sometimes you need to allow TCP port 8080 though out the firewall. If you are using Ubuntu, execute the following command.\nsudo ufw allow 8080 STEP 05: Access Jenkins Now, You can access your Jenkins server through the web browser.\nhttp://\u0026lt;YOUR-HOST-NAME-OR-IP\u0026gt;:8080/\nFor the 1st time, Jenkins will prompt to enter an unlock password. You need to execute the following command on your terminal and copy the output password and paste it into the Administrator password text box.\nTroubleshooting: If the \u0026quot;initialAdminPassword\u0026quot; not available, You may have to remove jenkins and try again.\nREF: Admin Password Not Available\nFor the first time, Jenkins will prompt us to unlock, and it will tell us to copy the password from this location. Copy this 32charactor alphanumeric password and paste it into the text box in the wizard.\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword 204bedca295343e483335ae2f26e75f4\nSTEP 07: Install Plugin In this section, You will need to install Jenkins plugins according to how going to use them. You can choose either option.\nIn this case, I\u0026rsquo;m choosing the \u0026ldquo;select plugins to install option.\u0026rdquo;\nOnce you choose the required plugins, click next. This plugin installation may take considerable time. Please wait until completed.\nSTEP 08: Create First Admin User Next, You have to provide your \u0026ldquo;name\u0026rdquo;, username\u0026quot;, \u0026ldquo;password\u0026rdquo;, and \u0026ldquo;email\u0026rdquo; for the Jenkins admin user.\nNext, You need to provide your FQDN or IP address. By default, the IP address will automatically load into the \u0026ldquo;instance URL\u0026rdquo; section.\nNow, Jenkins installation has been completed successfully. This is the 1st session of the Jenkins tutorial series. If you love to learn more about Jenkins, refer to my other articles available on this website.\nIf you are facing issues with the installation, please comment below. I will regularly reply here.\n","date":"March 7, 2021","image":"https://digitalavenue.dev/img/post-imgs/Jenins-Install-ubuntu/Jenkins-Ubuntu_hu00fdade2ec7ca3042c13dda3283ca565_167034_650x0_resize_q90_box.jpg","permalink":"/blog/2021-03-07-install-jenkins-ubuntu/","title":"How To Setup Jenkins On Ubuntu 20.04LTS"},{"categories":["DevOps"],"contents":"How to Run Jenkins on Docker with Persistent Volumes Do you want to setup Jenkins on Docker? But You may just curious about the persistence of your Jenkins data. Ok, I will explain them here step by step.\nYou may arise a few questions like\u0026hellip;\nHow can I access Jenkins data from outside of the container?\nAre these data persists even after the Jenkins container restarted?\nWhat happens if the container crashed?\nhow do I backup data after Jenkins is containerized?\nIn this tutorial, I will walk you through setting up the Jenkins server on docker with persistent volumes. First, I\u0026rsquo;ll show you each step manually, and then I\u0026rsquo;ll use docker-compose to make things faster and reusable.\nIn this tutorial, I will walk you through the following sections. Section 01. Run Jenkins on a Docker Container Manually Section 02. Run Jenkins on a Docker Container Using Docker Compose ( make things faster and reusable) What is Jenkins? Jenkins is an open-source continues integration and continues deployment tool. Which can use to automate application building, testing and deploying. Jenkins is the most popular CI/CD automation tool.\nWe can extend Jenkins\u0026rsquo;s capability by integrating with many other tools such as SonarQube and measuring code quality and standard before the deployment performed. And also, Jenkins supports thousands of plugins that can use to make things easier.\nPrerequisites Hardware Requirement\nMinimum Requirement Recommended Requirement 256MB+ of RAM 1GB+ of RAM 1GB+ HDD Capacity 50GB+ HDD Capacity Sections 01: Run Jenkins On Docker Container Manually STEP 01: Install Docker and Docker-Compose On Ubuntu You can directly jump into the \u0026ldquo;STEP 02\u0026rdquo; If you have already installed Docker Engine on your host. In the 1st step, I\u0026rsquo;m setting up my Ubuntu host to run the Docker container.\nNote: You can skip this section and jump to \u0026ldquo;STEP 02\u0026rdquo; if you already installed docker on your HOST. Or else you are going to do this on a Cloud Platform.\nInstall Docker Engine Setup The Repository Update Ubuntu once.\nsudo apt-get update -y Install Additionally Required Packages. sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common Install Official Trusted GPG Key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add Docker Stable Repository sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; Update OS sudo apt-get update -y Install Docker Docker Engine sudo apt-get install docker-ce docker-ce-cli containerd.io -y Start and Enable Docker Daemon Service sudo systemctl enable docker.service sudo systemctl start docker.service For Production usage, always use official, Long-Term Support(LTS) releases. But, the official Docker image doesn\u0026rsquo;t come with docker CLI inside the Jenkins image.\nInstall Docker Compose Download Docker Compose Binary sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose Provide executable permissions sudo chmod +x /usr/local/bin/docker-compose Set the binary executable path sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose Verify Installation docker-compose --version STEP 02: Create Docker Bridge Network First, I\u0026rsquo;m going to create a bridge network named \u0026ldquo;jeknins-net\u0026rdquo;. And I will attach all docker containers to this network.\nsudo docker network create jenkins-net STEP 03: Create Docker Volumes Here we are going to create volumes and map them into Jenkins container. We may need these volumes to store the Jenkins data persistently. This volume use to make sure you don\u0026rsquo;t lose your Jenkins data even after a reboot or crash container situations.\nI will create a Docker volume to store the \u0026ldquo;JENKINS_HOME\u0026rdquo; directory, which use to mount into the docker host machine.\nDocker Host Jenkins Container jenkins-data /var/jenkins_home sudo docker volume create jenkins-docker-data STEP 04: Start Jenkins Server Container In this step, we will use the same bridged network created as \u0026ldquo;jenkins-net\u0026rdquo; at STEP 02. Basically, Docker-Agent(Dind) and Jenkins Servier will be on the same Docker network.\nsudo docker container run --name jenkins-server --detach --restart unless-stopped --network jenkins-net --hostname jenkins --publish 8080:8080 --volume jenkins-data:/var jenkins_home jenkins/jenkins:lts If you were wondering what the arguments stand for, here is what each means:\n-d: detached mode\n-v: attach volume\n-p: assign port target\n—name: Name of the container\nTCP port 8080 is using to access the Jenkins dashboard. Instead of the default port 8080, we can use any other uncommon TCP port as well.\nIn the above command, we can see \u0026ldquo;\u0026ndash;volume\u0026rdquo; is the most crucial argument which helps docker link the volume into the docker container and preserve data persistency. The jenkins docker container path \u0026ldquo;/var/jenkins_home\u0026rdquo; where the Jenkins container is storing the data. You don\u0026rsquo;t need to worry about the docker container to restart or even after a container crashed. You can easily mount this volume into the new container in such a situation.\nFinally, You can see them out like this.\ndimuthu@svr05:~$ sudo docker container ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b45233ebcbba jenkins/jenkins:lts \u0026#34;/sbin/tini -- /usr/…\u0026#34; 7 seconds ago Up 5 seconds 0.0.0.0:8080-\u0026gt;8080/tcp, 50000/tcp jenkins-server STEP 05: Access Jenkins Console Access your Jenkins Server through the web browser\nhttp://\u0026lt;DOCKER-HOST-IP\u0026gt;:8080/ STEP 06: Get Jenkins Init Password In the post configuration wizard, It will be asking for an administrator password. So, You need to access the running Jenkins container to get this password. You can get the initial default Administrator password using the following command.\nYou can use Jenkins container ID or name and the following command. According to my setup, Jenkins-server is our Jenkins docker container name. And copy the output and paste it into the text box.\nsudo docker exec jenkins-server cat /var/jenkins_home/secrets/initialAdminPassword 02ac16bd27af4e8c8d1ffc82841f419f Or else you can use the following command. Replace your container name or ID.\ndocker container exec [CONTAINER ID or NAME] sh -c \u0026#34;cat /var/jenkins_home/secrets/initialAdminPassword\u0026#34; STEP 07: Install Jenkins Plugins As usually, We can install the required plugins from here. In my case, I\u0026rsquo;ll choose the \u0026ldquo;install suggested plugin\u0026rdquo; options. And please wait until Installation completed.\nSTEP 08: Create First Admin User Create an administrator user providing your details here.\nSTEP 09: Instance configuration After creating the admin user, next move on to set up the Instance configuration. Since you need to access Jenkins internally, leave the URL to your localhost URL. Either you can provide your FQDN or IP address. If you not sure, go to this URL as it is.\nNow, the Jenkins server has been set up successfully. Now, I\u0026rsquo;m going to check the data persistency.\nSTEP 10: Confirm Jenkins Data Persistency Once you logged into the Jenkins dashboard, I will create a sample job to whether Jenkins is working fine.\nNow head-over to Jenkins dashboard and click on \u0026ldquo;New Item.\u0026rdquo;\nOn the next screen, enter a job name. In this case, I\u0026rsquo;m naming it HelloWorld. and then choose the \u0026ldquo;Freestyle\u0026rdquo; project option.\nThen, In the next screen, move on to the \u0026ldquo;Build\u0026rdquo; tab and click on the \u0026ldquo;Add Build Step\u0026rdquo; button and choose the \u0026ldquo;Execute Shell\u0026rdquo; option.\nThen save the job and click on the \u0026ldquo;Build Now\u0026rdquo; button to start the job.\nClick on the Build Status (blue ball) under Build History (left sidebar) to view the console output. You should see that our command ran with no problems.\nOther than the step that I\u0026rsquo;ve mentioned above, there so many ways to create build jobs. That\u0026rsquo;s what makes Jenkins such a fantastic continuous deployment tool.\nSection 02. Run Jenkins on a Docker Container Using Docker Compose ( Make things faster and reusable) Run Jenkins on Docker Using Docker-Compose In the previous sections, I\u0026rsquo;ve created a docker container that runs the Jenkins instance. Docker containers are ephemeral. These containers can be stopped or destroyed and build new once anytime using docker images. If something terrible happens, it may cause Jenkins data losses too.\nNow, I\u0026rsquo;m going to do the same setup using docker-compose with persistent data volume mounts.\nWhere is the Jenkins Data in Docker Container? It\u0026rsquo;s always better to understand where the Jenkins data stored. To list Jenkins data, you can use the following command.\ndocker exec jenkins-server ls -l /var/jenkins_home In here, \u0026ldquo;jenkins-server\u0026rdquo; is my Jenkins container name. So, You can replace it with your container name. After executing the command, You can see what the output looks like below.\ntotal 108 -rw-r--r-- 1 jenkins jenkins 1647 Mar 7 14:23 config.xml -rw-r--r-- 1 jenkins jenkins 53 Mar 7 14:01 copy_reference_file.log -rw-r--r-- 1 jenkins jenkins 156 Mar 7 14:01 hudson.model.UpdateCenter.xml -rw-r--r-- 1 jenkins jenkins 370 Mar 7 14:18 hudson.plugins.git.GitTool.xml -rw------- 1 jenkins jenkins 1712 Mar 7 14:01 identity.key.enc -rw-r--r-- 1 jenkins jenkins 7 Mar 7 14:23 jenkins.install.InstallUtil.lastExecVersion -rw-r--r-- 1 jenkins jenkins 7 Mar 7 14:23 jenkins.install.UpgradeWizard.state -rw-r--r-- 1 jenkins jenkins 183 Mar 7 14:23 jenkins.model.JenkinsLocationConfiguration.xml -rw-r--r-- 1 jenkins jenkins 171 Mar 7 14:01 jenkins.telemetry.Correlator.xml drwxr-xr-x 3 jenkins jenkins 4096 Mar 7 16:21 jobs drwxr-xr-x 3 jenkins jenkins 4096 Mar 7 14:01 logs -rw-r--r-- 1 jenkins jenkins 907 Mar 7 14:01 nodeMonitors.xml drwxr-xr-x 2 jenkins jenkins 4096 Mar 7 14:01 nodes drwxr-xr-x 79 jenkins jenkins 12288 Mar 7 14:18 plugins -rw-r--r-- 1 jenkins jenkins 129 Mar 7 16:34 queue.xml -rw-r--r-- 1 jenkins jenkins 64 Mar 7 14:01 secret.key -rw-r--r-- 1 jenkins jenkins 0 Mar 7 14:01 secret.key.not-so-secret drwx------ 4 jenkins jenkins 4096 Mar 7 16:35 secrets -rw-rw-r-- 1 root root 7152 Feb 10 17:58 tini_pub.gpg drwxr-xr-x 2 jenkins jenkins 4096 Mar 7 14:18 updates drwxr-xr-x 2 jenkins jenkins 4096 Mar 7 14:01 userContent drwxr-xr-x 3 jenkins jenkins 4096 Mar 7 14:23 users drwxr-xr-x 11 jenkins jenkins 4096 Mar 7 14:01 war drwxr-xr-x 2 jenkins jenkins 4096 Mar 7 14:18 workflow-libs drwxr-xr-x 3 jenkins jenkins 4096 Mar 7 16:33 workspace Where is the Jenkins Data in Docker Host? As we did before, Our Jenkins container\u0026rsquo;s \u0026ldquo;JENKINS_HOME\u0026rdquo; directory is mounted to the \u0026ldquo;jenkins_home\u0026rdquo; directory in our Docker host machine.\nYou can find your docker volume using the command below.\ndimuthu@srv01:~/$ docker volume inspect jenkins-data [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2021-03-07T16:34:42Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/jenkins-data/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;jenkins-data\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] According to the output, You can see the \u0026ldquo;JENKINS_HOME\u0026rdquo; directory is mounted on \u0026ldquo;/var/lib/docker/volumes/jenkins-data/_data\u0026rdquo; docker volume. This means your data is persistent under that directory, and it is available even container is deleted. So, data will remain on the Docker host.\nSTEP 03: Create Docker-Compose.YAML Another best way to run Jenkins is by using the docker-compose command. By using docker-compose. yaml, you can deploy multiple Jenkins containers more quickly.\nOpen your favourite text editor and create a new file named \u0026ldquo;docker-compose.yaml\u0026rdquo;, and paste the below YAML manifest into the file. And the save and exit.\nsudo vim docker-compose.yml version: \u0026#34;3.9\u0026#34; services: jenkins: image: jenkins/jenkins:lts container_name: jenkins-server privileged: true hostname: jenkinsserver user: root labels: com.example.description: \u0026#34;Jenkins-Server by DigitalAvenue.dev\u0026#34; ports: - \u0026#34;8080:8080\u0026#34; - \u0026#34;50000:50000\u0026#34; networks: jenkins-net: aliases: - jenkins-net volumes: - jenkins-data:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock volumes: jenkins-data: networks: jenkins-net: You can change this docker-compose file as you want. But, here I\u0026rsquo;ll explain the most essential sections only.\nimage = The base image name used to create a docker instance. Generally, this will be pulled from the docker hub docker registry.\nPorts = This defines port mapped between docker container and the docker host machine\nvolume = This defines container data storage volume mapped between the docker container and the docker host machine. This allows accessing containerized data from the docker host.\nContainer_name = This defines the name of the container that you are going to spin up. In here, I\u0026rsquo;m using \u0026ldquo;jenkins-server\u0026rdquo; as my container name.\n04: Run Docker Container Using Docker-Compose In Detached Mode docker-compose up -d 05: Access Jenkins Console http://\u0026lt;YOUR-IP-OR-FQDN\u0026gt;:8080/\n06: Get Init Administrator Password docker exec jenkins-server cat /var/jenkins_home/secrets/initialAdminPassword Summary In this article, you learned to deploy Jenkins using imperatively and declaratively. You also learned how to get your hands dirty with docker volumes and how to mount them.\nYou also learned how to preserve container data on docker volumes by mounting docker volume into a docker container. Making the Jenkins settings persistent and consistent even if the Docker container is deleted.\nIf you are facing issues with the implementation, please comment below. I will regularly reply here.\n","date":"March 7, 2021","image":"https://digitalavenue.dev/img/post-imgs/jenkins-docker-per-vol/Jenkins-Docker-Thumb_hu938ee202d66bc186ecfdeac038bc3248_308056_650x0_resize_q90_box.jpg","permalink":"/blog/2021-03-07-run-jenkins-on-docker-compose/","title":"Run Jenkins In Docker Using Docker Compose With Persistent Volumes"},{"categories":["DevOps"],"contents":"How To Integrate SonarQube With Jenkins For Code Analysis I want to ensure the quality of the code, identify bugs, code vulnerabilities, code smells, and align with code standards after committing codes into repositories such as Github and Gitlab. And the same way build my code automatically, using Jenkins. I want to perform this task whenever I commit code and see the static code analysis report at SonarQube.\nIn this case, GitLab-Jenkins-SonarQube integration comes to play. In this tutorial, I\u0026rsquo;m going to demonstrate how to integrate SonarQube with the Jenkins server.\nWork Flow - How It Goes? Developer commit code changes to the GitLab/GitHub. Then, the Jenkins server will fetch/pull code changes from Git repository and do a static code analysis using Sonar-Scanner and send analysis reports to SonarQube server. Finally, Jenkins build the project code.\nBefore You Begin !!! I Assume\u0026hellip;\nYou have a Pre-Configured Jenkins server If not? Refer to my other article to complete Jenkins Installation\nYou have a Pre-Configured SonarQube server If not? Refer to this article to complete SonarQube Installation\nYou have a GitLab/GitHub account with a developer role. If not? Refer to this article to complete GitLab-Jenkins integration\nYou have integrated GitLab/GitHub with Jenkins server\nSTEP 01: Generate User Token Log in to SonarQube Server and go-to the \u0026ldquo;My Account\u0026rdquo; section on your profile. And move to the \u0026ldquo;Security\u0026rdquo; tab. Then, Generate a \u0026ldquo;User Access Token.\u0026rdquo;\nLogin \u0026gt; Profile \u0026gt; My Account \u0026gt; Security \u0026gt; Generate Token\nYou need to copy \u0026amp; save this code immediately. This code won\u0026rsquo;t be able to see again. It shows only once.\nJenkins-Auth-Token : 7a09705df7d034b99459b5127303a8315f5bdf6d\nSTEP 02: Install Sonar-Scanner on Jenkins Let\u0026rsquo;s move on to your Jenkins server and install the following plugins.\nSonarQube Scanner for Jenkins\nPlain Credentials Plugin\nCredentials Plugin\nManage Jenkins \u0026gt; Manage Plugins \u0026gt; Available [TAB] \u0026gt; Search For SonarQube \u0026gt; Install Plugins\nRestart once plugins installed on the Jenkins server.\nSTEP 03: Add SonarQube Authentication Token Into Jenkins Head-over to Jenkins server and go-to Jenkins \u0026gt; Credentials \u0026gt; System \u0026gt; Global Credentials \u0026gt; Add Credentials\nKind: Secret test\nSecret: SonarQube Authentication Token\nDescription: Provide a descriptive name\nClick OK to add new credentials.\nSTEP 04: Add SonarQube Server on Jenkins Now, We need to add SonarQube server settings into Jenkins.\nManage Jenkins \u0026gt; Configure System \u0026gt; SonarQube servers [Scrol Down]\nAdd the following settings in the \u0026ldquo;SonarQube server\u0026rdquo; section.\nEnable: Enable injection of SonarQube server configuration as build environment variables\nName: Provide a descriptive name for the connection.\nServer URL: Provide your SonarQube server URL with the port number.\nServer Authentication Token: Select credentials that we added previously as step 02.\nApply \u0026amp; Save.\nSTEP 05: Add Sonar-Scanner For Jenkins Goto Jenkins \u0026gt; Manage Jenkins \u0026gt; Global Tool Configuration \u0026gt; SonarQube Scanner [Scrol Down] \u0026gt; Add SonarQube-Scanner\nNow, We have two options. Either we can install automatically or manually. If you install a specific version manually, you need to define the \u0026ldquo;SONAR_RUNNER_HOME\u0026rdquo; path manually.\nIn this case, I\u0026rsquo;m going to install it automatically.\nName: Sonar Scanner 4\nInstall Automatically: Enabled\nVersion: Select a version\nFinally, Save \u0026amp; Apply changes.\nSTEP 05: Create a Jenkins Job Create a new freestyle project and do the following configurations.\nGo-to Jenkins \u0026gt; New Item \u0026gt; Enter Project Name \u0026gt; Select Freestyle Project \u0026gt; OK\nFill-out details on the general section\nHere I have used GitLab for my source code management task. Here you can provide your own Github/GitLab repository URL and SSH Key for GitLab, as shown in my \u0026ldquo;GitLab integration with Jenkins\u0026rdquo; tutorial.\nRefer to this article to know how to use SSH key authentic with Gitlab.\nREF: https://digitalave.github.io/spring/2020/05/09/GitLab-Integration-with-Jenkins.html Alternatively, You also can directly enter your GitLab username and password in Jenkins \u0026gt; Credentials \u0026gt; System \u0026gt; Global Credentials \u0026gt; Add Credentials \u0026gt; Select \u0026ldquo;Username Password\u0026rdquo; from the drop-down menu as the option for \u0026ldquo;Kind.\u0026rdquo;\nNow, Let\u0026rsquo;s move on to the \u0026ldquo;Build\u0026rdquo; section, and click \u0026ldquo;Add build step\u0026rdquo; and select the \u0026ldquo;Execute Sonar Scanner\u0026rdquo; option.\nBuild \u0026gt; Add Build Step \u0026gt; Execute Sonar Scanner\nThe task to run: Define a name for Scanner\nJDK: Leave it to default or set your own JAVA\nPath to project properties: In here, you can define the sonar-project. properties file location. [Optional]\nAnalysis properties: Define Analysis Properties\nsonar.projectBaseDir=/var/lib/jenkins/workspace/{YOUR_PRJECT_DIRECTORY} sonar.language={YOUR LANGUAGE} sonar.login={SONARQUBE_API_TOKEN} sonar.projectVersion=1.0 sonar.sources=. sonar.verbose=true sonar.projectKey={PROJECT_NAME} sonar.host.url={SONARQUBE_URL:PORT/DNS_NAME} sonar.projectName={PROJECT_NAME} sonar.sourceEncoding=UTF-8 sonar.project.settings=/var/lib/jenkins/workspace/{PROJECT_NAME}/sonar-project.properties sonar.analysis.mode=publish sonar.buildbreaker.skip=true Save \u0026amp; Apply the changes you made. Now, the Rest of the configuration has been completed. Now, Head Over to your project home on Jenkins and run the \u0026ldquo;Build Now\u0026rdquo; button.\nNow, Go to console output will show you a long list, and you\u0026rsquo;ll see \u0026ldquo;EXECUTION SUCCESS\u0026rdquo; status.\nGreat, Now head over to your SonarQube server. And you\u0026rsquo;ll see the analysis report for your newly built project.\nTroubleshooting Tips:\nIssue: Unable To Load Component Class \u0026mdash; Project.lock Report Task GitLab Errors Resolution: Remove GitLab Plugin from SonarQube Server\nhttps://github.com/adnovum/sonar-build-breaker https://github.com/gabrie-allaigre/sonar-gitlab-plugin ","date":"May 11, 2020","image":"https://digitalavenue.dev/img/post-imgs/Sonar-Jenkins/SonarQube+Jenkins_hu0f422448cb5abe666d27b9a9fe5ef971_609680_650x0_resize_box_3.png","permalink":"/blog/2020-05-11-sonarqube-integration-with-jenkins/","title":"How To Integrate SonarQube With Jenkins For Code Analysis"},{"categories":["DevOps"],"contents":" Before You Begin In this tutorial, I\u0026rsquo;m going to demonstrate to you how to integrate GitLab with Jenkins Server. I suppose You\u0026rsquo;ve already installed Jenkins on your own. If you haven\u0026rsquo;t Jenkins server, Please refer to my previous video and article. I\u0026rsquo;ll put the link in the description.\nREF: https://digitalavenue.dev/How-To-Install-and-configure-Jenkins-on-Ubuntu-18.04/ How To Install and Configure Jenkins : https://digitalave.github.io/spring/2020/04/06/How-To-Install-and-configure-Jenkins-on-Ubuntu-18.04.html If you don\u0026rsquo;t have a GitLab account, Please create an account.\nREF: https://gitlab.com/ Prerequisites : Pre-Configured Jenkins Server\nGitLab Account With Developer Role Permission\nIntroduction : Jenkins is an open-source software development platform enriched with continuous integration (CI) and many more DevOps automation capabilities.\nOrganizations using Jenkins to build and deploy applications and integrate with GitLab for other DevOps tools.\nGitLab - Jenkins integration allows you to build and deploy an application on Jenkins and reflect the output on the GitLab UI more convenience\nIntegration GitLab and Jenkins allows you to trigger a Jenkins build when a code is pushed to a repository or when a merge request is created.\nSTEP 01: Install GitLab Plugins on Jenkins Server Go to the Manage Plugin section, then search and install the following plugins on your Jenkins server.\n\u0026quot;Git\u0026quot;\n\u0026quot;GitLab Plugin\u0026quot;\n\u0026quot;GitLab API Plugin\u0026quot;\n\u0026quot;Credentials Plugin\u0026quot;\n\u0026quot;GitLab Authentication Plugin\u0026quot;\nManage Jenkins \u0026gt; Manage Plugins \u0026gt; Available Plugin [Search] \u0026gt; Install and Restart\nSTEP 02: Create a New GitLab User / Promote Existing GitLab User Create a new GitLab user with \u0026ldquo;Developer\u0026rdquo; role permission and grant access to each repository/project you want to integrate with Jenkins\nNote: User should have Developer Permission Role\nSTEP 03: Create Personal Access Token on GitLab Now, Let\u0026rsquo;s head over to the GitLab account and move to your profile setting section.\nUser Settings \u0026gt; Access Tokens\nCreate a new Personal Access Token for Jenkins authentication.\nName: Provide Token Name\nScope: API - Which Grant access to GitLab resources such a Projects, Groups and Registries.\nSave the deployed token somewhere safe. Once you leave or refresh the page, you won\u0026rsquo;t be able to reaccess it.\nNOTE: Once you\u0026rsquo;ve generated a Personal Access Token, copy it and save it in a separate file immediately.Because Token only visible only once.\nJenkins-GitLAB_API-Access : i5Jfi8a5foEFoC9CkVzl\nSTEP 04: Add GitLab Personal Access Token to Jenkins Again, Head-over to the Jenkins server and Then, We need to add an authentication token into the Jenkins server.\nJenkins \u0026gt; Credentials \u0026gt; System \u0026gt; Global Credentials \u0026gt; Add Credentials\nScroll down a little and click on \u0026ldquo;Global Credentials\u0026rdquo; under the Domain column and \u0026ldquo;Add Credentials\u0026rdquo;.\nSelect the following options for the \u0026ldquo;Global Credentials\u0026rdquo; section.\nKind: GitLab API Token\nAPI Token: Add Previously Generated Personal Access Token\nDescription: Provide a Descriptive Name\nSTEP 05: Configure GitLab API Settings on Jenkins Server Let\u0026rsquo;s move on to \u0026ldquo;GitLab\u0026rdquo; configuration section in Manage Jenkins \u0026gt; Configure System.\nManage Jenkins \u0026gt; Configure System \u0026gt; \u0026ldquo;GitLAB\u0026rdquo; Configuration Section.\nThen, Configure the following entries\u0026hellip;\nEnable authentication for \u0026lsquo;/project\u0026rsquo; end-point : Enable\nConnection Name: Provide a Descriptive Name\nGitLab host URL: GitLab server URL\nCredentials: Select previously added credentials from the drop-down menu.\nFinally, check whether the connection is successful by pressing \u0026ldquo;Test Connection\u0026rdquo; button. If the connection is successful. Then move to the next step.\nNow, the Connection between Jenkins and GitLab is OK. GitLab API plugin used to access for Jenkins to get metadata from GitLab.\nBut, We need to have an SSH key authentication to commit changes from Jenkins to GitLab.\nSTEP 06: Allow GitLab Build Commit Push-Pull Authentication To GitLab Two Methods : 1. Using SSH public-Private Key Pair Create SSH Key Pair on Jenkins Server Login to Jenkins host terminal console and switch to \u0026ldquo;jenkins\u0026rdquo; user and move to Jenkins home directory. And generate an SSH-Key-Pair.\nroot@SRV3:~# su - jenkins jenkins@SRV3:~$ cd /var/lib/jenkins/ jenkins@SRV3:~$ ssh-keygen Generating public/private rsa key pair. jenkins@SRV3:~$ cd /var/lib/jenkins/.ssh/ jenkins@SRV3:~/.ssh$ ls id_rsa id_rsa.pub known_hosts Open \u0026ldquo;id_rsa.pub\u0026rdquo; file and copy the content.\nProvide Public Key To GitLab Go to GitLab and deploy a new ssh key. Go to your GitLab profile \u0026ldquo;Setting\u0026rdquo; and then go to \u0026ldquo;SSH Keys\u0026rdquo; section.\nPaste code that we copied content from \u0026ldquo;id_rsa.pub\u0026rdquo; file.\nNow, a Public key has been added to the GitLab server.\nProvide Private Key To Jenkins Then, We need to add our private key to the Jenkins server.\nOpen \u0026ldquo;/var/lib/jenkins/.ssh/id_rsa\u0026rdquo; file and copy content from private key.\nvim /var/lib/jenkins/.ssh/id_rsa Go to Jenkins \u0026gt; Credentials \u0026gt; System \u0026gt; Global Credentials \u0026gt; Add Credentials\nClick the \u0026ldquo;Add\u0026rdquo; button next to the \u0026ldquo;Credentials\u0026rdquo; field and select the \u0026ldquo;Jenkins\u0026rdquo; option. In the resulting dialogue, select \u0026ldquo;SSH Username with private key\u0026rdquo; as the credential typeset the \u0026ldquo;Username\u0026rdquo; to git, and enter the content of the private key selected for use between GitLab and Jenkins.\nKind: SSH Username with private key\nDescription: Provide descriptive name\n** Username: git**\nPrivate Key: Enter private key\nRemember that you already attached the corresponding public key to your GitLab profile in step in the previous step.\nAdd copied public key content into \u0026ldquo;Key\u0026rdquo; section.\n2. Using Plain Credentials - Username:Password STEP 07: Push Local Project To GitLab - Optional Step Now, I\u0026rsquo;m going to push my local project into the GitLab repository.\nOpen Your Command Prompts / Or Use Your Own Method\nC:\\Users\\Dimuthu\u0026gt;git config --global user.name \u0026#34;Dimuthu Daundasekara\u0026#34; C:\\Users\\Dimuthu\u0026gt;git config --global user.email \u0026#34;dimuthu@gmail.com\u0026#34; C:\\Users\\Dimuthu\u0026gt;cd C:\\YoutubeDownloader-master\\YoutubeDownloader C:\\YoutubeDownloader-master\\YoutubeDownloader\u0026gt;git init C:\\YoutubeDownloader-master\\YoutubeDownloader\u0026gt;git remote add origin git@gitlab.com:dimuit86/youtubedownloader.git C:\\YoutubeDownloader\u0026gt;git add . C:\\YoutubeDownloader-master\\YoutubeDownloader\u0026gt;git commit -m \u0026#34;Initial commit\u0026#34; C:\\YoutubeDownloader\u0026gt;git config credential.helper store C:\\YoutubeDownloader-master\\YoutubeDownloader\u0026gt;git push https://gitlab.com/dimuit86/youtubedownloader.git STEP 07: Configure Jenkins Project Create FreeStyle Project on Jenkins Now, It\u0026rsquo;s time to create a new project and do further configuration.\nJenkins \u0026gt; New Item \u0026gt; FreeStyle Project\nGive a name to the project and continue.\nNow, Head-over to the \u0026ldquo;source code management\u0026rdquo; section and select \u0026ldquo;Git\u0026rdquo;.\nRepository URL : git@gitlab.com :dimuit86/shadowsocksx-ng-develop.git\nCredentials: Select added credentials from drop-down menu\nAttach new credentials to the SSH URL for the GitLab repository.\nOptional: You also can use the plain Username and password that we have added to the method (2).\nOn the same configuration page, find the \u0026ldquo;Build Triggers\u0026rdquo; section and check the option to \u0026ldquo;Build when a change is pushed to GitLab\u0026rdquo;.\nFinally, Apply and Save Changes\nNow, GitLab integration with Jenkins has been completed. Now, We can check the connectivity, using the build button and checking console logs.\nVoilà, It\u0026rsquo;s working.\n","date":"May 9, 2020","image":"https://digitalavenue.dev/img/post-imgs/GitLab_Jenkins/gitlab-jenkins_hu751b01a4d8dd53a43beb7bd9dc9e9630_40318_650x0_resize_q90_box.jpg","permalink":"/blog/2020-05-09-gitlab-integration-with-jenkins/","title":"How To Integrate GitLab With Jenkins"},{"categories":["DevOps"],"contents":" Introduction: SonarQube is an open-source tool that can be used to analyze the quality of the source code. It can detect your code bugs, vulnerabilities, security black holes, and code smells. SonarQube empowers you to write cleaner and safer codes without breaking standards and code methodologies.\nSonarQube is bundled with a static code analyzer for more than 27 programming languages. SonarQube performs continues code inspection using thousands of automated static code analysis rules.\nWe can perform code analysis manually or integrate with CICD DevOps tools such as Jenkins, Azure DevOps and Bamboo.\nAnd, also you can integrate SonarQube with your IDE tools such as Visual Studio and Eclipse.\nSonarQube provides code reliability by preventing bugs and application security by fixing vulnerabilities that compromise your code.\nSonarQube is an open-source platform. Which uses for static code analysis and continuous inspection of code quality. SonarQube can detect bugs, code smells and security vulnerabilities.SonarQube empowers developers to write cleaner and safer code.\nSonarQube provides code reliability by preventing bugs and application security by fixing vulnerabilities that compromise your code.\nSonarQube can integrate with CI/CD tools such as Jenkins, Azure DevOps, GitHub, GitLab, Bitbucket and many more.\nFeatures: Build Integration - Jenkins, Azure DevOps, Bamboo, etc\u0026hellip;\nIDE Integration - Visual Studio, Eclips, InteliJ, etc\u0026hellip;\nOther Pipeline Integration\nPrerequisites: OS - Ubuntu 18.04 / 16.04 LTS / Debian\nRAM - 4GB Minimum RAM\nCPU - 1vCPU\nJAVA - Oracle JRE 11 or OpenJDK 11\nNOTE: Please make sure to install a compatible Java version before continuing the installation.\nREF: https://docs.sonarqube.org/latest/requirements/requirements/ In this tutorial, I will be going to install SonarQube Community Edition v8.3 on Ubuntu 18.04. Which required OpenJDK 11 packages to be installed on the system.\nSonarQube 8.3 OpenJDK 11 PostgreSQL 12\nSTEP 01: Set kernel Parameters and System Limits First of all, we need to perform some OS-level modifications to \u0026ldquo;Kernel Parameters\u0026rdquo; and \u0026ldquo;System limits.\u0026rdquo;\nAppend these entries to the bottom of the \u0026ldquo;sysctl.conf\u0026rdquo; file.\nsudo vim /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 And, also append these entries at the end of the \u0026ldquo;limits.conf\u0026rdquo; file.\nsudo vim /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 4096 Make sure to reboot systems once the above changes made. Therefore New changes will reflect after the reboot.\nSTEP 02: Install OpenJDK 11 Download and Install JDK 11 APT Repositories\nNow, It\u0026rsquo;s time to install java on your system. Don\u0026rsquo;t forget to install a compatible Java version with your SonarQube version.\nFirst, perform a system update.\nsudo apt-get update -y Then, Install OpenJDK 11\nsudo apt-get install openjdk-11-jdk -y Set Default JDK Version\nThen, You need to set the newly installed Java version as your default Java version.\nsudo update-alternatives --config java Verify Install Java Version\njava -version STEP 02: Install and Configure PostgreSQL Database for SonarQube In this tutorial, I\u0026rsquo;m using PostgreSQL as my database engine. You also can use other compatible DB such as MySQL or Oracle.\nIt\u0026rsquo;s always better to check the version compatibility matrix, which recommends by SonarQube developers.\nREF: https://docs.sonarqube.org/latest/requirements/requirements/ Let\u0026rsquo;s do a system update again.\nsudo apt update Import Trusted PGP Key and PostgreSQL APT Repo\nThen, Install a trusted GPG key on your system. And create a repository file for PostgreSQL.\nwget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c \u0026#39;echo \u0026#34;deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/pgdg.list\u0026#39; Install PostgreSQL\nLet\u0026rsquo;s install PostgreSQL on your system.\nsudo apt install postgresql postgresql-contrib Check PostgreSQL Version\nsudo -u postgres psql -c \u0026#34;SELECT version();\u0026#34; Enable and Start PostgreSQL Service\nEnable and start service to be able to start at the system boots up.\nsudo systemctl enable postgresql. service sudo systemctl start postgresql. service Change PostgreSQL default user password\nChange default PostgreSQL password and set a new password.\nsudo passwd postgres Switch to PostgreSQL User\nNow, Switch into \u0026ldquo;postgres\u0026rdquo; user.\nsu - postgres Create New User \u0026ldquo;sonar\u0026rdquo;\nCreate a new database user named \u0026ldquo;sonar\u0026rdquo;.\ncreateuser sonar Log Into PostgreSQL Shell\nNow, log in to the PostgreSQL database shell.\npsql Set Password for SonarQube Database User \u0026ldquo;sonar\u0026rdquo;\nAnd, Then set a password for the database user \u0026ldquo;sonar\u0026rdquo;\nALTER USER sonar WITH ENCRYPTED PASSWORD \u0026#39;p@ssw0rd\u0026#39;; Create New Database \u0026ldquo;sonarqube\u0026rdquo;\nCreate a new database named \u0026ldquo;sonarqube.\u0026rdquo;\nCREATE DATABASE sonarqube OWNER sonar; Grant Privileges to \u0026ldquo;sonar\u0026rdquo; User on \u0026ldquo;sonarqube\u0026rdquo; Database\nNow, Grant all privileges to that user and database.\nGRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar; Exit From PostgreSQL Shell\nq Exit From \u0026ldquo;postgres\u0026rdquo; User\nexit Restart and recheck PostgreSQL DB Service Status\nEnable PostgreSQL service to be able to start automatically at systems boots-up.\nsystemctl restart postgresql systemctl status -l postgresql Now Check whether PostgreSQL is listing on default port \u0026ldquo;5432\u0026rdquo;\nnetstat -tulpena | grep postgres STEP 03: Download and Install SonarQube Now, It\u0026rsquo;s time to download the SonarQube binary archive file and extract it on our installation directory.\nDownload SonarQube Archive File\nREF: https://binaries.sonarsource.com/Distribution/sonarqube/ Now, Let\u0026rsquo;s create a temporary directory and download the SonarQube archive file.\nsudo mkdir /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip Additionally, you may need to install the \u0026ldquo;zip\u0026rdquo; apt package if not available in your system.\nsudo apt-get install zip Extract your downloaded archive into /opt/ directory.\nsudo unzip sonarqube-8.3.0.34182.zip -d /opt/ Move Extracted setup into /opt/sonarqube/ directory\nsudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube STEP 04: Create Group and User for SonarQube Now, We need to create a system user and group for the SonarQube service.\nCreate a group named \u0026ldquo;sonar\u0026rdquo;\nFirst, create a system group named \u0026ldquo;sonar.\u0026rdquo;\nsudo groupadd sonar Create a user named \u0026ldquo;sonar\u0026rdquo; and into the \u0026ldquo;sonar\u0026rdquo; group with directory access\nThen, create a user and add the user into the group with directory permission to the /opt/ directory.\nsudo useradd -c \u0026#34;SonarQube - User\u0026#34; -d /opt/sonarqube/ -g sonar sonar Provide user and group directory ownership to \u0026ldquo;/opt/sonarqube/\u0026rdquo;****\nsudo chown sonar:sonar /opt/sonarqube/ -R STEP 05: Configure SonarQube Now, Let\u0026rsquo;s head over to the \u0026ldquo;sonar.properties\u0026rdquo; configuration file and do the following changes.\nsudo vim /opt/sonarqube/conf/sonar.properties UnComment and type PostgreSQL database username and password that we\u0026rsquo;ve created at the previous step.\nNow, We need to point our PostgreSQL database to the SonarQube service. We are using \u0026ldquo;localhost\u0026rdquo; as a DB host since we\u0026rsquo;ve installed PostgreSQL on the same server.\nUn-comment these lines and modify them as necessary.\nsonar.jdbc.username=sonar sonar.jdbc.password=p@ssw0rd sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError ########### OPTIONAL USE ONLY ############# sonar.jdbc.username=sonar sonar.jdbc.password=sonar sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=127.0.0.1 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs ########################################### STEP 06: Configure Systemd Service For SonarQube Now, Create a startup script for the SonarQube service that starts at system boots.\nCreate a systemd service file for SonarQube to be run at system startup.\nvim /etc/systemd/system/sonarqube.service Add this content into the \u0026ldquo;sonarqube.service\u0026rdquo; file.\n[Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target Enable and Start SonarQube Service\nsystemctl daemon-reload systemctl enable sonarqube. service systemctl start sonarqube. service systemctl status -l sonarqube. service After sometime later, Check whether the port is listening.\nnetstat -tulpena | grep 9000 STEP 07: Configure NGINX Reverse Proxy For SonarQube Install NGINX Package\nNow we need to expose our SonarQube server outside as it is listening only on localhost. Therefore we are creating an Nginx reverse proxy to redirect outside traffic into the SonarQube.\napt-get install nginx -y Goto /etc/nginx/nginx.conf and un-comment these two lines\nvim /etc/nginx/nginx.conf include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; Create NGINX Configuration File For SonarQube\nCreate a reverse proxy configuration file\nsudo vim /etc/nginx/sites-enabled/sonarqube.conf\nCopy and paste this virtual-host server block and change the \u0026ldquo;server_name\u0026rdquo; entry as you required.\nserver{ listen 80; server_name sonarqube.da.com; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } Check NGINX configurations\nnginx -t\nEnable and Restart Nginx Service\nsystemctl enable nginx.service systemctl restart nginx.service systemctl status -l nginx.service Check whether port 80 listenings for connections\nnetstat -tulpena | grep 80 STEP 08: Firewall Configuration Allow TCP ports 9000, 9001, 80 through the firewall\nsudo ufw allow 80,9000,9001/tcp sudo ufw status STEP 09: Access SonarQube Through Web Browser Now, SonarQube installation and configuration has been completed. It\u0026rsquo;s time to access the web console through the web browser.\nProvide the default administrator account username and password as admin/admin\nDefault Username: admin\nDefault Password: admin\nhttp://172.25.10.10/ OR http://YOUR-SERVER-IP\nTorubleshooting TIPS Sometime SonaqQube will not start as we expected. Most of the time, the reason is related to elasticsearch service. SonarQube uses elasticsearch as it\u0026rsquo;s indexing engine. So, We may need to troubleshoot elasticsearch as well.\nHere are some troubleshooting tips:\nSonarQube stores its service logs under \u0026ldquo;/opt/sonarqube/logs\u0026rdquo; directory. You may need those log files in case of troubleshooting purpose.\nTroubleshooting Tips: Log Paths\n/opt/sonarqube/logs/es.log\n/opt/sonarqube/logs/sonar.log\n/opt/sonarqube/logs/web.log\nTroubleshooting Tips: JVM OPTION and HEAP MEMORY ISSUES\nAdditionally, you may be required to modify some entries related to elasticsearch and JVM options, Therefore SonarQube using elastciseach and JVM options. The reason is our system\u0026rsquo;s HEAP MEMORY will not be compatible with the JVM configurations.\nIf your sonarqube service not starting or keep restarting, check the following log file.\ntail -f /opt/sonarqube/logs/es.log\ntail -f /opt/sonarqube/logs/sonar.log\ntail -f /opt/sonarqube/logs/access.log\nAnd check port number 9000 or 9001 listing on localhost.\nIf not, your JVM.OPTION may not be compatible with your physical RAM amount. Then, You need to define the matching JAVA HEAP Memory size for your host machine.\nvim /opt/sonarqube/elasticsearch/config/jvm.options # Xms represents the initial size of total heap space # Xmx represents the maximum size of total heap space -Xms1g -Xmx1g You may need to adjust your HEAP MEMORY according to your physical usable memory size.\n/opt/sonarqube/elasticsearch/config/elasticsearch.yml\n/opt/sonarqube/elasticsearch/config/log4j2.properties\nSonarQube initial configuration has been completed. In the next tutorial, I will show you how to integrate and analyze your project code on SonarQube with the Jenkins server and GitLab. And analysis of code deployments real-time.\nIf you need further clarification, please ask in the YouTube video comment section.\n","date":"May 8, 2020","image":"https://digitalavenue.dev/img/post-imgs/SonarQube-Ubuntu/sonarqube_hu2f63b7ea7ed2876a5a6396d6545677e0_39904_650x0_resize_q90_box.jpg","permalink":"/blog/2020-05-08-install-sonarqube-on-ubuntu-debian/","title":"How to Install and Configure SonarQube 8 on Ubuntu 18.04"},{"categories":["DevOps"],"contents":" Introduction: Jenkins is an open-source automation server that provides hundreds of plugin to perform continuous integration and continues delivery to build and deploy projects.\nContinuous integration (CI) is a DevOps practice in which team members regularly commit their code changes to the version control repository. Automated builds and tests are run. Continuous delivery (CD) is a series of practices where code changes are automatically built, tested and deployed to production.\nIf you are a software developer, surely Jenkins suites and automates your CI/CD build tasks quickly.\nJenkins can automate continuous integration and continuous delivery (CI/CD) for any project. Support for hundreds of plugins in the update centre. Which provides infinite possibilities for what Jenkins can do. Jenkins can configure into a distributed system that distributes work across multiple node/machines.\nFeatures: CICD - Continues Integration and Continues Delivery\nPlugins - Hundreds of Plugin Support\nExtensible - Extended possibilities using its Plugins\nDistributed - Distribute work across multiple machines and projects\nPrerequisites: Hardware - RAM \u0026gt; 1GB | HDD \u0026gt; 50GB+ Software - JAVA (JRE 8 | JDK 11)\nFollowing JDK/JRE Versions support for current Jenkins versions\nOpenJDK JDK / JRE 8 - 64 bits OpenJDK JDK / JRE 11 - 64 bits\nNOTE: Always check JAVA version requirement before proceeding with the Jenkins installation.\nREF: https://www.jenkins.io/doc/administration/requirements/java/ STEP 01: Install Oracel/Open JDK 11 Now, I\u0026rsquo;m going to install OpenJDK 11 on my system.\nInstall OpenJDK sudo apt update sudo apt-get install openjdk-11-jdk -y OR\nREF: https://www.oracle.com/java/technologies/javase-jdk11-downloads.html sudo dpkg -i jdk-11.0.7_linux-x64_bin.deb Set Default JDK Version sudo update-alternatives --config java java -version STEP 02: Install Jenkins Using Debian Repository Import trusted PGP Key for Jenkins sudo wget -q -O - http://pkg.jenkins-ci.org/debian/jenkins-ci.org.key | sudo apt-key add - Add Jenkins Repository sudo wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add - sudo sh -c \u0026#39;echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list\u0026#39; Install Jenkins sudo apt-get update sudo apt-get install jenkins Jenkins service will automatically start after the installation process is complete. You can verify it by printing the service status.\nsudo systemctl status jenkins If it isn\u0026rsquo;t enabled,/started automatically.\nsudo systemctl enable jenkins sudo systemctl start jenkins STEP 03: Configure Firewall Allow port 8080 through the Ubuntu firewall.\nsudo ufw allow 8080/tcp STEP 04: Initial Setup For Jenkins Once the above configuration completed, Open-up your web browser and access it through the IP: PORT.\nhttp://your_ip_or_domain:8080\nNow, Head-over to the terminal again, and find out the Administrator password using this command.\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword Copy the password from the terminal and paste it into the required field\nThe next screen at the initial setup wizard will ask for Install suggested plugins or select specific plugins. Click on the Install suggested plugins box, and the installation process will start immediately.\nOnce the plugins are installed, you will be prompted to set up the first admin user. Fill out all required information and click Save and Continue.\nClick on the Start using Jenkins button. You will be redirected to the Jenkins dashboard logged in as the admin user you have created in one of the previous steps.\nNow you\u0026rsquo;ve successfully installed Jenkins on your Ubuntu system.\nBottom Line I hope you learned how to install Jenkins on Ubuntu. And In the next tutorial, I\u0026rsquo;ll show you how to integrate GitLab with your newly built Jenkins server. If you have any questions, please leave a comment on the YouTube comment section. ","date":"April 6, 2020","image":"https://digitalavenue.dev/img/post-imgs/jenkins-ubuntu/Jenkins_huf906d7a17f4288128a83ece685ed2ca2_55114_650x0_resize_q90_box.jpg","permalink":"/blog/2020-04-06-how-to-install-and-configure-jenkins-on-ubuntu-18.04/","title":"How to Install and Configure Jenkins on Ubuntu 18.04 LTS 16.04 Debian"},{"categories":["IT Security"],"contents":" What is Wazuh? Wazuh is a free, open source and enterprise-ready security detection and monitoring solution.\nWazuh is born as a fork of OSSEC (HIDS) host based intrusion detection system. Later is was integrated with Elastic stack and OpenSCAP.\nWhich can perform threat detection, integrity monitoring, incident response and compliance.\nWazuh System consist with several components OSSEC HIDS - Host Based Intrusion Detection System\nOpenSCAP - Open Vulnerability Assessment Language\nElastic Stack - Filebeat, Elasticsearch, Kibana\nWazuh is loaded with number of valued capabilities.\nMain Features: 1. Security Analytics: Wazuh is used to collect, aggregate, index and analyze security data which helping to detect intrusions, threats and anomalies.\nEndpoint Detection and Response (EDR)\nWazuh Agent actively perform security analysts discover, investigate and perform block a network attack, stop a malicious process or quarantine a malware infected file.\n2. Intrusion Detection Wazuh-Agent scan the monitored system looking for malware, rootkits and suspicious anomalies. Also It can detect hidden files, clocked processes or unregistered network listeners.\n3. Log Data Analysis Wazuh-Agent read operating system and application logs, and forward them to a central Wazuh-Manager for rule-based analysis.\nWhich helps you to aware of application or system errors,miss-configuration, attempted successful malicious activities, policy violations and many more.\n4. File Integrity Monitoring Wazuh monitors the file system, identifying changes in content, permissions, ownership and attributes of files that you need to keep an eye on.\nAlso It can identify users and applications used to create or modify files.\n5. Vulnerability Detection Wazuh agent pull software inventory data and send them to the Wazuh Manager server. Then, they matches with CVE (Common Vulnerabilities and Exposure) databases, in order to identify well-know vulnerable software.\nAutomated vulnerability assessment helps you find the weak spots in your critical assets and take corrective action before attackers exploit them to sabotage your business or steal confidential data.\n6. Configuration Assessment Wazuh monitors system and application configuration settings to ensure they are compliant with you security policies and standards.\nAgent automatically performs periodic scan to detect applications that are know to be vulnerable, unpatched, or insecurely configured.\nAnd also It alerts recommendations for better configuration and security hardening.\n7. Incident Response Wazuh take action against active threats such as blocking access from the threat source when certain criteria are met.\n8. Regulatory Compliance Wazuh provides some of necessary security controls to become complaint with industry standards and regulations.\n9. Cloud Security Wazuh helps monitoring cloud infrastructure as an API level. It can pull security data from instances on well known cloud providers such as AWS, Azure, Google Cloud Platform.\n10. Containers Security Wazhuh provides security visibility into your docker hosts and containers.\nSTEP 01: Perform Prerequisites A. Install latest yum repositories yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-12.noarch.rpm yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm yum install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm yum install http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm yum install https://download1.rpmfusion.org/free/el/rpmfusion-free-release-7.noarch.rpm yum install http://mirror.ghettoforge.org/distributions/gf/el/7/gf/x86_64/gf-release-7-10.gf.el7.noarch.rpm B. Set Hostname with Fully Qualified Domain Name (FQDN) [root@server1 ~]# vim /etc/hostname wazuh-elk [root@server1 ~]# vim /etc/hosts 172.25.10.50 wazuh-elk.da.com wazuh-elk C. SELinux Configuration [root@server1 ~]# vim /etc/selinux/config SELINUX=permissive D. Install Python3 root@server1 ~]# yum install python3 SETP 02: Install Wazuh Manager A. Install Development Tools, Compilers, and Other Required Packages. [root@server1 ~]# yum install make gcc policycoreutils-python automake autoconf libtool OpenSSL will be required to create a system certificate while authentication with Wazuh Agents.\n[root@server1 ~]# yum install openssl Install Latest EPEL-Release\n[root@server1 ~]# yum install epel-release yum-utils -y Install Budild Dependencies of CPython\n[root@server1 ~]# yum-builddep python34 -y B. Download and Extract Wazuh Installation Files [root@server1 ~]# curl -Ls https://github.com/wazuh/wazuh/archive/v3.11.1.tar.gz | tar zx [root@server1 ~]# cd wazuh-3.11.1/ C. Run the \u0026ldquo;Install.sh\u0026rdquo; Script This will guide you through the installation process. When Script ask for what kind of installation need to perform, type \u0026ldquo;manager\u0026rdquo; to install the Wazuh Manager.\n[root@server1 wazuh-3.11.1]# ./install.sh (en/br/cn/de/el/es/fr/hu/it/jp/nl/pl/ru/sr/tr) [en]: en Wazuh v3.11.1 (Rev. 31116) Installation Script - http://www.wazuh.com You are about to start the installation process of Wazuh. You must have a C compiler pre-installed in your system. - System: Linux server1 3.10.0-862.el7.x86_64 (centos 7.5) - User: root - Host: server1 1- What kind of installation do you want (manager, agent, local, hybrid or help)? manager 2- Setting up the installation environment. - Choose where to install Wazuh [/var/ossec]: 3- Configuring Wazuh. 3.1- Do you want e-mail notification? (y/n) [n]: y - What\u0026#39;s your e-mail address? user@gmail.com - We found your SMTP server as: alt1.gmail-smtp-in.l.google.com. - Do you want to use it? (y/n) [y]: y --- Using SMTP server: alt1.gmail-smtp-in.l.google.com. 3.2- Do you want to run the integrity check daemon? (y/n) [y]: y - Running syscheck (integrity check daemon). 3.3- Do you want to run the rootkit detection engine? (y/n) [y]: y - Running rootcheck (rootkit detection). 3.4- Do you want to run policy monitoring checks? (OpenSCAP) (y/n) [y]: y - Running OpenSCAP (policy monitoring checks). 3.5- Active response allows you to execute a specific command based on the events received. By default, no active responses are defined. - Default white list for the active response: - 172.25.10.1 - 8.8.8.8 - Do you want to add more IPs to the white list? (y/n)? [n]: n 3.6- Do you want to enable remote syslog (port 514 udp)? (y/n) [y]: y - Remote syslog enabled. 3.7 - Do you want to run the Auth daemon? (y/n) [y]: y - Running Auth daemon. 3.8- Do you want to start Wazuh after the installation? (y/n) [y]: y - Wazuh will start at the end of installation. 3.9- Setting the configuration to analyze the following logs: -- /var/log/audit/audit.log -- /var/ossec/logs/active-responses.log -- /var/log/messages -- /var/log/secure -- /var/log/maillog - If you want to monitor any other file, just change the ossec.conf and add a new localfile entry. Any questions about the configuration can be answered by visiting us online at https://documentation.wazuh.com/. 4- Installing the system - Running the Makefile Finished processing dependencies for wazuh==3.11.1 Installing SCAP security policies... Generating self-signed certificate for ossec-authd... Building CDB lists... Created symlink from /etc/systemd/system/multi-user.target.wants/wazuh-manager.service to /etc/systemd/system/wazuh-manager.service. Starting Wazuh... - Configuration finished properly. - To start Wazuh: /var/ossec/bin/ossec-control start - To stop Wazuh: /var/ossec/bin/ossec-control stop - The configuration can be viewed or modified at /var/ossec/etc/ossec.conf [root@server1 wazuh-3.11.1]# /var/ossec/bin/ossec-control start Starting Wazuh v3.11.1... Started ossec-csyslogd... Started ossec-dbd... 2020/01/13 05:13:08 ossec-integratord: INFO: Remote integrations not configured. Clean exit. Started ossec-integratord... Started ossec-agentlessd... ossec-authd already running... wazuh-db already running... ossec-execd already running... ossec-maild already running... ossec-analysisd already running... ossec-syscheckd already running... ossec-remoted already running... ossec-logcollector already running... ossec-monitord already running... wazuh-modulesd already running... Completed. D. Allow Wazuh Manager Service Ports Through The Firewall [root@wazuh-elk ~]# firewall-cmd --permanent --add-port={514,1514,1515,1516}/tcp [root@wazuh-elk ~]# firewall-cmd --permanent --add-port={514,1514}/udp [root@wazuh-elk ~]# firewall-cmd --reload 514 - send collected events from syslogs\n1514 - Send collected event from agents\n1515 - Agents registration service\n1516 - Wazuh cluster communications\nE. Enable Service \u0026amp; Restart [root@server1 wazuh-3.11.1]# systemctl daemon-reload [root@server1 wazuh-3.11.1]# systemctl enable wazuh-manager.service [root@server1 wazuh-3.11.1]# systemctl restart wazuh-manager.service [root@server1 wazuh-3.11.1]# systemctl status -l wazuh-manager.service STEP 03: Install Wazuh API A. Install NodeJS NodeJS required in order to run Wazuh API.\n[root@server1 ~]# curl --silent --location https://rpm.nodesource.com/setup_8.x | bash - [root@server1 ~]# sudo yum install gcc-c++ make [root@server1 ~]# sudo yum install -y nodejs B. Install Yarn [root@server1 ~]# curl -sL https://dl.yarnpkg.com/rpm/yarn.repo | sudo tee /etc/yum.repos.d/yarn.repo [root@server1 ~]# sudo yum install yarn C. Install Wazuh API Download \u0026amp; Extract Wazuh API Package\nRun this command to download and install the Wazuh API\n[root@server1 ~]# curl -s -o install_api.sh https://raw.githubusercontent.com/wazuh/wazuh-api/v3.11.1/install_api.sh \u0026amp;\u0026amp; bash ./install_api.sh download API URL: http://host_ip:55000/ user: \u0026#39;foo\u0026#39; password: \u0026#39;bar\u0026#39; Configuration: /var/ossec/api/configuration Test: curl -u foo:bar -k http://127.0.0.1:55000?pretty Note: You can configure the API executing /var/ossec/api/scripts/configure_api.sh ### [API installed successfully] ### D. Allow Wazuh-API Service Port Through the Firewall 55000/TCP - Incoming HTTP requests\n[root@wazuh-elk ~]# firewall-cmd --permanent --add-port=55000/tcp [root@wazuh-elk ~]# firewall-cmd --reload E. Enable Service \u0026amp; Restart [root@wazuh-elk ~]# systemctl enable wazuh-api.service [root@wazuh-elk ~]# systemctl restart wazuh-api.service [root@wazuh-elk ~]# systemctl status -l wazuh-api.service REF: https://documentation.wazuh.com/3.11/installation-guide/installing-wazuh-manager/linux/centos/wazuh_server_sources_centos.html#wazuh-server-sources-centos STEP 04: Install Filebeat Filebeat is a fork of the Elastic Stack. Filebeat use to forward alerts and event to Elasticsearch.\nA. Import Public GPG Key [root@wazuh-elk ~]# rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch B. Create YUM Repository File [root@wazuh-elk ~]# vim /etc/yum.repos.d/elastic.repo [elasticsearch-7.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md C. Install Filebeat [root@wazuh-elk ~]# yum install filebeat D. Download pre-configured Configuration File This file pre-configured to forward alerts Elasticsearch.\n[root@wazuh-elk ~]# curl -so /etc/filebeat/filebeat.yml https://raw.githubusercontent.com/wazuh/wazuh/v3.11.1/extensions/filebeat/7.x/filebeat.yml Provide required permission\n[root@wazuh-elk ~]# chmod go+r /etc/filebeat/filebeat.yml E. Download alerts template for Elasticsearch [root@wazuh-elk ~]# curl -so /etc/filebeat/wazuh-template.json https://raw.githubusercontent.com/wazuh/wazuh/v3.11.1/extensions/elasticsearch/7.x/wazuh-template.json Provide required permission\n[root@wazuh-elk ~]# chmod go+r /etc/filebeat/wazuh-template.json F. Download Wazuh module for Filebeat [root@wazuh-elk ~]# curl -s https://packages.wazuh.com/3.x/filebeat/wazuh-filebeat-0.1.tar.gz | sudo tar -xvz -C /usr/share/filebeat/module REF: https://documentation.wazuh.com/3.11/installation-guide/installing-wazuh-manager/linux/centos/wazuh_server_packages_centos.html#wazuh-server-packages-centos-filebeat STEP 05: Install JAVA JDK Java is required for the Elastic stack deployment. Java is to be installed on Elasticseach node.\nA. Download \u0026amp; Install JDK RPM [root@wazuh-elk /]# rpm -ivh jdk-13.0.1_linux-x64_bin.rpm B. Set Default JAVA Version [root@wazuh-elk /]# alternatives --config java [root@wazuh-elk /]# alternatives --set jar /usr/java/jdk-13.0.1/bin/jar [root@wazuh-elk /]# alternatives --set javac /usr/java/jdk-13.0.1/bin/javac C. Set JAVA Environment Variables Set the JAR and JAVAC paths Add entries into .bashrc and .bash_profile for the persistence even after the reboot.\n[root@wazuh-elk /]# export JAVA_HOME=/usr/java/jdk-13.0.1/ [root@wazuh-elk /]# export PATH=$PATH:/usr/java/jdk-13.0.1/bin/ [root@wazuh-elk /]# vim ~/.bashrc export JAVA_HOME=/usr/java/jdk-13.0.1/ export PATH=$PATH:/usr/java/jdk-13.0.1/bin/ [root@wazuh-elk /]# vim ~/.bash_profile export JAVA_HOME=/usr/java/jdk-13.0.1/ export PATH=$PATH:/usr/java/jdk-13.0.1/bin/ STEP 06: Install Elasticsearch A. Install Elasticsearch From YUM Repositories. [root@wazuh-elk ~]# yum install elasticsearch B. Allow Elasticearch Service Ports Through The Firewall 9200/tcp - Elasticearch RESTful API 9300-9400/tcp - Elasticsearch cluster communication\n[root@wazuh-elk ~]# firewall-cmd --permanent --add-port={9200,9300}/tcp [root@wazuh-elk ~]# firewall-cmd --reload C. Configure Elasticsearch Make Elasticsearch to be able to listening on non loopback IP address. In here you can add current machine IP address. I\u0026rsquo;m not going to change the loopback IP, because this is the one Elastic node I\u0026rsquo;m going to setup. So, It doesn\u0026rsquo;t need to talk outside.\n[root@wazuh-elk ~]# vim /etc/elasticsearch/elasticsearch.yml node.name: node-1 network.host: 127.0.0.1 http.port: 9200 cluster.initial_master_nodes: [\u0026#34;node-1\u0026#34;\u0026#34;] [root@wazuh-elk ~]# cat /etc/elasticsearch/elasticsearch.yml | grep -v \u0026#34;^#\u0026#34; node.name: node-1 path.data: /var/lib/elasticsearch path.logs: /var/log/elasticsearch network.host: 127.0.0.1 http.port: 9200 cluster.initial_master_nodes: [\u0026#34;node-1\u0026#34;] D. JVM Option Configuration Set initial/maximum size of total heap space. If your system has less memory. You should configure it to use small megabytes of ram.\nThis amount of assigned memory may be vary to amount of your server\u0026rsquo;s total physical memory.\nIn this case I have 6GiB of total physical memory for the demonstration purpose.\n[root@wazuh-elk ~]# vim /etc/elasticsearch/jvm.options -Xms2g -Xmx2g D. Enalbe Service \u0026amp; Restart [root@wazuh-elk /]# sudo systemctl daemon-reload [root@wazuh-elk /]# sudo systemctl enable elasticsearch.service [root@wazuh-elk /]# sudo systemctl start elasticsearch.service STEP 07: Configure Filebeat Output For Elasticsearch [root@wazuh-elk ~]# vim /etc/filebeat/filebeat.yml output.elasticsearch.hosts: [\u0026#39;http://127.0.0.1:9200\u0026#39;] REF: https://documentation.wazuh.com/3.11/installation-guide/installing-wazuh-manager/linux/centos/wazuh_server_packages_centos.html#wazuh-server-packages-centos-filebeat STEP 08: Load Filebeat Template [root@wazuh-elk ~]# filebeat setup --index-management -E setup.template.json.enabled=false REF: https://documentation.wazuh.com/3.11/installation-guide/installing-elastic-stack/elastic_server_rpm.html STEP 09: Install Kibana Kibana is the Web interface for visualizing the events stored in elasticsearch.\nA. Install Kibana [root@wazuh-elk ~]# yum install kibana B. Install Wazuh App plugin For Kibana [root@wazuh-elk /]# sudo -u kibana /usr/share/kibana/bin/kibana-plugin install https://packages.wazuh.com/wazuhapp/wazuhapp-3.11.1_7.5.1.zip C. Configure Kibana Make Kibana to be able to listening on outside of the network and provide Elasticsearch instance IP address.\nIn this case I\u0026rsquo;m going to keep loopback IP for Elasticsearch, Since I don\u0026rsquo;t want to expose outside network.\n[root@wazuh-elk /]# vim /etc/kibana/kibana.yml server.port: 5601 server.host: \u0026#34;172.25.10.50\u0026#34; elasticsearch.hosts: [\u0026#34;http://127.0.0.1:9200\u0026#34;] D. Allow Kibana Service Port Through The Firewall 5601/tcp - Kibana Web Interface\n[root@wazuh-elk /]# firewall-cmd --permanent --add-port=5601/tcp [root@wazuh-elk /]# firewall-cmd --reload E. Enable Service \u0026amp; Restart [root@wazuh-elk /]# systemctl daemon-reload [root@wazuh-elk /]# systemctl enable kibana.service [root@wazuh-elk /]# systemctl start kibana.service REF: https://documentation.wazuh.com/3.11/installation-guide/installing-elastic-stack/elastic_server_rpm.html TROUBLESHOOTING TIPS ERROR: wazuh-elk kibana[6972]: Browserslist: caniuse-lite is outdated. Please run next command `npm update` [root@wazuh-elk ~]# yarn install [root@wazuh-elk ~]# yarn upgrade caniuse-lite browserlist [root@wazuh-elk ~]# npm update CLIENT SIDE CONFIGURATION - Adding \u0026amp; Registering Wazuh Agent Wazuh Agent: Wazuh agent runs on the host that you want to monitor.\nWazuh Agent has following capabilities Log \u0026amp; data collection File integrity monitoring Rootkit and malware detection security policy monitoring Configuration assessment software inventory In this scenario I\u0026rsquo;m going to install Wazuh Agent on CentOS and Ubuntu.\nSTEP 01: Install Wazuh Agent A. Install Development Tools and Compilers [root@cl1 ~]# yum install make gcc policycoreutils-python automake autoconf libtool Install \u0026ldquo;openssl\u0026rdquo; in order to support certificate creation while installation.\n[root@cl1 ~]# yum install openssl B. Download \u0026amp; Extract Latest Wazuh Package [root@cl1 ~]# curl -Ls https://github.com/wazuh/wazuh/archive/v3.11.1.tar.gz | tar zx C. Install Wazuh Agent Run \u0026ldquo;install.sh\u0026rdquo; script. This will guide you through out the installation process.\nProvide necessary responses while the installation process.\n[root@cl1 ~]# cd wazuh-3.11.1/ [root@cl1 wazuh-3.11.1]# ./install.sh [root@cl1 wazuh-3.11.1]# ./install.sh ** For installation in English, choose [en]. (en/br/cn/de/el/es/fr/hu/it/jp/nl/pl/ru/sr/tr) [en]: en Wazuh v3.11.1 (Rev. 31116) Installation Script - http://www.wazuh.com You are about to start the installation process of Wazuh. You must have a C compiler pre-installed in your system. - System: Linux cl1 3.10.0-862.el7.x86_64 (centos 7.5) - User: root - Host: cl1 1- What kind of installation do you want (manager, agent, local, hybrid or help)? agent - Agent (client) installation chosen. 2- Setting up the installation environment. - Choose where to install Wazuh [/var/ossec]: - Installation will be made at /var/ossec . 3- Configuring Wazuh. 3.1- What\u0026#39;s the IP Address or hostname of the Wazuh server?: 172.25.10.50 - Adding Server IP 3.2- Do you want to run the integrity check daemon? (y/n) [y]: y - Running syscheck (integrity check daemon). 3.3- Do you want to run the rootkit detection engine? (y/n) [y]: y - Running rootcheck (rootkit detection). 3.4- Do you want to run policy monitoring checks? (OpenSCAP) (y/n) [y]: y - Running OpenSCAP (policy monitoring checks). 3.5 - Do you want to enable active response? (y/n) [y]: y - Active response enabled. 3.6- Remote upgrades use packages signed by the system maintainer. The corresponding certificate (or root certificate) must be installed in the system in order to verify the WPK packages. By default, the root certificate by Wazuh is installed. - Do you want to add more certificates? (y/n)? [n]: 3.7- Setting the configuration to analyze the following logs: -- /var/log/audit/audit.log -- /var/ossec/logs/active-responses.log -- /var/log/messages -- /var/log/secure -- /var/log/maillog - If you want to monitor any other file, just change the ossec.conf and add a new localfile entry. Any questions about the configuration can be answered by visiting us online at https://documentation.wazuh.com/. 4- Installing the system - Running the Makefile - Configuration finished properly. - To start Wazuh: /var/ossec/bin/ossec-control start - To stop Wazuh: /var/ossec/bin/ossec-control stop - The configuration can be viewed or modified at /var/ossec/etc/ossec.conf - You first need to add this agent to the server so they can communicate with each other. When you have done so, you can run the \u0026#39;manage_agents\u0026#39; tool to import the authentication key from the server. /var/ossec/bin/manage_agents D. Allow Wazuh-Agent Service Port Through The Firewall [root@cl1 ~]# firewall-cmd --permanent --add-port={514,1514}/udp [root@cl1 ~]# firewall-cmd --reload Now, Wazuh manger and agent configuration has been completed. Then, We need to add Wazuh-Agents into Wazuh-Manager.\nREF: https://documentation.wazuh.com/3.11/installation-guide/installing-wazuh-agent/linux/centos6-or-greater/wazuh_agent_sources_centos6_or_greater.html#wazuh-agent-sources-centos6-or-greater Registering Agents STEP 01: Add an agent - Add a CentOS/Redhat Client Agent To Wazuh-Manager Let\u0026rsquo;s head over to Wazuh-Manager and perform these configurations.\nOn the Manager:\nExecute this command to add new agent to wazuh-manager.\n/var/ossec/bin/manage_agents -a \u0026lt;Agent_IP_Adress\u0026gt; -n \u0026lt;Agent_Hostname\u0026gt;\n[root@wazuh-elk /]# /var/ossec/bin/manage_agents -a 172.25.10.100 -n cl1 **************************************** * Wazuh v3.11.1 Agent manager. * * The following options are available: * **************************************** (A)dd an agent (A). (E)xtract key for an agent (E). (L)ist already added agents (L). (R)emove an agent (R). (Q)uit. Choose your action: A,E,L,R or Q: - Adding a new agent (use \u0026#39;q\u0026#39; to return to the main menu). Please provide the following: * A name for the new agent: * The IP Address of the new agent: Confirm adding it?(y/n): Agent added with ID 001. manage_agents: Exiting. STEP 02: List The Agents Execute this command to list the agent to get the ID of the added agent (cl1).\n[root@wazuh-elk /]# /var/ossec/bin/manage_agents -l OR\n[root@wazuh-elk /]# /var/ossec/bin/manage_agents **************************************** * Wazuh v3.11.1 Agent manager. * * The following options are available: * **************************************** (A)dd an agent (A). (E)xtract key for an agent (E). (L)ist already added agents (L). (R)emove an agent (R). (Q)uit. Choose your action: A,E,L,R or Q: L Available agents: ID: 001, Name: cl1, IP: 172.25.10.100 STEP 03: Extract Newly Added Agent\u0026rsquo;s Key Execute this command with the agent \u0026ldquo;ID\u0026rdquo; and extract the new agent\u0026rsquo;s key. Cpy this key you will need it for the agent registration.\n[root@wazuh-elk /]# /var/ossec/bin/manage_agents -e 001 Agent key information for \u0026#39;001\u0026#39; is: MDAxIGNsMSAxNzIuMjUuMTAuMTAwIDU4N2Y2ZmYyMDA0NmY1NzZmMDc2ODJkOWFlMzM5ZmM1OWY0YzQzYjBhZTk0MjI3NTQyZTkxYTczZmEyZTk4Njk= Now, Let\u0026rsquo;s head over to agent host.\nOn the Agent\nSTEP 04: Import the Key To The Agent and Connect Agent To The Manger [root@cl1 /]# /var/ossec/bin/manage_agents -i MDAxIGNsMSAxNzIuMjUuMTAuMTAwIDU4N2Y2ZmYyMDA0NmY1NzZmMDc2ODJkOWFlMzM5ZmM1OWY0YzQzYjBhZTk0MjI3NTQyZTkxYTczZmEyZTk4Njk= Agent information: ID:001 Name:cl1 IP Address:172.25.10.100 Confirm adding it?(y/n): y Added. OPTIONAL - Check for Wazuh Manager\u0026rsquo;s IP Edit the Wazuh agent configuration in \u0026ldquo;/var/ossec/etc/ossec.conf\u0026rdquo; to add/change the Wazuh Manager Server IP address.\n[root@cl1 /]# vim /var/ossec/etc/ossec.conf STEP 05: Restart Wazuh Agent Service [root@cl1 /]# systemctl restart wazuh-agent [root@cl1 /]# systemctl status -l wazuh-agent REF: https://documentation.wazuh.com/3.11/user-manual/registering/cli/using-command-line-linux.html Add a Ubuntu Client Agent To Wazuh-Manager On The Manager:\n[root@wazuh-elk ~]# /var/ossec/bin/manage_agents -a 172.25.10.110 -n DA-PC **************************************** * Wazuh v3.11.1 Agent manager. * * The following options are available: * **************************************** (A)dd an agent (A). (E)xtract key for an agent (E). (L)ist already added agents (L). (R)emove an agent (R). (Q)uit. Choose your action: A,E,L,R or Q: - Adding a new agent (use \u0026#39;q\u0026#39; to return to the main menu). Please provide the following: * A name for the new agent: * * The IP Address of the new agent: * Confirm adding it?(y/n): * Agent added with ID 002. manage_agents: Exiting. Follow the on screen guide and generate a code.\nSTEP 01: Adding an agent root@DA-PC:~# curl -so wazuh-agent.deb https://packages.wazuh.com/3.x/apt/pool/main/w/wazuh-agent/wazuh-agent_3.11.1-1_amd64.deb \u0026amp;\u0026amp; sudo WAZUH_MANAGER_IP=\u0026#39;172.25.10.50\u0026#39; dpkg -i ./wazuh-agent.deb STEP 02: Extract new agent\u0026rsquo;s key [root@wazuh-elk ~]# /var/ossec/bin/manage_agents -e 002 Agent key information for \u0026#39;002\u0026#39; is: MDAyIERBLVBDIDE3Mi4yNS4xMC4xMTAgMDU3NGM5NGE3M2I5MDE4ZTE5NDdmMGQ4NTBjYzE0MWIwNDgwYTViZGY4YmY0NmU5MTA1ZWE2YWZmN2M3ODc2OA== On the Agent STEP 03: Import the created key for new agent at the Manger root@DA-PC:~# /var/ossec/bin/manage_agents -i MDAyIERBLVBDIDE3Mi4yNS4xMC4xMTAgMDU3NGM5NGE3M2I5MDE4ZTE5NDdmMGQ4NTBjYzE0MWIwNDgwYTViZGY4YmY0NmU5MTA1ZWE2YWZmN2M3ODc2OA== Agent information: ID:002 Name:DA-PC IP Address:172.25.10.110 Confirm adding it?(y/n): y Added. STEP 04: Restart Wazuh Agent Service root@DA-PC:~# systemctl restart wazuh-agent.service ","date":"January 15, 2020","image":"https://digitalavenue.dev/img/post-imgs/wazuh/wazuh_hu42fec9d25320dd4958853d7fb01dd893_246269_650x0_resize_box_3.png","permalink":"/blog/2020-01-15-install-and-configre-wazuh-security-monitoring-and-detection-system/","title":"Install Wazuh on CentOS and RHEL An Intrusion Detection System"},{"categories":["Monitoring"],"contents":" In this tutorial, I\u0026rsquo;m going to cover up how to install and configure Zabbix 4.4 Monitoring Server on CentOS8\nIntroduction: Zabbix is an enterprise-class free open source monitoring solution.\nNewly released Zabbix Monitoring solution empowered with unlimited capabilities of monitoring any IT device in house.\nWhich allows you to keep eye on any kinds of IT infrastructure, services, applications and resources.\nYou will get ability to monitor any performance metrics and incidents in your network such as Power status, System Status, CPU, Memory and Network bandwidth usage, Packet Loss rate, Link status, New device added or removed. And many more\u0026hellip;\nWhich provides real-time graphing and highly configurable graphing facility to users. Available thousands of templates on the Zabbix share. Which is a community to share thousands of templates and scripts accomplish the monitoring tasks. Automatic network discovery provides agent auto registration facility. Zabbix API provides interface for 3rd party software integration such as Grafana and many more. For this tutorial, I will going to install Zabbix using yum destitution package for CentOS and RHEL.\nSTEP 01: Configure Prerequisitess A. Set Resolvable Hostname: Now, I\u0026rsquo;m going to set host-name with fully qualified domain name(FQDN).\nvim /etc/hostname vim /etc/hosts B. Install Latest EPEL Release: Now, I\u0026rsquo;m going to install latest epel release\nrpm -ivh https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/e/epel-release-8-7.el8.noarch.rpm C. Update Operating System: Then, Update the Operating System.\nyum -y update D. Configure SELinux: You need to execute these commands, If you have enabled SELinux in \u0026ldquo;enforcing\u0026rdquo; mode.\nsetsebool -P httpd_can_connect_zabbix on setsebool -P httpd_can_network_connect_db on Either, You also can set SELinux into \u0026ldquo;permissive\u0026rdquo; mode\nvim /etc/selinux/config SELINUX=permissive E. Reboot the System: STEP 02: Install Apache HTTPD Web Server A. Install HTTPD Package: Now, I\u0026rsquo;ll going to install httpd package for web server.\nyum install httpd httpd-tools B. Enable \u0026amp; Start HTTPD Service: Enable service to start automatically at the system boots.\nsystemctl enable httpd.service Then, Start service.\nsystemctl start httpd.service C. Allow HTTPD Through The Firewall: Allow port 80 for Apache service through out the system firewall.\nfirewall-cmd --permanent --add-service=http firewall-cmd --permanent --add-service=https firewall-cmd --permanent --add-port=80/tcp firewall-cmd --permanent --add-port=443/tcp firewall-cmd --reload D. verify The Service is Running: Finally, Check weather the Apache service and listen through the port 80\nnetstat -tulpen | grep httpd Now, goto /var/www/html/ directory and create a index.html file.\nvim /var/www/html/index.html \u0026quot;THIS IS A TEST PAGE\u0026quot;\nSave and exit.\nThen, Open up the web browser and hit the server\u0026rsquo;s IP address\nhttp://\u0026lt;your_server_ip_address\u0026gt;/\nOK, Now you\u0026rsquo;ll see Apache testing page. Now Apache configuration has been completed.\nLet\u0026rsquo;s head-over to next step.\nSTEP 03: Install \u0026amp; Configure PHP A. Install PHP \u0026amp; Dependencies: Now, I\u0026rsquo;m going to install PHP and required PHP dependencies which required to run Zabbix fronted.\nI Here, PHP used to gather matrices from MariaDB/Mysql database and process to display dynamic content via Apache web server\nyum -y install php php-cli php-common php-devel php-pear php-gd php-mbstring php-mysqlnd php-xml php-bcmath Then, We need to configure \u0026ldquo;php.ini\u0026rdquo; file with these variables.\nB. Configure PHP: vim /etc/php.ini OR\necho \u0026#34;\u0026lt;?php phpinfo(); ?\u0026gt;\u0026#34; | sudo tee /var/www/html/info.php max_execution_time = 600 max_input_time = 600 memory_limit = 1024M post_max_size = 32M upload_max_filesize = 32M date.timezone = Asia/Colombo C. Test PHP: Finally, Create a \u0026ldquo;info.php\u0026rdquo; file to check whether PHP working well.\nvim /var/www/html/info.php \u0026lt;?php phpinfo(); ?\u0026gt; Restart Apache service again.\nsystemctl restart httpd.service Then, Open the web browser and hit server\u0026rsquo;s IP address with \u0026ldquo;info.php\u0026rdquo;. The you would see a page like this.\nhttp://\u0026lt;your_server_ip_address\u0026gt;/info.php\nSTEP 04: Install and Configure MariaDB A. Install MariaDB: Then, Install MariaDB on our Zabbix hosting server.\nyum install mariadb mariadb-server C. Allow MySQL Service Through The Firewall: Allow MariaDB/MySQL service through the firewall.\nfirewall-cmd --permanent --add-service=mysql firewall-cmd --reload Start and enable Mariadb/MySQL service on reboot.\nsystemctl start mariadb.service systemctl enable mariadb.service D. MySQL Initial Setup: MariaDB Initial configuration:\nmysql_secure_installation Set root password? [Y/n] Y New password: Re-enter new password: Remove anonymous users? [Y/n] Y Disallow root login remotely? [Y/n] Y Reload privilege tables now? [Y/n] Y Log into MariaDB Server:\nmysql -uroot -p MariaDB [(none)]\u0026gt; select version(); mysql -V STEP 05: Create \u0026ldquo;zabbixdb\u0026rdquo; database and \u0026ldquo;zabbixuser\u0026rdquo; user A. Create a Zabbix User \u0026amp; Dsatabase: Now, I\u0026rsquo;m going to create a Zabbix user and a database. In this time, I\u0026rsquo;m going use \u0026ldquo;zabbixuser\u0026rdquo; as a user and \u0026ldquo;zabbixdb\u0026rdquo; as a database.\nFirst login into MariaDB/MySQL.\nmysql -uroot -p Enter password: Then create a database.\nCREATE DATABASE zabbixdb CHARACTER SET UTF8 COLLATE UTF8_BIN; Grant privileges to zabbixuser and zabbixdb with a password.\nGRANT ALL PRIVILEGES ON zabbixdb.* to zabbixuser@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;PASSWORD\u0026#39;; GRANT ALL PRIVILEGES ON zabbixdb.* to zabbixuser@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;PASSWORD\u0026#39;; Refresh permissons.\nFLUSH PRIVILEGES; QUIT; Now, I have completed the installation of LAMP stack which include Apache, PHP and MariaDB.\nNow we can go straight forward to the Zabbix package installation and configuration.\nSTEP 06: Install \u0026amp; Configure Zabbix Server First, We need to import PGP trusted signing key.\nrpm --import http://repo.zabbix.com/RPM-GPG-KEY-ZABBIX Next, Install the repository configuration package for the Zabbix.\nRHEL8 / CENTOS8\nrpm -Uvh https://repo.zabbix.com/zabbix/4.4/rhel/8/x86_64/zabbix-release-4.4-1.el8.noarch.rpm RHEL7 / CENTOS7\nrpm -Uvh https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm Then, Install Zabbix Server.\nyum install zabbix-server-mysql zabbix-web-mysql zabbix-apache-conf Finally, Import Zabbix server database schema.\nzcat /usr/share/doc/zabbix-server-mysql/create.sql.gz | mysql -uzabbixuser -pPASSWORD zabbixdb NOTE: This may take a while to import the database schema. So, Please wait untill it done.\nConfigure Zabbix Server:\nLet\u0026rsquo;s headover to Zabbix server main configuration file which located in \u0026ldquo;/etc/zabbix/zabbix_server.conf\u0026rdquo; and do these changes.\nvim /etc/zabbix/zabbix_server.conf In here you need to Zabbix server\u0026rsquo;s IP address, MySQL or MariaDB username and password\n## DATABASE NAME DBName=zabbixdb ## DATABASE USERNAME DBUser=zabbixuser ## DATABASE PASSWORD DBPassword=PASSWORD Allow Zabbix Service Through The Firewall:\nfirewall-cmd --permanent --add-port=10050/tcp firewall-cmd --permanent --add-port=10051/tcp firewall-cmd --reload Set Zabbix server name:\nvim /etc/httpd/conf/httpd.conf ServerName zabbix.da.com:80 ServerAdmin zabbix@da.com systemctl restart httpd.service Configure PHP for Zabbix Frontend:\nLet\u0026rsquo;s move to Zabbix server\u0026rsquo;s Apache configuration file.\nvim /etc/httpd/conf.d/zabbix.conf Some PHP values has be configured. These values you may be need to change later when your requirements getting higher. It depends on your Zabbix server where you going to use. Don\u0026rsquo;t forget to uncomment and set your Time Zone.\nvim /etc/httpd/conf.d/zabbix.conf Allow from All php_value max_execution_time 600 php_value memory_limit 1024M php_value post_max_size 32M php_value upload_max_filesize 32M php_value max_input_time 600 php_value max_input_vars 10000 php_value always_populate_raw_post_data -1 php_value date.timezone Asia/Colombo Start \u0026amp; enable service to start at system boot.\nsudo systemctl enable zabbix-server sudo systemctl restart zabbix-server STEP 07: Further SELinux Configuration For Zabbix If your SELinux configuration is still on \u0026ldquo;enforcing\u0026rdquo; mode. then you need to execute these commands to make connection between Zabbix frontend and the server.\nsetsebool -P httpd_can_connect_zabbix on STEP 08: Restart \u0026amp; Enable All Services Again. Now, You I\u0026rsquo;ll enable \u0026amp; restart all services once.\nsystemctl enable httpd.service mariadb.service zabbix-server.service systemctl restart -l httpd.service mariadb.service zabbix-server.service And Check whether they are running well.\nsystemctl status -l httpd.service mariadb.service zabbix-server.service Now, We are almost ready to complete the Zabbix server setup. Let\u0026rsquo;s Head over to the web browser and enter Zabbix server\u0026rsquo;s IP address with the \u0026ldquo;zabbix\u0026rdquo; suffix. and hit enter.\nhttp://zabbix_server_IP/zabbix/\nFor the first time you may need to log into the system using default username and password.\nDefault Zabbix Username: Admin\nDefault Zabbix Password: zabbix\nThen, follow the instruction as seen on installation wizard.\n","date":"January 9, 2020","image":"https://digitalavenue.dev/img/post-imgs/Zabbix4.4/zabbix_44_hue6933961a3c298ee8166b4f93d52a22f_107729_650x0_resize_q90_box.jpg","permalink":"/blog/2020-01-09-install_and_configure_zabbix_server-4.4_on_centos8-rhel8/","title":"Install and Configure Zabbix 4.4 on CentOS8 RHEL8"},{"categories":["IT Security"],"contents":" Snort is an intrusion detection and prevention system. Snort protects your network against hackers, security threats such as exploits, DDOS attacks and viruses.\nSnort detects attack methods, including denial of service, buffer overflow, CGI attacks, stealth port scans, and SMB probes\nSnort monitors network traffic and analize against a predefined rules. Then catagorized network attack. Finally, Sonrt involks actions against matching rules.\nIn this case you should consider deploying intrution detection and prevention system to detect and protect your network from attackers.\nSTEP 01: Install Snort Now Let\u0026rsquo;s start to install Snort package.\nHeadover to\u0026hellip;\nSystem \u0026gt; Package Manager \u0026gt; Available Packages\nSearch for a package named \u0026ldquo;snort\u0026rdquo;\nHit the Install button \u0026amp; then confirm the installation to proceed.\nIt grabs required repos from pfsense repositories.\nSTEP 02: Configure Snort Global Settings Services \u0026gt; Snort \u0026gt; Global Settings\nTo setting up Snort for the first time, We need to head over to \u0026ldquo;Global Settings\u0026rdquo; tab and enable required rule sets to be downloaded.\nEnable Snort VRT\nSnort VRT : Enabled Either you can sign up as a free user or paid user. After you singed up, you will get a \u0026ldquo;snort OinkMaster Code\u0026rdquo;\nFirst of all you\u0026rsquo;ll need to get registered on www.snort.org . They will provide a unique code after the registration.\nNow I\u0026rsquo;m going to sign up for a free account.\nREF: https://www.snort.org/ They have thousands of pre defined rule sets. These, rules are the source for the Snort System.\nPhase the Oinkmater Code in the \u0026ldquo;Snort Oikmaster Code\u0026rdquo; In order to download the free Snort rule sets.\nThis rule databases update every 24hrs..\nAnd also can use Snort GPLv2 Community Rules and the Emerging Threats (ET )Open Rules freely. You don\u0026rsquo;t need to register to use them.\nIn here we can enable both of them as well.\nSnort GPLv2 : Enabled Emergin Threats (ET) Open : Enabled OpenAppIP : Enabled RULES OpenAppID : Enabled Application detection (AppID)\nOpenAppID detector rules enables application detection and filtering facility to the Snort.\nOpenAppID has an ability to look at the application layer. Which is Layer 7. Which can look at the applications which running in the system.\nRules Update Settings Here I\u0026rsquo;m going to set the update interval into a one day.\nUpdate Interval : 1 Day General Settings Set a reasonable time for \u0026ldquo;Remove Blocked Hours Interval\u0026rdquo;. In here I\u0026rsquo;ll set 30mins.\nRemove Blocked Hours Interval : 30 Mins If someone did nasty thing in your network. That device will automatically will be blocked by the system. Then, Either you need to unblock that device manually or 30mins after it will ublocked automatically.\nNow, Hit the \u0026ldquo;Save\u0026rdquo; button.\nSTEP 03: Update Snort Rule Databases Now head over to Update tab and hit the \u0026ldquo;Update Rules\u0026rdquo; button to download the latest updates.\nSystem \u0026gt; Snort \u0026gt; Update Rules\nIt will download all required rules automatically. Initially this take a little logner time. wait untill it completed.\nSTEP 04: Add Snort To an Interface Then we need to add a WAN interace to \u0026ldquo;Snort Interface\u0026rdquo; sections. Now, let\u0026rsquo;s move to \u0026ldquo;Snort Interfaces\u0026rdquo; tab and add new Snort interface.\nGoto \u0026ldquo;Services\u0026rdquo; menu and select Snort, Then select Interface tab.\nServices \u0026gt; Snort \u0026gt; Interfaces\nSnort Interfaces \u0026gt; Add New Interface\nEnable Interface Always selecet WAN Interface Provide a Description Send Alterts to System Logs Block Offenders : Enabled Search Optimize: Enable search optimization And finally hit save\nSTEP 05: Select Which Types of Rules Will Protect The Network Head over to \u0026ldquo;Interfaces\u0026rdquo; and Select an configured interface, hit the edit button. And move to \u0026ldquo;WAN Categories\u0026rdquo; tab.\nServices \u0026gt; Snort \u0026gt; Edit Interface\u0026gt; WAN2\nSnort VRT IPS Policy Selection User IPS Policy : Enabled IPS Policy Selection : Security If you are not familiar with the Snort, I recommend you to use \u0026ldquo;Connectivity\u0026rdquo; option as a starting point.\nIn here I\u0026rsquo;m going to use \u0026ldquo;Security\u0026rdquo; as my IPS Policy.\nAnd also you can enable and select other rule sets such as GPL Community Rules, ET Open Rules and Open AppID Rules.\nSelect the rule-sets what ever you need. At this point, I\u0026rsquo;m going to enable the all ET Rules for the demostration purpose.\nIgnore rest of other settings below.\nMove down further and hit the \u0026ldquo;Save\u0026rdquo; button.\nSTEP 06: WAN Pre-processor Configuration Now, I\u0026rsquo;m going to configure Snort further. Let\u0026rsquo;s, head over to\u0026hellip;\nServices \u0026gt; Snort \u0026gt; Snort interfaces \u0026gt; WAN Pre-Processes\nEnable \u0026ldquo;Performance Stats\u0026rdquo; option if you want to have logging in depth details.\nEnable Performance Stats : If you wanna have logging in depth details through the rules.\nAuto Rule Disable : Enabled\nMove down further and go to\u0026hellip;\nApplication ID detection with OpenApp ID: Application ID Detection: Enabled Enable \u0026ldquo;Application ID Detection\u0026rdquo;. And double check both check-boxes to enable detectors and rules download for \u0026ldquo;Source fire OpenAppID Detection\u0026rdquo; section in the Global Settings.\nBetter, If rest of settings leave as it is\u0026hellip;\nThen, Hit save\nMake sure to \u0026ldquo;Update\u0026rdquo; Once again after the AppID enabled.\nSTEP 07: View Detected Apps Then, We need to know how to view Snort Alters.\nNow move to \u0026ldquo;Alerts\u0026rdquo; tab to view the detected applications.\nAt this time we don\u0026rsquo;t have any alters, but I\u0026rsquo;ll show you after the Snort service started.\nSTEP 08: Start Snort Service Let\u0026rsquo;s head-over to\nServices \u0026gt; Snort \u0026gt; Interfaces\nand click on the small play button. Then it will start the service on the WAN interface.\nAt this time you will notice, These are considarable resources are using by Snort service. Better if you are consider upgrading pfSense box hardware resources if it is pron to resource hungry when the Snort service starts.\nSTEP 09: Getting to know the alerts All the Snort logs will be recorded in the General Logs section.\nStatus \u0026gt; System Logs\nAlso you can use \u0026ldquo;Alerts\u0026rdquo; tab to view alerts generated by the Snort.\nService \u0026gt; Snort \u0026gt; Alerts\nSTEP 10: Managing blocked hosts Now, Move to Service \u0026gt; Snort \u0026gt; Blocked Hosts\nThe \u0026ldquo;blocked\u0026rdquo; tab shows that hosts are currently being blocked by Snort.\nBefore that, Make sure that you have enabled \u0026ldquo;Block Offenders\u0026rdquo; option in the selected \u0026ldquo;interface Settings\u0026rdquo; tab.\nAlso you can see what events has been blocked by Snort.\nIn here you can see most of activities going on now.\nSTEP 11: Managing Pass lists Now, Let\u0026rsquo;s move to the\u0026hellip;\nServices \u0026gt; Snort \u0026gt; Pass List\ntab.\n\u0026ldquo;Pass Lists\u0026rdquo; are lists of IP addresses that Snort should never blocked by Snort.\nIn here you can add or define \u0026ldquo;Firewall Aliases\u0026rdquo; to bypass the Snort.\nBottom Line:\nSnort on pfSense bit resource hungry application. therefore, You may need to upgrade your hardware resources on pfsense box.\nCheck the resource utilization before implemented on a production environment.\nOK, Now you may have some idea and knowledge to playing around with the Snort Intrusion Detection Service. Hope this helps you to keep your network away from unwanted attackers.\nFinally, Please don\u0026rsquo;t forget to Like, Comment and hit the Subscribe for video guides like this. See you from a new video.\nREF:\nhttps://pfsense-docs.readthedocs.io/en/latest/ids-ips/setup-snort-package.html https://turbofuture.com/internet/How-to-Set-Up-an-Intrusion-Detection-System-Using-Snort-on-pfSense-20 https://community.spiceworks.com/how_to/126207-set-up-snort-on-pfsense-for-ids-ips https://linoxide.com/firewall/install-configure-snort-pfsense-firewall/ https://vorkbaard.nl/installing-snort-for-idsips-on-pfsense-2-4/ https://www.moh10ly.com/configuring-snort-on-pfsense/ https://www.snort.org/ ","date":"December 16, 2019","image":"https://digitalavenue.dev/img/post-imgs/Snort-pfsense/snort-pfSense_hu5fd46534e0f5816ac840d283dcc492f0_233689_650x0_resize_q90_box.jpg","permalink":"/blog/2019-12-16-how_to_setup_intrusion_detection_using_snort_on_pfsense/","title":"How To Setup Intrusion Detection Using Snort on PfSense"},{"categories":["IT Security"],"contents":"pfBlockerNG extends the capability of the pfSense firewall and beyond the traditional state full firewall. pfBlockerNG provides the ability to pfSense firewall to make allow/deny decisions based upon items such as Geo-location, IP address, Alexa rating and the domain name.\nIn this guide, I will walk you through the configuration of pfBlockerNG on offence as a domain and content filter.\nWeb Site Filtering using pfBlocker using pfSense: Filtering Content with DNS: SquidGuard doesn\u0026rsquo;t work properly with mobile phone and IoT devices. Because Squid can\u0026rsquo;t do the man in the middle, most sites moved into SSL encryption. Because most site using encrypted SSL encryption. That\u0026rsquo;s why Squid can\u0026rsquo;t do the man in the middle and filter some websites.\nRequirements: pfSense should be the default DNS server that pointed to the client\u0026rsquo;s hosts.\nCreate Two Firewall Rules For DNS STEP 01: Install pfBlockerNG Package First of all, you need to install the package on the pfSense appliance. Once logged in to the main pfSense page, click on the \u0026ldquo;System\u0026rdquo; drop-down and then select \u0026ldquo;Package Manager\u0026rdquo;.\nNow head over to System \u0026gt; Package Manager \u0026gt; Available Packages.\nOnce the \u0026lsquo;Available Packages\u0026rsquo; page loads, type \u0026lsquo;pfBlocker\u0026rsquo; into the \u0026lsquo;Search term\u0026rsquo; box and click the \u0026lsquo;Search\u0026rsquo;. The first item that is returned should be pfBlockerNG. Locate the \u0026lsquo;Install\u0026rsquo; button to the right of the pfBlockerNG description and click the \u0026lsquo;+\u0026rsquo; to install the package.\nThe page will reload, then continue the installation by clicking \u0026ldquo;Confirm\u0026rdquo;.\nIn the Search section, fill the following fields:\nSearch terms: Type pfBlockerNG\nClick on the Search button.\nIn the Packages section, the pfBlockerNG will appear.\nClick on + Install and then on Confirm buttons to launch the installation.\nOnce the installation is completed, pfBlockerNG appears in System \u0026gt; Package Manager \u0026gt; Installed Packages.\nNow, the pfBlockerNG configuration can begin. Before that, we need to reconfigure our pfSense DNS resolver according to our requirement, which needs to redirect DNS requests in order to filter out bad domains. This means clients on the LAN interface need to use the pfSense as the default and primary DNS resolver.\nSTEP 02: Configure DNS Resolver Now, head over to \u0026ldquo;Services\u0026rdquo; and select \u0026ldquo;DNS Resolver\u0026rdquo; from the drop-down menu.\nEnable DNS Resolver\nSet the listening port 53\nIngress Network interface should be LAN \u0026amp; localhost\nEgress Outgoing Network Interface should be WAN interfaces\nFinally, hit \u0026ldquo;Save\u0026rdquo; \u0026amp; \u0026ldquo;Apply Changes\u0026rdquo;.\nSTEP 03: Configure pfBlockerNG - General Settings The next step is the configuration of pfBlocker specifically.\nHead over to the general tab do the following changes.\nLet\u0026rsquo;s move to Firewall \u0026gt; pfBlockerNG \u0026gt; General.\nIn General Settings section, fill the following fields:\nEnable pfBlockerNG: Checked Keep Settings: Checked Cron Settings: Select Every hour, select 0 as a minute, hour and Daily/Weekly Move down a bit further.\nIn the Interface/Rules Configuration section, fill the following fields:\nInbound Firewall Rules: Select WAN interfaces and Block Outbound Firewall Rules: Select LAN interfaces and Reject The rest of the other settings leave as it is.\nAnd click save.\nSTEP 04: Configure DNSBL Now move to the DNSBL tab and do the following changes.\nTo configure DNSBL, go to Firewall \u0026gt; pfBlockerNG \u0026gt; DNSBL \u0026gt; DNSBL\nIn the DNSBL section, fill the following fields:\nEnable DNSBL: Checked Enable TLD: Not checked DNSBL Virtual IP: Enter an IP address that is not in your internal networks, something like 10.10.10.10 DNSBL Listening Port: Enter 8081 DNSBL SSL Listening Port: Enter 8443 DNSBL Listening Interface: Select LAN or another internal interface to listen on DNSBL Firewall Rule: Checked if you have multiple LAN interfaces In the DNSBL IP Firewall Rule Settings section, fill the following fields:\nList Action: Select Deny Both\nEnable Logging: Select Enable\nIn Advanced Inbound Firewall Rule Settings: Don\u0026rsquo;t change\nIn Advanced Outbound Firewall Rule Settings: Don\u0026rsquo;t change\nIn Alexa Whitelist: Don\u0026rsquo;t change\nIn Custom Domain Whitelist :\nTo begin, enter the following white-list domains and modify them as you like.\n.play.google.com .drive.google.com .accounts.google.com .www.google.com .github.com .outlook.live.com .edge-live.outlook.office.com # CNAME for (outlook.live.com) .outlook.ha-live.office365.com # CNAME for (outlook.live.com) .outlook.ha.office365.com # CNAME for (outlook.live.com) .outlook.ms-acdc.office.com # CNAME for (outlook.live.com) .amazonaws.com .login.live.com .login.msa.akadns6.net # CNAME for (login.live.com) .ipv4.login.msa.akadns6.net # CNAME for (login.live.com) .mail.google.com .googlemail.l.google.com # CNAME for (mail.google.com) .pbs.twimg.com .wildcard.twimg.com # CNAME for (pbs.twimg.com) .sites.google.com .www3.l.google.com # CNAME for (sites.google.com) .docs.google.com .mobile.free.fr .plus.google.com .samsungcloudsolution.net .samsungelectronics.com .icloud.com .microsoft.com .windows.com .skype.com .googleusercontent.com In TLD Exclusion List: Don\u0026rsquo;t change In TLD Blacklist: Don\u0026rsquo;t change In TLD Whitelist: Don\u0026rsquo;t change Click on the Save button once all field is filling.\nSTEP 05: Configure DNSBL Feeds To configure DNSBL feeds, go to Firewall \u0026gt; pfBlockerNG \u0026gt; DNSBL \u0026gt; DNSBL Feeds\nClick on the + Add button.\nGo to this URL and add preferred lists.\nhttps://github.com/StevenBlack/hosts In DNSBL Feeds section, fill the following fields:\nDNS GROUP Name: Enter DNSBlockListGroup Description: Enter DNS Blocklist DNSBL Settings – These are the actual lists State – Whether that source is used or not and how it is obtained Source – The link/source of the DNS Black List Header/Label – User choice; no special characters List Action – Set to Unbound Update Frequency – How often the list should be updated You also can define custom blacklist domains. Now, save.\nNext, head over to the \u0026ldquo;Update\u0026rdquo; tab.\nSelect \u0026ldquo;update\u0026rdquo; tick box and hit the run button.\nNow, It will start to update the blacklist index.\nNow, head over to \u0026ldquo;Status\u0026rdquo; tab and select \u0026ldquo;Services\u0026rdquo; from the drop-down menu.\nNow, restart both dnsbl \u0026amp; unbound services.\nSTEP 05: Test Effectiveness by Browsing on Client Side. Reference:\nHere are the required resources to complete this tutorial:\nMy Website:\nhttps://digitalave.github.io/spring/2019/12/02/site_domain_filtering_using-pfblocker_on_pfsense.html Steven\u0026rsquo;s Block List:\nhttps://github.com/StevenBlack/hosts YouTube Black List:\n3.https://raw.githubusercontent.com/digitalave/digitalave.github.io/master/Config/youtube\nPi-Hole Black List: https://github.com/pi-hole/pi-hole/wiki/Customising-sources-for-ad-lists ","date":"December 2, 2019","image":"https://digitalavenue.dev/img/post-imgs/pfBlocker-pfsense/pfblocker_N_hu36344aff508f26aa221582199832ce36_186973_650x0_resize_q90_box.jpg","permalink":"/blog/2019-12-02-site_domain_filtering_using-pfblocker_on_pfsense/","title":"Internet Content Filtering and Site Blocking Using pfBlockerNG on pfSense"},{"categories":["Linux","Monitoring"],"contents":"In my previous video, I instructed you on how to install Nagios, configure the basics, and have your network monitored. Today, I am going to show you how to configure Nagios for email alerts that can be associated with certain services to certain administrators.\nBefore the installation begin, We need to setup an email relay server on the Nagios server. Which use to send an automated emails when alerts are triggered.\nSTEP 01: Install \u0026amp; Configure Postfix Mail Server 1. Install Postfix Package [root@nagios ~]# systemctl restart postfix 2. Create SASL Password to able to authenticate with the Gmail server Open or create the /etc/postfix/sasl/sasl_passwd file and add the SMTP Host, username, and password information: Store Password Inside /etc/postfix/sasl_passwd\n[smtp.gmail.com]:587 \u0026lt;GMAIL_ID\u0026gt;@gmail.com:\n[root@nagios ~]# vim /etc/postfix/sasl/sasl_passwd [smtp.gmail.com]:587 username@gmail.com:exhhwavdchsihk3l 6. Create a HASH DB for the password Create the hash db file for Postfix by running the postmap command:\npostmap /etc/postfix/sasl/sasl_passwd Compile password to db (we can safely remove previous clear-text file now)\n7. Set required ownership/permissions for the following files: sudo chown root:root /etc/postfix/sasl/sasl_passwd /etc/postfix/sasl/sasl_passwd.db sudo chmod 0600 /etc/postfix/sasl/sasl_passwd /etc/postfix/sasl sasl_passwd.db 8. Modify relayhost directive in /etc/postfix/main.cf to match the following example: Define the following directives at the end of /etc/postfix/main.cf file:\n# This tells Postfix to hand off all messages to Gmail, and never do direct delivery. relayhost = [smtp.gmail.com]:587 # This tells Postfix to provide the username/password when Gmail asks for one. # Enable SASL authentication smtp_sasl_auth_enable = yes # Disallow methods that allow anonymous authentication smtp_sasl_security_options = noanonymous # Location of sasl_passwd smtp_sasl_password_maps = hash:/etc/postfix/sasl/sasl_passwd # Enable STARTTLS encryption smtp_tls_security_level = encrypt # Location of CA certificates smtp_tls_security_level = verify smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt 9: Enable \u0026amp; Restart Postfix Service [root@nagios ~]# systemctl enable postfix [root@nagios ~]# systemctl restart postfix 10: Sending an Test Email: echo \u0026#34;Mail Body - Test Message from NAgiso Digital Avenue\u0026#34; | mailx -vvv -s \u0026#34;Subjct is Mail Sending from Digital Avenue\u0026#34; username@gmail.com STEP 02: Define Email Notification Commands [root@nagios ~]# vim /usr/local/nagios/etc/objects/commands.cfg # \u0026#39;notify-host-by-email\u0026#39; command definition define command { command_name notify-host-by-email command_line /usr/bin/printf \u0026#34;%b\u0026#34; \u0026#34;***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\nHost: $HOSTNAME$\\nState: $HOSTSTATE$\\nAddress: $HOSTADDRESS$\\nInfo: $HOSTOUTPUT$\\n\\nDate/Time: $LONGDATETIME$\\n\u0026#34; | mailx -vvv -s \u0026#34;** $NOTIFICATIONTYPE$ Host Alert: $HOSTNAME$ is $HOSTSTATE$ **\u0026#34; $CONTACTEMAIL$ } # \u0026#39;notify-service-by-email\u0026#39; command definition define command { command_name notify-service-by-email command_line /usr/bin/printf \u0026#34;%b\u0026#34; \u0026#34;***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\n\\nService: $SERVICEDESC$\\nHost: $HOSTALIAS$\\nAddress: $HOSTADDRESS$\\nState: $SERVICESTATE$\\n\\nDate/Time: $LONGDATETIME$\\n\\nAdditional Info:\\n\\n$SERVICEOUTPUT$\\n\u0026#34; | mailx -vvv -s \u0026#34;** $NOTIFICATIONTYPE$ Service Alert: $HOSTALIAS$/$SERVICEDESC$ is $SERVICESTATE$ **\u0026#34; $CONTACTEMAIL$ } STEP 03: Define Email Contacts and Groups [root@nagios ~]# vim /usr/local/nagios/etc/objects/contacts.cfg Once you have that done all you will need is the email addresses you want to use for your alerts. Define contact section and define email address to need to be get notified.\nModify \u0026ldquo;contact_name\u0026rdquo; and \u0026ldquo;email\u0026rdquo; address\ndefine contact { contact_name nagiosadmin ; Short name of user use generic-contact ; Inherit default values from generic-contact template (defined above) alias Nagios Admin ; Full name of user email username1@gmail.com ; \u0026lt;\u0026lt;***** CHANGE THIS TO YOUR EMAIL ADDRESS ****** } ############### NEW CONTACT ################### define contact{ contact_name webadmin use generic-contact alias Web Admin email username1@orelit.com service_notification_commands notify-service-by-email host_notification_commands notify-host-by-email service_notification_period 24×7 host_notification_period 24×7 service_notification_options w,u,c,r,f host_notification_options d,u,r,f } w = notify on warning states c = critical states r = recovery f = start/stop of flapping d = notify on down states u = notify on unreachable states s = notify on stopped states\nYou are now ready to move on. The next section will be to define a contact group. Contact groups allow you to group people together so it is easier to alert specific people to certain events.\nEach group would have a specific user (or users) associated with it who would be alerted if a problem arises.\nDefine email group to which the mail need to be sent.\n[root@nagios ~]# vim /usr/local/nagios/etc/objects/contacts.cfg ############# NEW CONTACT GROUPS ############## define contactgroup{ contactgroup_name admins alias Nagios Administrators members webadmin,mailadmin,nagiosadmin } ############################################### STEP 04: Define Contact Details in the Host \u0026amp; Service Configuration Section Once you have defined all of your groups, save that file and close it. Now you have to attach groups to services so those groups will be alerted when something is wrong with their specific service. To do this open up the file\u0026hellip;\n[root@nagios ~]# vim /usr/local/nagios/etc/hosts/cl2.cfg Head over to host configuration file and define \u0026ldquo;contacts\u0026rdquo; and \u0026ldquo;contact_groups\u0026rdquo;\n# Host configuration Section define host { use linux-server host_name cl2 alias CentOS 7 - Server address 172.25.10.100 register 1 contact_groups admins } Also you can define \u0026ldquo;contacts\u0026rdquo; and \u0026ldquo;contact_groups\u0026rdquo; on the\n## SNMP Service Definiton - Uptime define service{ use generic-service host_name cl2 service_description System uptime check_command SNMP-Uptime!-C digitalavenue contacts username@gmail.com } STEP 05: VERIFY NAGIOS CONFIGS AND CHECK FOR ERROR [root@nagios ~]# /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg Now restart Nagios server to reflect the configuration changes.\n[root@nagios ~]# systemctl restart nagios httpd Once your changes have been made, restart Nagios. You can test notifications via the Nagios web portal. Follows these steps:\nClick “Services” in the left menu. Click on any failed service (yellow) in the main area. Click “Send custom service notifications\u0026quot; in the right menu. Enter a custom \u0026ldquo;comment\u0026rdquo; and click the \u0026ldquo;commit\u0026rdquo; button. Check your inbox for your notification email. If you don’t get it, check your server logs to see if there is a system issue on the Nagios server.\n","date":"October 25, 2019","image":"https://digitalavenue.dev/img/post-imgs/nagios-snmp/1_N_hu96497b785ebc09501f53597695ad8820_213631_650x0_resize_q90_box.jpg","permalink":"/blog/2019-10-25-how_to_send_alerts_from_nagios_using_gmail/","title":"How To Send Alerts From Nagios Core Using Gmail"},{"categories":["Monitoring","Linux"],"contents":" Nagios is one of the most popular industry slandered enterprise class server \u0026amp; network monitoring solution.\nNagios core is free and open source tool which allows you to monitor your entire IT infrastructure to ensure host, services and applications functioning properly.\nIn this article I\u0026rsquo;m going to demonstrate how to monitor Linux host clients with Nagios Core using with SNMP.\nSNMP. Which stands for Simple Network Management Protocol..\nIf you have already installed and setup Nagios Core Now Its time to continue with me.\nIn the previous two videos I demonstrated how to install Nagios core on CentOS 7 and Monitor Remote Hosts and Services using NRPE agent plugin.\nREMOTE CLIENT SIDE CONFIGURATION: STEP 01: Install and Configure SNMP on Remote CentOS 7 Host Now, install SNMP and SNMP-UTILLS with some dependencies.\n[root@cl2 ~]# yum -y install net-snmp net-snmp-utils STEP 02: Backup Default SNMP configuration file. Let\u0026rsquo;t make backup of \u0026ldquo;snmpd.conf\u0026rdquo; default configuration file.\n[root@cl2 ~]# cp /etc/snmp/snmpd.conf /etc/snmp/snmpd.conf.orig STEP 03: Configure \u0026ldquo;snmpd.conf\u0026rdquo; File And do the following modifications into \u0026ldquo;snmpd.conf\u0026rdquo;\n[root@cl2 ~]# vim /etc/snmp/snmpd.conf Insert following text content into the newly created \u0026ldquo;snmpd.conf\u0026rdquo; file. Change the \u0026ldquo;community name\u0026rdquo; from public to something more secure. Edit the file “/etc/snmp/snmpd.conf” using this example as a guide:\nIn this case I\u0026rsquo;m going to add \u0026ldquo;digitalavenue\u0026rdquo; as my snmp community string. By default it would be as \u0026ldquo;public\u0026rdquo;\n# First, map the community name \u0026#34;public\u0026#34; into a \u0026#34;security name\u0026#34; # sec.name source community #com2sec notConfigUser default public com2sec danetwork 172.25.10.0/24 digitalavenue com2sec danetwork 127.0.0.1 digitalavenue #### # Second, map the security name into a group name: # groupName securityModel securityName #group notConfigGroup v1 notConfigUser #group notConfigGroup v2c notConfigUser group DAGROUP v1 danetwork group DAGROUP v2c danetwork #### # Third, create a view for us to let the group have rights to: # Make at least snmpwalk -v 1 localhost -c public system fast again. # name incl/excl subtree mask(optional) #view systemview included .1.3.6.1.2.1.1 #view systemview included .1.3.6.1.2.1.25.1.1 view all included .1 #### # Finally, grant the group read-only access to the systemview view. # group context sec.model sec.level prefix read write notif #access notConfigGroup \u0026#34;\u0026#34; any noauth exact systemview none none access DAGROUP \u0026#34;\u0026#34; any noauth exact all none none #syscontact Root \u0026lt;root@localhost\u0026gt; (configure /etc/snmp/snmp.local.conf) syslocation Rack01,Server Room 01,Digital Avenue, Colombo (edit /etc/snmp/snmpd.conf) syscontact Admin digitalavenue@gmail.com (configure /etc/snmp/snmp.local.conf) Also you can define physical location of your server and relevant contact email.\nNow check weather it\u0026rsquo;s working by entering this command. snmpwalk -v2c -c digitalavenue localhost:161 You will get plenty of information.\nSTEP 05: Firewall Configuration Now I\u0026rsquo;m going to allow SNMP service port through the Firewall So, Make sure to allow UDP port “161” through your firewall/router to this SNMP service.\n[root@cl1 ~]# firewall-cmd --permanent --add-port=161/udp [root@cl1 ~]# firewall-cmd --reload STEP 06: Restart SNMPD Service Restart \u0026ldquo;snmpd.service\u0026rdquo; to reflect the changes And enable the service to start at system boots.\n[root@cl1 ~]# systemctl enable snmpd.service [root@cl1 ~]# systemctl restart snmpd.service STEP 07: Test SNMP Service Now, This command will return plenty of information in the SNMP installed host.\nReplace \u0026ldquo;digitalavenue\u0026rdquo; my community string with your snmp community string that have mentioned in the \u0026ldquo;snmpd.conf\u0026rdquo; file.\n[root@cl2 ~]# snmpwalk -v2c -c digitalavenue localhost:161 NAGIOS SERVER SIDE CONFIGURATIONS: STEP 01: Add Command Definitions Now I\u0026rsquo;m going to use the default Nagios SNMP monitoring plugin, check_snmp,check_snmp_storage.pl, check_snmp_storage.pl,check_snmp_load.pl, check_snmp_int.pl scripts to uptime, Physical Memory, Partition Size, Process, CPU Load and more.\nBy defult, if check you will find all nagios plugins under /usr/local/nagios/libexec/ directory.\nYou can customize your commands your own way. so, please take some time to check carefully for each plugin and their syntax.\n[root@nagios ~]# ./check_dns -h Usage: check_dns -H host [-s server] [-q type ] [-a expected-address] [-A] [-n] [-t timeout] [-w warn] [-c crit] Generally most of snmp scripts comes with -H for host IP, -w for warning level, -c for critical level and -C for snmp community string.\nAnd sometimes you may need to add some specific values which depends on your script that you going to use.\nSOME EXAMPLE COMMANDS: Check for Up-time\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.2.1.1.3.0\nRunning processes\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.2.1.25.1.6.0 -w 300 -c 400\nLoad Average\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.4.1.2021.10.1.3.1 -w 2.0 -c 5.0\nLogged In Users\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.2.1.25.1.5.0 -w 5 -c 10\nTotal Physical Memory Available\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.4.1.2021.4.5.0 -w %50 -c %75\nTotal Physical Memory Used\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.4.1.2021.4.6.0 -w %75 -c %90\nTotal Physical Memory Used\n/usr/local/nagios/libexec/check_snmp -H 172.25.10.110 -C digitalavenue -o .1.3.6.1.4.1.2021.4.11.0 -w %30 -c %25\nOptionally you can collect snmp OIDs and thier names to retrive specific data and build your own commands. 1. Run snmpwalk to view OID name\nsnmpwalk -v 1 -c digitalavenue 172.25.10.110 make sure to change -c with your snmp community string\n2. Pick one, ie: HOST-RESOURCES-MIB::hrStorageSize.1 (Total Physical Memory)\n3. Use snmptranslate to get OID number\nsnmptranslate -On HOST-RESOURCES-MIB::hrStorageSize.1 4. Check the OID number with check_snmp manually\n/usr/local/nagios/libexec/check_snmp -H 192.168.56.1 -C digitalavenue -o .1.3.6.1.2.1.25.2.3.1.5.1 5. And create your own command definition\ndefine command{ command_name check_snmp_total_memory command_line $USER1$/check_snmp -H $HOSTADDRESS$ -C digitalavenue -o $ARG1$ } 6. Finally need to create a service definition\nAdd values to host-name and service_description accordingly.\ndefine service{ use generic-service host_name cl2 service_description Total Physical Memory check_command check_snmp_total_memory!.1.3.6.1.2.1.25.2.3.1.5.1 } And one another thing to keep in mind is all these arguments should separated by ! mark.\nHeadover to \u0026ldquo;/usr/local/nagios/etc/objects/commands.cfg\u0026rdquo; file and define command definitons. Which will be used to check uptime, disk usage, physical memory used.\n[root@nagios ~]# vim /usr/local/nagios/etc/objects/commands.cfg ######################## USER DEFINED COMMANDS ############################# ########## SNMP COMMAND CONFIGURATION SECTION ## check_snmp plugin commands definitions ## SNMP Command Definition - Up time define command{ command_name SNMP-Uptime command_line $USER1$/check_snmp -o 1.3.6.1.2.1.25.1.1.0 -H $HOSTADDRESS$ $ARG1$ } ## SNMP Command Definition - Total RAM installed define command{ command_name SNMP-TotalRAMInstalled command_line $USER1$/check_snmp -o 1.3.6.1.4.1.2021.4.5.0 -H $HOSTADDRESS$ $ARG1$ } ## SNMP Command Definition - Total Physical Memory define command{ command_name HOST-RESOURCES-MIB::hrStorageSize.1 command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.2.3.1.5.1 -H $HOSTADDRESS$ $ARG1$ } ## SNMP Command Definition - Used Physical Memory define command{ command_name HOST-RESOURCES-MIB::hrStorageUsed.1 command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.2.3.1.6.1 -H $HOSTADDRESS$ $ARG1$ } ## SNMP Command Definition - Cached Physical Memory define command{ command_name HOST-RESOURCES-MIB::hrStorageSize.7 command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.2.3.1.5.7 -H $HOSTADDRESS$ $ARG1$ } ## SNMP Command Definition - Buffered Physical Memory define command{ command_name HOST-RESOURCES-MIB::hrStorageDescr.6 command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.2.3.1.5.6 -H $HOSTADDRESS$ $ARG1$ } # Total ROOT Partition Size define command{ command_name HOST-RESOURCES-MIB::hrStorageDescr.31 command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.2.3.1.3.31 -H $HOSTADDRESS$ $ARG1$ } # Used ROOT Partition Size define command{ command_name HOST-RESOURCES-MIB::hrStorageUsed.31 command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.2.3.1.6.31 -H $HOSTADDRESS$ $ARG1$ } # Running processes define command{ command_name SNMP-TotalProccessRunning command_line $USER1$/check_snmp -o .1.3.6.1.2.1.25.1.6.0 -H $HOSTADDRESS$ $ARG1$ } # CPU Load 1m define command{ command_name snmp_load_1m command_line $USER1$/check_snmp -o .1.3.6.1.4.1.2021.10.1.3.1 -H $HOSTADDRESS$ $ARG1$ } # CPU Load 15m define command{ command_name snmp_load_15m command_line $USER1$/check_snmp -o .1.3.6.1.4.1.2021.10.1.3.3 -H $HOSTADDRESS$ $ARG1$ } ## check_snmp_storage command definitions ## SNMP Command Definiton - DISK define command{ command_name\tcheck_snmp_storage command_line\t$USER1$/check_snmp_storage.pl -H $HOSTADDRESS$ $USER8$ -m $ARG1$ -w $ARG2$ -c $ARG3$ $ARG4$ -C digitalavenue } ## check_snmp_mem command definitions ## SNMP Command Definition - Physical Memory define command{ command_name\tcheck_snmp_mem command_line\t$USER1$/check_snmp_storage.pl -H $HOSTADDRESS$ $USER8$ -m $ARG1$ -w $ARG2$ -c $ARG3$ $ARG4$ -C digitalavenue } ## check_snmp_process command definitions ## SNMP Command Definition - Process define command{ command_name\tcheck_snmp_process command_line\t$USER1$/check_snmp_process.pl -H $HOSTADDRESS$ $USER7$ -n $ARG1$ -w $ARG2$ -c $ARG3$ $ARG4$ -C digitalavenue } ## check_snmp_load command definitions ## SNMP Command Definiton - CPU Load define command{ command_name\tcheck_snmp_load command_line\t$USER1$/check_snmp_load.pl -H $HOSTADDRESS$ $USER7$ -T $ARG1$ -w $ARG2$ -c $ARG3$ $ARG4$ -C digitalavenue } ## check_snmp_int command definitions ## SNMP Command Definition - Interface define command{ command_name\tcheck_snmp_iface command_line\t$USER1$/check_snmp_int.pl -H $HOSTADDRESS$ $USER7$ -n $ARG1$ -w $ARG2$ -c $ARG3$ -C digitalavenue } ## \u0026#39;check_dns\u0026#39; command definitions ## SNMP Command Definition - DNS define command { command_name\tcheck_dns command_line\t$USER1$/check_dns -H digitalave.github.io -s 8.8.8.8 -t 15 | sed -e \u0026#34;s/returns.*//g\u0026#34; } ########## checking using NRPE/Script ############# ## check_volume command definition define command{ command_name\tcheck_volume command_line\t$USER1$/check_volume.sh -v $ARG1$ -w $ARG2$ -c $ARG3$ } STEP 02: CREATE CONFIG DIRECTORY Now, I will add the corresponding service definitions to apply the above commands that I\u0026rsquo;ve added into commands.cfg file.\nCreate a directory that will store the configuration files for each server that you will need to monitor.\n[root@nagios ~]# sudo mkdir /usr/local/nagios/etc/hosts STEP 03: CREATE HOST/SERVICE DEFINITION FILE Create a new configuration file for each of the remote hosts that we want to monitor.\nIn this case I’m going to put all the host and service definitions into a single file.\n[root@nagios ~]# vim /usr/local/nagios/etc/hosts/cl2.cfg Now we need to add new host \u0026amp; service entries for each remote server you will need to monitor.\nNow I create a new cl2.cfg file for remote host which contain host \u0026amp; service definitions all together.\nAdd host definition section and change \u0026ldquo;host_name\u0026rdquo; and \u0026ldquo;address\u0026rdquo; sections.\nfinally we need to mention each service you want to monitor.\n[root@nagios ~]# vim /usr/local/nagios/etc/hosts/cl2.cfg # Host configuration Section define host { use linux-server host_name cl2 alias CentOS 7 - Server address 172.25.10.100 register 1 contact_groups admins } # Service Configuration Section ########## SNMP SERVICE CONFIGURATION SECTION ## check_snmp plugin service definitions ## SNMP Service Definition - Up time define service{ use generic-service host_name cl2 service_description System uptime check_command SNMP-Uptime!-C digitalavenue } ## SNMP Service Definition - Total RAM installed define service{ use generic-service host_name cl2 service_description Total Memory Installed check_command SNMP-TotalRAMInstalled!-C digitalavenue } ## SNMP Service Definition - Total Physical Memory define service{ use generic-service host_name cl2 service_description Total Physical Memory check_command HOST-RESOURCES-MIB::hrStorageSize.1!-C digitalavenue } ## SNMP Service Definition - Used Physical Memory define service{ use generic-service host_name cl2 service_description Used Physical Momory check_command HOST-RESOURCES-MIB::hrStorageUsed.1!-C digitalavenue } ## SNMP Service Definition - Cached Physical Memory define service{ use generic-service host_name cl2 service_description Cached Physical Momory check_command HOST-RESOURCES-MIB::hrStorageSize.7!-C digitalavenue } ## SNMP Service Definition - Buffered Physical Memory define service{ use generic-service host_name cl2 service_description Buffered Physical Momory check_command HOST-RESOURCES-MIB::hrStorageDescr.6!-C digitalavenue } ## SNMP Service Definition - Total Running Process define service{ use generic-service host_name cl2 service_description Total Running Process check_command SNMP-TotalProccessRunning!-C digitalavenue } ## SNMP Service Definition - CPU Load 1M define service{ use generic-service host_name cl2 service_description CPU Load 1Minute check_command snmp_load_1m!-C digitalavenue } ## SNMP Service Definition - CPU Load 15M define service{ use generic-service host_name cl2 service_description CPU Load 15Minute check_command snmp_load_15m!-C digitalavenue } # check_snmp_storage service definitions ## SNMP Service Definition - DISK ROOT define service{ use generic-service host_name cl2 service_description Disk Root check_command check_snmp_storage!\u0026#34;^/$\u0026#34;!80!90 } # SNMP Service Definition - DISK BOOT define service{ use generic-service host_name cl2 service_description Disk Boot check_command check_snmp_storage!^/boot!80!90 } # check_snmp_mem service definitions # SNMP Service Definition - Memory define service { use generic-service host_name cl2 service_description Memory check_command check_snmp_mem!Physical Memory!80!90 } # check_snmp_process service definitions # SNMP Service Definition - Process SNMPD define service { use generic-service host_name cl2 service_description Process SNMPD check_command check_snmp_process!snmpd!5,100!0!-2 -m 20,30 -u 90,99 } # SNMP Service Definition - Process HTTPD define service { use generic-service host_name cl2 service_description Process HTTPD check_command check_snmp_process!httpd!5,100!0!-2 -m 20,30 -u 90,99 } # SNMP Service Definition - CPU Load define service { use generic-service host_name cl2 service_description CPU Load check_command check_snmp_load!netsc!50!90 } # SNMP Service Definition - CPU Load define service { use generic-service host_name cl2 service_description Interface Status check_command check_snmp_iface!enp0s3!enp0s3!100,50!0,0 } ########## checking using NRPE/Script ############# # \u0026#34;check_volume.sh\u0026#34; Service Definition - Partition Root define service{ use local-service host_name cl2 service_description Partition ROOT check_command check_volume!/!75!90 } # \u0026#34;check_volume.sh\u0026#34; Service Definition - Partition Root define service{ use local-service host_name cl2 service_description Partition BOOT check_command check_volume!/boot!75!90 } # \u0026#34;check_volume.sh\u0026#34; Service Definition - Partition Root define service{ use local-service host_name cl2 service_description Partition HOME check_command check_volume!/home!75!90 } STEP 04: Add Host Definition File Path Into Nagios Main Configuration File. [root@nagios etc]# vim /usr/local/nagios/etc/nagios.cfg cfg_file=/usr/local/nagios/etc/hosts/cl2.cfg STEP 05: VERIFY NAGIOS CONFIGS AND CHECK FOR ERROR At last, verify Nagios configuration files for any errors.\n/usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg STEP 06: Restart Nagios Service Finally restart nagios server.\n[root@nagios /]# systemctl restart nagios httpd Now, headover to the Nagios Web Interface. And then you will see newly added host and its service status.\nI hope, now you will have a clear understanding of how to add Linux hosts into nagios monitoring using SNMP agent.\nIf you find this video helpful, Please subscribe my channel to keep in touch with new video guides like this.\n","date":"October 25, 2019","image":"https://digitalavenue.dev/img/post-imgs/nagios-snmp/1_N_hu96497b785ebc09501f53597695ad8820_213631_650x0_resize_q90_box.jpg","permalink":"/blog/2019-10-25-monitor-linux-server-with-nagios-core-using-snmp/","title":"Monitor Linux Server With Nagios Core Using SNMP"},{"categories":["Linux"],"contents":" In this tutorial I\u0026rsquo;m going to tech you how to configure an email relay server for the with your Gmail account. Which helps you to send email through the Linux terminal and email automation tasks such as Nagios, Zabbix, and many more other scripts.\nHere I\u0026rsquo;m going to set up the email server with authentication using smtp.gmail.com as a relay host in the following way\nSTEP 01: Generate App Password For Postfix\n1. First you need to have a Gmail email account. Login to Google account from here:\nManage your account access and security settings.\nScroll down to “Password \u0026amp; sign-in method\u0026quot; and click 2-Step Verification. Then enable it.\nhttps://myaccount.google.com/security 2. Click the following link to Generate an App password for Postfix. https://security.google.com/settings/security/apppasswords 3. Genarate an App Passwaord Click Select app and choose Other (provide any name) from the drop-down menu. Enter “Postfix” and click Generate . And save the password.\nSTEP 02: Install \u0026amp; Configure Postfix Mail Server\n1. Install Postfix Package [root@nagios ~]# systemctl restart postfix 2. Install SASL binaries and libraries [root@nagios ~]# sudo yum install cyrus-sasl cyrus-sasl-lib cyrus-sasl-plain 3. Create SASL Password to able to authenticate with the Gmail server Open or create the /etc/postfix/sasl/sasl_passwd file and add the SMTP Host, username, and password information: Store Password Inside /etc/postfix/sasl_passwd\n[smtp.gmail.com]:587 \u0026lt;GMAIL_ID\u0026gt;@gmail.com:\u0026lt;APP PASSWORD\u0026gt; [root@nagios ~]# vim /etc/postfix/sasl/sasl_passwd [smtp.gmail.com]:587 username@gmail.com:exhhwavdchsihk3l 6. Create a HASH DB for the password Create the hash db file for Postfix by running the postmap command:\npostmap /etc/postfix/sasl/sasl_passwd Compile password to db (we can safely remove previous clear-text file now)\n7. Set required ownership/permissions for the following files: sudo chown root:root /etc/postfix/sasl/sasl_passwd /etc/postfix/sasl/sasl_passwd.db sudo chmod 0600 /etc/postfix/sasl/sasl_passwd /etc/postfix/sasl sasl_passwd.db 8. Modify relayhost directive in /etc/postfix/main.cf to match the following example: Define the following directives at the end of /etc/postfix/main.cf file:\n# This tells Postfix to hand off all messages to Gmail, and never do direct delivery. relayhost = [smtp.gmail.com]:587 # This tells Postfix to provide the username/password when Gmail asks for one. # Enable SASL authentication smtp_sasl_auth_enable = yes # Disallow methods that allow anonymous authentication smtp_sasl_security_options = noanonymous # Location of sasl_passwd smtp_sasl_password_maps = hash:/etc/postfix/sasl/sasl_passwd # Enable STARTTLS encryption smtp_tls_security_level = encrypt # Location of CA certificates smtp_tls_security_level = verify smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt Either, You can add following entries using \u0026ldquo;postconf -e\u0026rdquo; command\npostconf -e relayhost=[smtp.gmail.com]:587 postconf -e smtp_sasl_auth_enable=yes postconf -e smtp_sasl_security_options=noanonymous postconf -e smtp_sasl_password_maps=hash:/etc/postfix/sasl/sasl_passwd postconf -e smtp_tls_security_level=encrypt postconf -e smtp_tls_security_level=verify postconf -e smtp_tls_CAfile=/etc/ssl/certs/ca-bundle.crt Finally \u0026ldquo;main.cf\u0026rdquo; file will be look alike this\u0026hellip;\nalias_database = hash:/etc/aliases alias_maps = hash:/etc/aliases command_directory = /usr/sbin config_directory = /etc/postfix daemon_directory = /usr/libexec/postfix data_directory = /var/lib/postfix debug_peer_level = 2 debugger_command = PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin ddd $daemon_directory/$process_name $process_id \u0026amp; sleep 5 html_directory = no inet_interfaces = all inet_protocols = ipv4 mail_owner = postfix mailq_path = /usr/bin/mailq.postfix manpage_directory = /usr/share/man mydestination = $myhostname, localhost.$mydomain, localhost newaliases_path = /usr/bin/newaliases.postfix queue_directory = /var/spool/postfix readme_directory = /usr/share/doc/postfix-2.10.1/README_FILES relayhost = [smtp.gmail.com]:587 sample_directory = /usr/share/doc/postfix-2.10.1/samples sendmail_path = /usr/sbin/sendmail.postfix setgid_group = postdrop smtp_always_send_ehlo = yes smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl/sasl_passwd smtp_sasl_security_options = noanonymous smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt smtp_tls_security_level = verify unknown_local_recipient_reject_code = 550 8 Set Sender Email Address in /etc/aliases File [root@nagios ~]# vim /etc/aliases root: Mail_ID@gmail.com 9: Enable \u0026amp; Restart Postfix Service [root@nagios ~]# systemctl enable postfix [root@nagios ~]# systemctl restart postfix 10: Sending an Test Email: echo \u0026#34;Mail Body - Test Message from NAgiso Digital Avenue\u0026#34; | mailx -vvv -s \u0026#34;Subjct is Mail Sending from Digital Avenue\u0026#34; username@gmail.com ","date":"October 25, 2019","image":"https://digitalavenue.dev/img/post-imgs/postfix-gmail/1_N_huf40cc6b71c791466a35ab2e4a9f18415_180740_650x0_resize_q90_box.jpg","permalink":"/blog/2019-10-25-setup-postfix-to-send-emails-using-gmail-relay/","title":"Setup Postfix To Send Emails Using Gmail Relay"},{"categories":null,"contents":" In this video, I will show you how to add remote Linux client host and it\u0026rsquo;s services to Nagios Monitoring Server using NRPE agent.\nIn my previous video, I demonstrated how to install Nagios server on Centos7. I hope you already have installed Nagios server.\nThe NRPE is stands for Nagios Remote Plugin Executor. As its says by the name, NRPE allows Nagios server to discover client host resources and their services through the network.\nREMOTE CLEINT SIDE CONFIGURATION: A. Install Prerequisites We need to install required libraries.\n[root@cl1 ~]# yum install -y gcc glibc glibc-common openssl openssl-devel perl wget B. Install Latest EPEL Repository And Update The System. Install Latest EPEL YUM Repository\n[root@cl1 ~]# wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm [root@cl1 ~]# rpm -Uvh epel-release-latest-7.noarch.rpm [root@cl1 ~]# yum -y update C. Install (NRPE v3) Nagios Remote Plugin Executor [root@cl1 ~]# yum -y install nrpe D. Update NRPE Configuration File Add the private IP address of the Nagios server\nallowed_hosts=\u0026lt;Nagios_Server_IP\u0026gt;\n[root@cl1 ~]# vim /etc/nagios/nrpe.cfg allowed_hosts=127.0.0.1,172.25.10.50 E. Allow NRPE service Port Through the Firewall Allow TCP port 5666 through the firewall to be able to listening through the TCP port 5666\n[root@cl1 ~]# firewall-cmd --permanent --add-port=5666/tcp [root@cl1 ~]# firewall-cmd --reload F. Enable NRPE Service To Start at Boot [root@cl1 ~]# systemctl enable nrpe.service [root@cl1 ~]# systemctl restart nrpe.service [root@cl1 ~]# systemctl status -l nrpe.service NAGIOS SERVER SIDE CONFIGURATION: A. Create Config a Directory Create a directory that will store the configuration files for each server that you will need to monitor.\n[root@nagios ~]# sudo mkdir /usr/local/nagios/etc/hosts B. Create Host Definition File Create a new configuration file for each of the remote hosts that we want to monitor.\nIn this case I\u0026rsquo;m going to put all the host and service definitions into a single file.\n[root@nagios ~]# vim /usr/local/nagios/etc/hosts/cl1.cfg # Host configuration file define host { use linux-server host_name cl1 alias Ubuntu Host address 172.25.10.100 register 1 } define service { host_name cl1 service_description PING check_command check_ping!100.0,20%!500.0,60% max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 } define service { host_name cl1 service_description PING check_command check_ping!100.0,20%!500.0,60% max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 check_freshness 1 contact_groups admins notification_interval 2 notification_period 24x7 notifications_enabled 1 register 1 } define service { host_name cl1 service_description Check Users check_command check_local_users!20!50 max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 check_freshness 1 contact_groups admins notification_interval 2 notification_period 24x7 notifications_enabled 1 register 1 } define service { host_name cl1 service_description Local Disk check_command check_local_disk!20%!10%!/ max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 check_freshness 1 contact_groups admins notification_interval 2 notification_period 24x7 notifications_enabled 1 register 1 } define service { host_name cl1 service_description Check SSH check_command check_ssh max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 check_freshness 1 contact_groups admins notification_interval 2 notification_period 24x7 notifications_enabled 1 register 1 } define service { host_name cl1 service_description Total Process check_command check_local_procs!250!400!RSZDT max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 check_freshness 1 contact_groups admins notification_interval 2 notification_period 24x7 notifications_enabled 1 register 1 } define service { host_name cl1 service_description DNS check_command check_dns max_check_attempts 2 check_interval 2 retry_interval 2 check_period 24x7 check_freshness 1 contact_groups admins notification_interval 2 notification_period 24x7 notifications_enabled 1 register 1 } C. Modify Nagios Main Config Then, Add \u0026ldquo;cl1.cfg\u0026rdquo; config file path into Nagios main configuration file.\n[root@nagios ~]# vim /usr/local/nagios/etc/nagios.cfg # Definitions for monitoring the local (Linux) host cfg_file=/usr/local/nagios/etc/hosts/cl1.cfg D. Define New Commands Edit the file commands.cfg and add the lines below.\nAdd this command definition config into the bottom of the \u0026ldquo;commands.cfg\u0026rdquo; file.\nIn this case, I\u0026rsquo;m going to add \u0026ldquo;check_dns\u0026rdquo; command into the \u0026ldquo;commands.cfg\u0026rdquo; as a new service check command.\n[root@nagios ~]# vim /usr/local/nagios/etc/objects/commands.cfg Here, You can define your own commands that you want to execute on the remote host.\nYou can add and modify new command definitions by editing this section.\n#\u0026#39;check_dns\u0026#39; command definition define command { command_name check_dns command_line $USER1$/check_dns -H $HOSTADDRESS$ -s $ARG1$ } OR\n#\u0026#39;check_dns\u0026#39; command definition define command { command_name check_dns command_line $USER1$/check_dns -H digitalave.github.io -s 8.8.8.8 -t 15 | sed -e \u0026#34;s/returns.*//g\u0026#34; } E. Verify NRPE Daemon Finally verify the configuration that we made.\n/usr/local/nagios/libexec/check_nrpe -H \u0026lt;remote_linux_ip_address\u0026gt; F. Verify Nagios Configs and Check For Error [root@nagios ~]# /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg Now restart Nagios server to reflect the configuration changes.\n[root@nagios ~]# systemctl restart nagios httpd G. Log Into Nagios Web Console ","date":"October 10, 2019","image":"https://digitalavenue.dev/img/post-imgs/add-linux-to-nagios/linux2nagios_N_hu0a2cd497c9a6e210867c3f3afa122254_216046_650x0_resize_q90_box.jpg","permalink":"/blog/2019-10-10-add_linux_host_to_nagios_using_nrpe/","title":"How to Add Linux Host to Nagios Monitoring Server Using NRPE Plugin"},{"categories":["Monitoring","Linux"],"contents":" Introduction: Nagios is a industry slandered IT infrastructure monitoring solutions. Which is one of the most popular, open source , powerful server and network monitoring system. It enables organizations to identify and resolve IT infrastructure problems before they affect critical business processes. Nagios has the capability of monitoring application, services with an entire IT infrastructure.\nWhich can monitor host resources such as CPU Load,CPU Usage, DISK Throughput, Disk Iops, Network Speed, Network Throughput and finally trigger an alert via email.\nLAB SETUP Host IP Nagios 4 Server 172.25.10.50 Client 172.25.10.100 OS / Package Version CentOS - Nagios Server 7 CentOS - Client Host 7 Nagios Core 4.4.5 Nagios Plugin 2.2.1 NRPE Package 3 SERVER SIDE CONFIGURATIONS STEP 01: Complete Prerequisites A. SELinux Configuration: SELinux need to disabled \u0026amp; set it into permissive mode.\nsed -i \u0026#39;s/SELINUX=.*/SELINUX=disabled/g\u0026#39; /etc/selinux/config setenforce 0 Initially, We need to have a freshly installed CentOS7 / RHEL7 operating system.\nB. Update an Operating System. yum -y update C. Install Required Packages. Now, We need to install Apache, PHP, SNMP and some other packages.\nyum install -y gcc glibc glibc-common wget unzip httpd php gd gd-devel perl postfix net-snmp STEP 02: Install Nagios Core A. Download Nagios Core Source Package. Create a download directory. And download Nagios Core version 4.4 into that directory.\nmkdir /nagios cd /nagios wget -O nagioscore.tar.gz https://github.com/NagiosEnterprises/nagioscore/archive/nagios-4.4.5.tar.gz Extract nagios source package.\ntar xvzf nagioscore.tar.gz B. Compile Nagios Package. Configure Nagios settings.\ncd /nagios/nagioscore-nagios-4.4.5/ ./configure C. Compile the Main Program \u0026amp; CGIs. make all D. Create Nagios User \u0026amp; Group. This adds the users and groups if they do not exist This will creates a user and a group named \u0026ldquo;nagios\u0026rdquo;. The apache user is also added to the nagios group.\nmake install-groups-users usermod -a -G nagios apache E. Install the Main Program, CGIs and HTML files. make install H. Setup Services \u0026amp; Daemons. make install-init I. Install Service / Daemon This installs the service or daemon files and also configures them to start on boot.\nmake install-daemoninit J. Set Permissions on the External Commands Directory. make install-commandmode K. Install Sample Config Files. This installs SAMPLE config files in /usr/local/nagios/etc You\u0026rsquo;ll have to modify these sample files before you can use Nagios.\nmake install-config L. Install Apache Configs For Nagios Web Interface. make install-webconf M. Install Nagios Interface Theme. This installs the Exfoliation theme for the Nagios web interface.\nmake install-exfoliation M. Firewall Configuration. Allow inbound traffic via tcp port 80.\nfirewall-cmd --zone=public --add-port=80/tcp --permanent firewall-cmd --reload O. Create \u0026ldquo;nagiosadmin\u0026rdquo; User Account. htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin P. Start \u0026amp; Enable Apache Web Server. Enable service to start automatically while system boot.\nsystemctl start httpd.service systemctl enable httpd.service Q. Enable \u0026amp; Start Service systemctl enable nagios.service systemctl restart nagios.service Q. Accessing the Nagios Web Interface Now, Nagios Core server installation and configuration has been completed.\nNow you can log into the Nagios Web Interface through the web browser.\nNote: You will notice there some error unders the hosts \u0026amp; service,Eventhough We have installed Nagios Core.\n(No output on stdout) stderr: execvp(/usr/local/nagios/libexec/check_load, ...) failed. errno is 2: No such file or directory These errors wiil be resoloved once you installed nagios pluings.\nSTEP 04: Installing Nagios Plugins From YUM Repository A. Install Prerequisites\nyum install -y gcc glibc glibc-common make gettext automake autoconf wget openssl-devel net-snmp net-snmp-utils epel-release yum install -y perl-Net-SNMP B. Install Nagios Plugins\nyum -y install nagios-plugins-all C. Copy plugins if not available in the directory.\ncp /usr/lib64/nagios/plugins/check_* /usr/local/nagios/libexec/ STEP 05: Restart Nagios, Apache and NRPE Servers systemctl restart nagios httpd nrpe STEP 06: How To Install NRPE v3 From YUM Repository A. Install Prerequisites\nyum install -y gcc glibc glibc-common openssl openssl-devel perl wget B. Install Latest EPEL Repository And System Updates\nInstall Latest EPEL YUM Repository\nyum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum -y update C. Install (NRPE v3) Nagios Remote Plugin Executor\nyum -y install nrpe D. Allow NRPE service Port Through the Firewall\n[root@nagios /]# firewall-cmd --permanent --add-port=5666/tcp [root@nagios /]# firewall-cmd --reload F. Enable NRPE Service To Start at Boot\nsystemctl enable nrpe.service systemctl restart nrpe.service ","date":"October 8, 2019","image":"https://digitalavenue.dev/img/post-imgs/nagios-install-centos7/nagios_N_hua32cc5012020516a8ad82bb427c7f190_210098_650x0_resize_q90_box.jpg","permalink":"/blog/2019-10-08-install_nagios_core_on_centos7/","title":"How to Install Nagios Core 4.4 Server on CentOS7 RHEL7"},{"categories":["devops"],"contents":"In this tutorial I\u0026rsquo;m going to teach you the basics you should have known before dive into depth.\nNow, I\u0026rsquo;m going to cover up fundamentals you must know.\nUnderstand Docker Images: A. List Docker Images\n$docker images\nRepository - Image where it came from TAG - Version Number Image ID - Internal representation of Docker Image\nB. Running an Image\n$docker run -ti ubuntu:latest bash\nDocker image tuns into a running container. Docker run takes images into container.\n-ti Terminal Interactive - stands for interactive shell (Which help for TAB Completion \u0026amp; work correctly)\nbash - Run command on a bash shell\nNow I\u0026rsquo;m in a Ubuntu environment\nNow check\n#cat /etc/lsb-release\nTo exit\n#exit\nD. List the Running Images\n$docker ps\nID : Running Container\u0026rsquo;s ID IMAGE : Image which made from COMMAND : Command it running on STATUS\nPORT\nE. List the Recently Stopped Last Containers\n$docker ps -l\nF. List All Stopped Containers\n$docker ps -al\nG. Docker Commit Docker Commit takes containers back to new images.\n$docker run -ti ubuntu:latest bash\n#touch test.file\n#exit\n$docker ps -l\nID : XXXXXXXX IMAGE : ubuntu COMMAND : bash CREATED : STATUS : PORTS : NAMES : ubuntu_happy\n$docker commit OR\n$docker commit Which returns Image ID sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nNow, I have a newly created Docker Images.\nH. Give Images a Name Using TAG\n$docker tag my-new-image\nNow List Docker Images\n$docker images\nI. Now, Run Docker Image using The TAG\n$docker run -ti my-new-image bash\n#ls -la /test.file\nNow, It has my test.file in it.\n###################################\nHow To Run Processes In Docker Containers\n$docker run \u0026ndash;rm -ti ubuntu sleep 5\n$docker run -ti ubuntu bash -c \u0026ldquo;sleep 3; echo all done\u0026rdquo;\n\u0026ndash;rm Delete this container when it exits\n-ti Interactive Terminal\nKeep Things Running on Container\n$docker run -d -ti ubuntu\n-d detach things running and leaving running in the background\nNow, Jump into the task again that I had started over there.\n$docker attach \u0026lt;container_name\u0026gt;\nBy Pressing Ctrl+P, Ctrl+Q these key combinations you can exit from terminal by detaching from it. But, leaves it running. So you can attach back again.\nagain you can find running docker container using \u0026ldquo;docker ps\u0026rdquo; and you can run it again using \u0026ldquo;docker attach \u0026rdquo;\nDocker Logs: By looking at the container output. Even though, you start the container, but it didn\u0026rsquo;t work. Now \u0026ldquo;docker logs\u0026rdquo; command come to an action.\n$docker run \u0026ndash;name example -d ubuntu bash -c \u0026ldquo;lose /etc/passwd\u0026rdquo; docker logs \u0026lt;container_name\u0026gt; use look at what the output was.\ndocker logs \u0026lt;container_name\u0026gt;\n$docker logs exmaple\nStopping \u0026amp; Removing Containers:\ndocker kill \u0026lt;container_name\u0026gt; Its stop the container.\ndocker rm \u0026lt;container_name\u0026gt; Its remove the container.\n$docker kill \u0026lt;container_name\u0026gt; docker rm \u0026lt;container_name\u0026gt; $docker ps -l\nDocker Resource Limit Enforce Management: Docker has a feature to limit \u0026amp; enforce limit the resources. You can limit it to a fixed amount of memory as you need.\nDocker Commands Docker Management Commands Docker \u0026ldquo;help\u0026rdquo; Command: Using \u0026ldquo;help\u0026rdquo; command you can see what are the other available commands.\ndocker -h | more To all the management command which associated with \u0026ldquo;docker image\u0026rdquo; command. [root@svr1 ~]# docker image --help Usage:\tdocker image COMMAND Manage images Commands: build Build an image from a Dockerfile history Show the history of an image import Import the contents from a tarball to create a filesystem image inspect Display detailed information on one or more images load Load an image from a tar archive or STDIN ls List images prune Remove unused images pull Pull an image or a repository from a registry push Push an image or a repository to a registry rm Remove one or more images save Save one or more images to a tar archive (streamed to STDOUT by default) tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE Run \u0026#39;docker image COMMAND --help\u0026#39; for more information on a command. List Docker Images: docker image ls Pull Docker image: /assets/img/post-imgs/ Pull an image or a repository from a registry (Docker Hub).\ndocker image pull nginx View Image Information: docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest f949e7d76d63 5 days ago 126MB httpd latest 19459a872194 2 weeks ago 154MB hello-world latest fce289e99eb9 9 months ago 1.84kB docker image inspect fce289e99eb9 To list the all all command related to \u0026ldquo;docker container\u0026rdquo; command. [root@svr1 ~]# docker container --help Usage:\tdocker container COMMAND Manage containers Commands: attach Attach local standard input, output, and error streams to a running container commit Create a new image from a container\u0026#39;s changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container\u0026#39;s filesystem exec Run a command in a running container export Export a container\u0026#39;s filesystem as a tar archive inspect Display detailed information on one or more containers kill Kill one or more running containers logs Fetch the logs of a container ls List containers pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container prune Remove all stopped containers rename Rename a container restart Restart one or more containers rm Remove one or more containers run Run a command in a new container start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers wait Block until one or more containers stop, then print their exit codes Run \u0026#39;docker container COMMAND --help\u0026#39; for more information on a command. List Running Containers: docker container ls Run Command in a new container: docker container run nginx:latest List All Containers: docker container ls -a https://digitalavenue.dev//img/post-imgs/Docker-Installation/6.jpg does not exist\rRun Docker Container: docker container run -P -d nginx -P Port\n-d Run program in the background\nhttps://digitalavenue.dev//img/post-imgs/Docker-Installation/7.jpg does not exist\r[admin@svr1 ~]$ curl http://172.17.0.3 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List all the running Containers docker container ps https://digitalavenue.dev//img/post-imgs/Docker-Installation/8.jpg does not exist\rDisplay detailed information on the specific container: docker container inspect 4d625f4a7921 docker container inspect \u0026lt;CONTAINER_ID\u0026gt; https://digitalavenue.dev//img/post-imgs/Docker-Installation/9.jpg does not exist\rDisplay the running process of specific container: docker container top 4d625f4a7921 docker container top \u0026lt;CONTAINER_ID\u0026gt; https://digitalavenue.dev//img/post-imgs/Docker-Installation/10.jpg does not exist\rAttach Docker Containers: docker container run -P -d nginx:latest docker container ls -a https://digitalavenue.dev//img/post-imgs/Docker-Installation/12.jpg does not exist\rhttps://digitalavenue.dev//img/post-imgs/Docker-Installation/11.jpg does not exist\r[root@svr1 ~]# docker container attach 7e9428fa3cbc 172.17.0.1 - - [01/Oct/2019:09:07:57 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;-\u0026#34; View Stats for Specific Docker Container docker container stats 7e9428fa3cbc Run Command in a Specific Container: Which can use for execute commands on the container. [root@svr1 ~]# docker container exec -it 7e9428fa3cbc /bin/bash root@7e9428fa3cbc:/# ls bin boot dev\tetc home lib\tlib64 media mnt opt\tproc root run sbin srv sys tmp usr var Remove Docker Container: docker container rm -f 7e9428fa3cbc docker container rm -f \u0026lt;CONTAINER_ID\u0026gt; Remove all the Stopped Containers: docker container prune ###############################################################################\nCreating Docker Containers -d Detach from the container run the container in the background.\n[root@svr1 ~]docker container run -d nginx 8317663311f0d2d987ccae41d8c85802ef5ed93512f5f79479901ed27857bad9 [root@svr1 ~]# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8317663311f0 nginx \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 8 seconds ago Up 7 seconds 80/tcp hardcore_leakey Here it can see running in the background.\n-i Keep \u0026ldquo;STDIN\u0026rdquo; open\n[root@svr1 ~]# docker container run -it ubuntu:latest root@fdc372caedf7:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@fdc372caedf7:/# cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=18.04 DISTRIB_CODENAME=bionic DISTRIB_DESCRIPTION=\u0026#34;Ubuntu 18.04.3 LTS\u0026#34; -rm Automatically remove the container when its program exits.\ndocker container run --rm ubuntu:latest \u0026ndash;name string - Assign a name to a docker container\ndocker container run --name my_webserver nginx Exposing \u0026amp; Publishing Container Ports -d run as a background process\n[root@svr1 ~]# docker container run -d nginx aac2d187d875c571ab7461614cad5b64a6453c6083fd4761c3e9e564e5517fc6 [root@svr1 ~]# curl http://172.25.10.50 curl: (7) Failed connect to 172.25.10.50:80; Connection refused In here you can see, we can\u0026rsquo;t access nginx welcome page even though nginx running. The reason is, we didn\u0026rsquo;t setup port mapping for nginx container. just only we did is expose port 3000.\n\u0026ndash;expose open up port 3000\n[root@svr1 ~]# docker container run -d --expose 3000 nginx 1999e3a8a32c3f73e0a03663372141093ca096c712d1de76da609702474a242d [root@svr1 ~]# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1999e3a8a32c nginx \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 10 seconds ago Up 9 seconds 80/tcp, 3000/tcp nice_lewin 8317663311f0 nginx \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 46 minutes ago Up 46 minutes 80/tcp hardcore_leakey Now, I\u0026rsquo;m going to expose \u0026amp; map the port same time.\n[root@svr1 ~]# docker container run -d --expose 3000 -p 8080:80 nginx 14a093de7fe4474bad8bdbf9a6480c4932b1e10aa8cded51528ee4d0a191e5c4 [root@svr1 ~]# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 14a093de7fe4 nginx \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 5 seconds ago Up 4 seconds 3000/tcp, 0.0.0.0:8080-\u0026gt;80/tcp infallible_ellis [root@svr1 ~]# curl localhost:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; As same as above, Now I\u0026rsquo; forwarding nginx container default port 80 into host port 8018.\ndocker container run -d -p \u0026lt;HOST_PORT\u0026gt;:\u0026lt;CONTAINER_PORT\u0026gt; \u0026lt;IMAGE_ID\u0026gt; root@svr1 ~]# docker container run -d -p 8081:80 nginx:latest df99bbd4a46acb5acb08a70f1102296c836f0db28c745f6ba23bc0b4b5a43d56 [root@svr1 ~]# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES df99bbd4a46a nginx:latest \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 6 seconds ago Up 5 seconds 0.0.0.0:8081-\u0026gt;80/tcp goofy_swanson [root@svr1 ~]# curl localhost:8081 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List All Port Mappings:\nTo see all the port mapping for a specific docker container.\n[root@svr1 ~]# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES df99bbd4a46a nginx:latest \u0026#34;nginx -g \u0026#39;daemon of…\u0026#34; 5 minutes ago Up 5 minutes 0.0.0.0:8081-\u0026gt;80/tcp goofy_swanson [root@svr1 ~]# docker port df99bbd4a46a 80/tcp -\u0026gt; 0.0.0.0:8081 ############# EXECUTE COMMANDS ON A CONTAINER ##################\nThere Three Ways\u0026hellip;\nDockerfile At the end of the Dockerfile for the nginx has mentioned \u0026ldquo;CMD\u0026rdquo;\nDuring a Docker run [root@svr1 ~]# docker container run -it nginx /bin/bash root@509e7ea33288:/# pwd / root@509e7ea33288:/# ls /etc/nginx/conf.d/ default.conf root@509e7ea33288:/# ls /etc/nginx/ conf.d/ koi-utf mime.types nginx.conf uwsgi_params fastcgi_params koi-win modules/ scgi_params win-utf root@509e7ea33288:/# ls /etc/nginx/ conf.d\tfastcgi_params\tkoi-utf koi-win mime.types modules nginx.conf scgi_params\tuwsgi_params win-utf Using the \u0026ldquo;exec\u0026rdquo; command ","date":"October 1, 2019","image":"https://digitalavenue.dev/img/post-imgs/Docker-Installation/docker_N_hu2401304e80cc219473e4537b84919667_122429_650x0_resize_q90_box.jpg","permalink":"/blog/2019-10-01-docker_lesson_2_docker_starting_point/","title":"Lession 02 - Docker Basics - Where Am I Start Learn Docker"},{"categories":null,"contents":"Install Docker on CentOS7 and Redhat RHEL7 Introduction This is the very first session of Docker lesson series. In this tutorial I\u0026rsquo;m going to demonstrate how to install Docker on CentOS7 with the little bit of explanation about Docker.\nCatagory Requirements, Software Versions Used OS CentOS 7 Software Docker Version 18 Before the installation, We should know about little bit of Docker.\nWhat Docker ? Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.\nDocker is Opensource.\nDocker is a set of platform-as-a-service.\nThat use OS-level virtualization to deliver software in packages.\nThey are called containers.\nThese containers are isolated from one another and bundle their own software, libraries and configuration files\nThey can communicate with each other.\nDocker turns your Computer into isolated containers which runs your codes or services independently. The single operating system carved up into isolated little spaces.\nOverview of Docker editions: Docker is available in three Editions:\nDocker Engine - Community Edition (CE): Good starting point for individual developers, small teams and those who are learning docker.\nDocker Engine - Enterprise Edition (EE): Designed for enterprise level development of docker containers with better enhanced security.\nDocker Enterprise: Designed for enterprise development and IT teams who build, ship, and run business critical apps in production.\nWhat is a Container ? A self-contained sealed unit of software. It has everything in the container required to run the code.\nA Single Container Includes:\nCode Configs Processes Networking Services All Dependencies for your code need to run Operating System Supported Platforms\nInstall Docker Engine - Community For CentOS 7 / RHEL 7 OS requirements:\ncentos-extras repository must be enable. This is enable by default on CentOS.\nThe overlay2 storage driver is recommended.\nInstall From Docker yum repository STEP 01: SETUP THE DOCKER YUM REPOSITORY\nyum update -y Install required packages For YUM repository management.\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2 Download and setup docker repository from docker.com\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Install docker package\nThis will install Docker latest version.\nsudo yum install docker-ce docker-ce-cli containerd.io STEP 02: Start \u0026amp; Enable Docker Daemon Once docker-ce installed on the system, we must start \u0026amp; enable the docker daemon. So, that it will be launched automatically on system boots.\nsudo systemctl start docker sudo systemctl enable --now docker STEP 03: Verify Installation Now check whether docker has been installed correctly.\nsystemctl is-active docker sudo docker run hello-world which automatically start to download basic \u0026ldquo;hello-world\u0026rdquo; images from Docker Hub.\nSTEP 04: Testing Docker\nNow, I\u0026rsquo;m going to build an image and run a container. In this case I\u0026rsquo;m going to use official \u0026ldquo;httpd\u0026rdquo;.\nsudo docker run --rm --name=linuxconfig-test -p 80:80 httpd Since the httpd image does not exists locally it will be automatically fetched and built. Finally, a container based on it will be launched in the foreground (it will be automatically removed when stopped). If our firewall is configured to allow access to port 80, we should be able to see the It works! message when we reach our machine ip via browser.\nManage Docker as a non-root user: If you don’t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group.\nSTEP 01: Create The Docker Group:\nsudo groupadd docker STEP 02: Add User To Docker Group:\nsudo usermod -aG docker $USER In here $USER automatically resolve current user logged in.\nNow, You need to logout from $USER and log into $USER again.\n$USER is the username which user wants to run without \u0026ldquo;sudo\u0026rdquo; prefix.\nVerify that you can run docker commands without sudo\ndocker run hello-world STEP 03: Configure Docker to Start on Boot:\nsudo systemctl enable docker docker version Install Docker on Ubuntu/Debian Install Docker Engine - Community\nA. Setup apt Repository\nsudo apt-get update B. Install required Packages to Allow apt Repositories over HTTPS.\nsudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common C. Add Docker\u0026rsquo;s GPG Key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - D. Setup Docker Repository\nsudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; E. Update apt Package Index Again\nsudo apt-get update F. Install Docker Engine\nsudo apt-get install docker-ce docker-ce-cli containerd.io G. Verify The Docker Engine Installed Correctly\nsudo docker run hello-world Post Installation - Manage Docker as a Non Root User A. Create a group named \u0026ldquo;docker\u0026rdquo;\nsudo groupadd docker B. Add User to the group named \u0026ldquo;docker\u0026rdquo;\nsudo usermod -aG docker $USER C. Restart the Host Machine\nreboot D. Verify the Docker Installation\ndocker run hello-world E. Docker Start on Boot\nsudo systemctl enable docker Grate, Now Docker installation on CentOS / Ubuntu has been completed. Now, Lets start to get hands dirty\u0026hellip;\nHope this helps for those who are looking for a starting point to learn Docker. I wish to teach more about Docker in the next lesson. You can keep in touch with the future tutorials by Subscribing me on Youtube.\n","date":"September 27, 2019","image":"https://digitalavenue.dev/img/post-imgs/Docker-Installation/docker_N_hu2401304e80cc219473e4537b84919667_122429_650x0_resize_q90_box.jpg","permalink":"/blog/2019-09-27-docker_lesson_1_get_started_with_docker/","title":"Lession 01 - Install Docker on CentOS7 /Redhat RHEL7"},{"categories":["pfSense","Linux","IT Security"],"contents":" Introduction: OpenVPN is really useful to access your office when you are out of the office.\nIn this tutorial, I\u0026rsquo;m going to show you how to set up your own VPN connection using with OpenVPN service on the pfSense firewall. But, the really special thing is what would happen if your Internet link goes down?\nNow, Here\u0026rsquo;s the solution\u0026hellip;\nBut, How ???\nTo achieve this task, you may need to have two WAN interfaces attached to your pfSense firewall.\nYou will have uninterrupted VPN access, After the completion of this Multi-WAN OpenVPN setup.\nSTEP 01: SETUP OpenVPN Now head over to the \u0026ldquo;VPN\u0026rdquo; tab and select \u0026ldquo;OpenVPN\u0026rdquo; from the drop-down.\nClick \u0026amp; open \u0026ldquo;Wizard\u0026rdquo;\nSelect \u0026ldquo;Local User Access\u0026rdquo; as the Authentication Backend Type. Click next \u0026amp; move onto \u0026ldquo;Create CA Certificate\u0026rdquo; section.\nCreate New CA Certificate with Descriptive Certificate Name, Country Code, State of Province, City and Organization.\nThen move onto the next step.\nNow, We have to create a new server certificate, the same as we did earlier. Move to the next step after that.\nNow, We need to modify \u0026ldquo;General Settings\u0026rdquo; for the OpenVPN As the first option, we need to select an interface wish to configure. For now, I\u0026rsquo;m going to select the WAN1 interface.\nYou may not need to modify the rest of the other fields. But, At this time, I\u0026rsquo;m going to change \u0026ldquo;Encryption algorithem.\u0026rdquo;\nNow head over to the \u0026ldquo;Tunnel Settings\u0026rdquo; section. Add \u0026ldquo;Tunnel Network\u0026rdquo; address. This is a virtual network address that provides an IP address to the client. It helps private communication between OpenVPN server \u0026amp; client. Provide a network address which is not existing on your network.\nThen, Add the network address of your \u0026ldquo;Local Area Network\u0026rdquo;. This network should be the network that you wants to access via OpenVPN.\nSimply enable \u0026ldquo;Inter Clint Communication\u0026rdquo; to enable the communication between other OpenVPN clients.\nYou may need to provide \u0026ldquo;domain name\u0026rdquo; and \u0026ldquo;DNS server\u0026rsquo;s for the \u0026ldquo;Client Settings\u0026rdquo; section\nAs the final step, You need to \u0026ldquo;Configure Firewall\u0026rdquo; to allow connection through the tunnel network.\nAnd click finish button.\nOK, the OpenVPN Server configuration has been completed so far.\nBut, We need to create two firewall NAT Port forwarding rules since we are going to use 2 Wan interfaces.\nSTEP 02: Create Port Forwarding Rules. Now head on to, \u0026ldquo;Firewall\u0026rdquo; Tab and select \u0026ldquo;NAT\u0026rdquo; from the drop-down menu.\nNow create two port forwarding rules for the WAN1 interface.\nForward OpenVPN connections to the localhost, which are coming from WAN1 \u0026amp; WAN2 interfaces.\nFor example: If there are two WANs and the OpenVPN server is running on port 1194, set the Interface to Localhost, then add two port forwards:\n*WAN1 - UDP, Source , Destination WAN1 Address port 1194, redirect target 127.0.0.1 port 1194 *WAN2 - UDP, Source , Destination WAN2 Address port 1194, redirect target 127.0.0.1 port 1194\nNow create the same rule for the WAN2 interface.\nSTEP 03: Install \u0026ldquo;OpenVPN Client Export\u0026rdquo; package Move to the \u0026ldquo;System\u0026rdquo; menu and select \u0026ldquo;Package Manager.\u0026rdquo;\nthen search for the \u0026ldquo;openvpn-client-export\u0026rdquo; package\nAnd install.\nSTEP 03: Create OpenVPN Users\nThen move to \u0026ldquo;System\u0026rdquo; and then \u0026ldquo;User Manger\u0026rdquo; and Create an OpenVPN client user.\nCreate a user certificate for each user by enabling the \u0026ldquo;Certificate\u0026rdquo; option\nSTEP 04: OpenVPN Server Modification Finally, head on to the \u0026ldquo;Client Export\u0026rdquo; section in the OpenVPN.\nAnd modify \u0026ldquo;OpenVPN Server\u0026rdquo; settings.\nNow, Change \u0026ldquo;Host name resolution\u0026rdquo; into \u0026ldquo;localhost.\u0026rdquo;\nAnd move down.\nAnd add these entries to the \u0026ldquo;Advanced Configurations\u0026rdquo;.\nThese are the Public IP addresses which, OpenVPN connection going to use.\nremote 192.168.0.2 1194 udp\nremote 192.168.10.250 1194 udp\nOK, Now OpenVPN configuration with Dual-WAN connections is completed. Now, It works as a fail-over connection to either connection. If one link goes down, another interface comes into action. Especially keep in mind, If one link goes down, it will take nearly 60 seconds to establish a connection with the other interface.\nBottom line: Now, I think may you learn how to set up a fail-over Multi-WAN OpenVPN server on pfSense. I hope this helps you to achieve this task. If you feel it worth it, don\u0026rsquo;t forget to subscribe to me for more tech videos like this.\n** Thank you \u0026amp; see you in the next tutorial.**\n","date":"September 24, 2019","image":"https://digitalavenue.dev/img/post-imgs/dual-wan-pf-vpn/Dual-WAN-OVPN-pfSense_N_hub7a779ede2f43fe6727b4ccafb1c1610_257411_650x0_resize_q90_box.jpg","permalink":"/blog/2019-09-24-how-to-setup-openvpn_on-pfsense_with_daul_wan_interfaces_as_failover/","title":"How To Setup an OpenVPN on pfSense with Dual-WAN Interfaces as Fail-over"},{"categories":["Linux","Monitoring"],"contents":" INTRODUCTION Hi Folks, In this tutorial I\u0026rsquo;m going to install \u0026amp; configure ELK Stack as a Log server inside my testing Lab.\n“ELK” is the acronym for the three open source projects call Elasticsearch, Logstash and Kibana. ELK stack made easier to analyze logs to system administrators. ELK stack collect logs from clients using Beats protocol\nELK STACK MAIN COMPONENTS Elasticsearch is an open source, distributed, RESTful, JSON based search and analytic engine. Easy to use and flexible. Elasticsearch is the heart of ELK stack. Elasticsearch is a No-SQL database.\nLogstash is a open source, server-side data processing pipeline that pull events data from multitude of sources simultaneously, transform it, and then sends it to Elasticsearch. Easily pull data from logs, metrics, web applications, data sources and various AWS services. Logstash dynamically transforms and prepares data regardless of format or complexity. Derive structure from unstructured data with grock.\nKibana provides GUI for users visualize data with charts and graphs real-time. It is a window into the Elastic Stack. Provides data exploration, visualization and dashboarding.\nBeats is the platform for single-purpose data shippers. They install as lightweight agents and send data from numerous machines to Logstash of\nSoftware Versions I have used in this tutorial.\nHost IP Server/Client elk.da.com 192.168.10.10 Server cl1.da.com 192.168.10.110 Client Package Version Elasticsearch Version: 7.3 Logstash Version: 7.3 Kibana Version: 7.3 Java JDK jdk-11.0.4 STEP 1: COMPLETE PREREQUSITES A: SET HOSTNAME \u0026amp; FQDN vim /etc/hostname\nelk vim /etc/hosts\n192.168.10.10 elk.da.com elk B: CLEAR AND REMOVE YUM CACHE (Optional) sudo rm /etc/yum.repos.d/$REPONAME.repo yum clean all Delete the yum cache for the repo\nsudo rm -rf /var/cache/yum/x86_64/6/$REPONAME Clearing the yum Caches\nsu -c \u0026#39;yum clean headers\u0026#39; su -c \u0026#39;yum clean packages\u0026#39; su -c \u0026#39;yum clean metadata\u0026#39; C: Install and Update Latest RPM Repositories rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -ivh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm rpm -ivh https://download1.rpmfusion.org/free/el/rpmfusion-free-release-7.noarch.rpm rpm -ivh http://repository.it4i.cz/mirrors/repoforge/redhat/el7/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el7.rf.x86_64.rpm D: SeLinux Configuration vim /etc/selinux/config\nSELINUX=permissive STEP 3: INSTALL JAVA JDK Java is required for the Elastic stack deployment. Elasticsearch requires Java 8, it is recommended to use the Oracle JDK 1.8. I will install Java 8 from the official Oracle rpm package. ELK requires the Oracle Java JDK package has to be installed. The same JVM version should be installed on all Elasticsearch nodes and clients.\nA: Download JAVA JDK 11.0.4 curl -o jdk-11.0.4_linux-x64_bin.rpm https://download.oracle.com/otn/java/jdk/11.0.4+10/cf1bbcbf431a474eb9fc550051f4ee78/jdk-11.0.4_linux-x64_bin.rpm?AuthParam=1566470470_04821224cc5f90794bc98fdb1d1b171a B: INSTALL JDK RPM 11.0.4 rpm -ivh jdk-11.0.4_linux-x64_bin.rpm C: SET DEFAULT JAVA VERSION alternatives --config java alternatives --set jar /usr/java/jdk-11.0.4/bin/jar alternatives --set javac /usr/java/jdk-11.0.4/bin/javac D: SET JAVA ENVIRONMENT VARIABLES SET JAVAC AND JAR PATHS\nexport JAVA_HOME=/usr/java/jdk-11.0.4/ export PATH=$PATH:/usr/java/jdk-11.0.4/bin/ vim ~/.bashrc\nexport JAVA_HOME=/usr/java/jdk-11.0.4/ export PATH=$PATH:/usr/java/jdk-11.0.4/bin/ vim ~/.bash_profile\nexport JAVA_HOME=/usr/java/jdk-11.0.4/ export PATH=$PATH:/usr/java/jdk-11.0.4/bin/ E: CHECK JAVA VERSION java -version\njava version \u0026#34;11.0.4\u0026#34; 2019-07-16 LTS Java(TM) SE Runtime Environment 18.9 (build 11.0.4+10-LTS) Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.4+10-LTS, mixed mode) STEP 3: INSTALL AND CONFIGURE ELASTICSEARCH In this step, I will install and configure Elasticsearch version 7.3\nA: IMPORT PUBLIC GPG KEY TO THE ELK-STACK SERVER rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch B: CREATE YUM REPO FILE FOR ELASTICSEARCH vim /etc/yum.repos.d/elasticsearch.repo\n[elasticsearch-7.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md C: INSTALL ELASTICSEARCH YUM PACKAGES sudo yum -y install elasticsearch CONFIGURE ELASTICSEARCH Do the following changes\nvim /etc/elasticsearch/elasticsearch.yml\ncluster.name: elk node.name: node-1 path.data: /var/lib/elasticsearch path.logs: /var/log/elasticsearch network.host: 127.0.0.1 http.host: 0.0.0.0 http.port: 9200 JVM Options Configuration\nSet initial/maximum size of total heap space. If your system has less memory. You should configure it to use small megabytes of ram.\nvim /etc/elasticsearch/jvm.options\n-Xms4g -Xmx4g FIREWALL CONFIGURATION\nAllow traffic through the TCP port 9200 in the firewall.\nfirewall-cmd --permanent --add-port=9200/tcp firewall-cmd --permanent --add-port=9300/tcp firewall-cmd --reload START \u0026amp; ENABLE ELASTICSEARCH AT SYSTEM BOOT sudo yum install elasticsearch sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable elasticsearch.service sudo /bin/systemctl restart elasticsearch.service sudo /bin/systemctl status -l elasticsearch.service sudo journalctl -f sudo journalctl --unit elasticsearch TEST ELASTICSEARCH Check Elasticsearch port “9200” state as “LISTEN”\nnetstat -plntu OPEN IN BROWSER http://192.168.10.10:9200/?pretty OPEN IN TERMINAL curl -XGET \u0026#39;192.168.10.10:9200/?pretty\u0026#39; STEP 4: INSTALL AND CONFIGURE LOGSTASH In this step I will install Logstash version 7.3 and configure it as a central log server, receives logs from clients with Filebeat/Auditbeat, then filter and transform the syslog/Audit data and move it into the stash (Elasticsearch)\nA: IMPORT PUBLIC GPG KEY TO THE ELK-STACK SERVER rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch B: CREATE YUM REPO FILE FOR ELASTICSEARCH vim /etc/yum.repos.d/logstash.repo\n[logstash-7.x] name=Elastic repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md C: INSTALL LOGSTASH YUM PACKAGES sudo yum -y install logstash NOTE: Need to genarate SSL Certificate if you using SSL.This step is optional\nGENERATE A NEW SSL CERTIFICATE Create new ssl certificate for securing communication between Logstash \u0026amp; Filebeat (clients). SSL Certificate file use clients to identify the elastic server.\nDo the following changes under the “[ V3_ca ]” section for the server identification.\nvim /etc/pki/tls/openssl.cnf\n[ v3_ca ] #Server IP Address subjectAltName = IP: 192.168.10.10 Generate the certificate file with the openssl command.\nopenssl req -config /etc/pki/tls/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout /etc/pki/tls/private/logstash-forwarder.key -out /etc/pki/tls/certs/logstash-forwarder.crt Once ssl certificate is ready, this certificate should be copied to all the clients using scp command.\nD: CONFIGURE LOGSTASH vim /etc/logstash/logstash.yml\npath.data: /var/lib/logstash http.host: \u0026#34;192.168.10.10\u0026#34; path.logs: /var/log/logstash sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable logstash.service sudo /bin/systemctl restart logstash.service sudo /bin/systemctl status -l logstash.service sudo journalctl -f sudo journalctl --unit elasticsearch https://digitalavenue.dev//img/post-imgs/elk-stack68\\logstash1.PNG does not exist\rE: JVM configuration vim /etc/logstash/jvm.options\n-Xms4g -Xmx4g F: Create Following Files Under /etc/logstash/conf.d/ Directory. vim /etc/logstash/conf.d/auditbeat.conf\n### INPUT SECTION ### ### This section make Logstash to listen on port 5044 for incoming logs \u0026amp; provides SSL certificate for secure connection. input { beats { port =\u0026gt; 5044 # ssl =\u0026gt; true # ssl_certificate =\u0026gt; \u0026#34;/etc/pki/tls/certs/logstash-forwarder.crt\u0026#34; # ssl_key =\u0026gt; \u0026#34;/etc/pki/tls/private/logstash-forwarder.key\u0026#34; } } ### OUTPUT SECTION ### ### This section defines the storage for the logs to be stored. output { elasticsearch { hosts =\u0026gt; [\u0026#34;http://192.168.10.10:9200\u0026#34;] manage_template =\u0026gt; false index =\u0026gt; \u0026#34;%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.DD}\u0026#34; document_type =\u0026gt; \u0026#34;%{[@metadata][type]}\u0026#34; } } G: FIREWALL CONFIGURATION Allow traffic through the TCP port 5044 in the firewall.\nfirewall-cmd --permanent --add-port=5044/tcp firewall-cmd --reload ENABLE \u0026amp; START LOGSTASH SERVICE sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable logstash.service sudo /bin/systemctl restart logstash.service sudo /bin/systemctl status -l logstash.service sudo journalctl -f sudo journalctl --unit elasticsearch STEP 5: INSTALL AND CONFIGURE KIBANA A: IMPORT PUBLIC GPG KEY TO THE ELK-STACK SERVER rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch B: CREATE YUM REPO FILE FOR KIBANA vim /etc/yum.repos.d/kibana.repo\n[kibana-7.x] name=Kibana repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md C: INSTALL KIBANA YUM PACKAGES sudo yum install kibana D: CONFIGURE KIBANA vim /etc/kibana/kibana.yml\nserver.port: 5601 server.host: \u0026#34;0.0.0.0\u0026#34; server.name: \u0026#34;elk\u0026#34; elasticsearch.hosts: [\u0026#34;http://127.0.0.1:9200\u0026#34;] https://digitalavenue.dev//img/post-imgs/elk-stack68\\kibana1.PNG does not exist\rE: FIREWALL CONFIGURATION Allow traffic through the TCP port 5044 in the firewall.\nfirewall-cmd --permanent --add-port=5601/tcp firewall-cmd --reload F: ENABLE \u0026amp; START LOGSTASH SERVICE sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service sudo /bin/systemctl restart kibana.service sudo /bin/systemctl status -l kibana.service netstat -tulpena | grep 5601 STEP 6: INSTALL AND CONFIGURE NGINX A: INSATLL EPEL REPOSITORY yum install epel-release B: INSTALL NGINX \u0026amp; HTTPD-TOOLS yum install nginx httpd-tools CREATE USERNAME “ADMIN” AND PASSWORD “PASSWORD” FOR KIBANA WEB INTERFACE\nhtpasswd -c /etc/nginx/htpasswd.kibana admin C: CONFIGURE NGINX Edit the Nginx configuration file and remove the ‘server { }’ block, so we can add a new virtual host configuration.\nvim /etc/nginx/nginx.conf\nCOMMENT {Server} Block:\nCreate new virtual host configuration file named “kibana.conf” under the conf.d directory.\nD: Create VHOST FOr KIBANA: vim /etc/nginx/conf.d/kibana.conf\nserver { listen 80; server_name elk-stack.co; auth_basic \u0026#34;Restricted Access\u0026#34;; auth_basic_user_file /etc/nginx/htpasswd.kibana; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } E: Check Nginx Configuration nginx -t F: FIREWALL CONFIGURATION Allow traffic through the TCP port 80 in the firewall.\nfirewall-cmd --permanent --add-port=80/tcp firewall-cmd --permanent --add-service=http firewall-cmd --reload G: ENABLE \u0026amp; START NGINX SERVICE systemctl enable nginx.service systemctl restart nginx.service SELINUX CONFGURATION setsebool -P httpd_can_network_connect 1 STEP 07: Connect Kibana Frontend With Elasticsearch You need assign Kibana to which Elasticsearch indeces you want yo explore. Configure the Elasticsearch Indices what you want to access with Kibana.\nOpen Web Browser and Point To\u0026hellip; (Only via Kibana) http://YOURIP.com:5601\nOR\n(If nginx/apache proxy redirect with VHOST) http://YOURIP.com:80\n","date":"August 26, 2019","image":"https://digitalavenue.dev/img/post-imgs/elk-stack/ELK_huebddfe80cefaa5dacd0646e45c091998_46954_650x0_resize_box_3.png","permalink":"/blog/2019-08-26-install__configure_elk_stack_7.3_on_centos7/","title":"Install and Configure ELK Stack 7.3 On CentOS7"},{"categories":["Linux"],"contents":"How To Kill Zombie Processes on Linux What is a Zombie Process ? Normally, When we launch a program, It starts their task \u0026amp; once task is over, Its end that process. Once process ended, It hos to be removed from the process table.\nSometimes, If programs are not programmed well enough to remove it\u0026rsquo;s child processes (Zombies). Because of this, some of these child zombie processes are reside in the process table even after it has completed its execution.\nSo, These process we called Zombie processes.\nSTEPS: Run a Program \u0026gt;\u0026gt; Creates a Parent Process with Child Processes \u0026gt;\u0026gt; They utilize System's resources \u0026gt;\u0026gt; Complete Child Process Task \u0026gt;\u0026gt; Receive EXIT system call \u0026gt;\u0026gt; Die Child Process \u0026gt;\u0026gt; EXIT system call read by Parent \u0026gt;\u0026gt; Finally Child Process removed from Process table.\nIf Parent failed to read the EXIT system call from the child process, The child processes are already exists on the process table even though that child process are already dead.\nAre they Harmful ? No, But, If there are lot of Zombie process which will stored/collected in the RAM.\nHow To Find Zombie Processes ? ps aux |grep Z OR\nps aux |grep \u0026#34;defunct\u0026#34; OR\nps -A -ostat,ppid | awk \u0026#39;/[zZ]/ \u0026amp;\u0026amp; !a[$2]++ {print $2}\u0026#39; How To Kill Zombie Process ? kill -s SIGCHLD \u0026lt;PID OF PARENT\u0026gt; OR\nkill -9 $(ps -A -ostat,ppid | awk \u0026#39;/[zZ]/ \u0026amp;\u0026amp; !a[$2]++ {print $2}\u0026#39;) Note: You Can Use This For Rsync Zombie Processes As Well.\nAlternatively, You can use PKILL command like this way. kill -9 $(ps -Z | grep \u0026#34;\u0026lt;PROCESS_EXACT_NAME\u0026gt;\u0026#34; | awk \u0026#39;{print $2}\u0026#39;) OR\npkill --signal --SIGCHLD rsync ","date":"July 7, 2019","image":"https://digitalavenue.dev/img/post-imgs/zombie_process/Zombie_N_N_hu216c1b953a9559deed2527aa1dc510f5_37114_650x0_resize_q90_box.jpg","permalink":"/blog/2019-07-07-how_to_kill_zombie_proceesses_on_linux/","title":"How To Kill Zombie Processes on Linux"},{"categories":["DevOps"],"contents":" INTRODUCTION “ELK” is the acronym for the three open source projects call Elasticsearch, Logstash and Kibana. ELK stack made easier to analyze logs to system administrators. ELK stack collect logs from clients using Beats protocol\nELK STACK MAIN COMPONENTS Elasticsearch is an open source, distributed, RESTful, JSON based search and analytic engine. Easy to use and flexible. Elasticsearch is the heart of ELK stack. Elasticsearch is a No-SQL database.\nLogstash is a open source, server-side data processing pipeline that pull events data from multitude of sources simultaneously, transform it, and then sends it to Elasticsearch. Easily pull data from logs, metrics, web applications, data sources and various AWS services. Logstash dynamically transforms and prepares data regardless of format or complexity. Derive structure from unstructured data with grock.\nKibana provides GUI for users visualize data with charts and graphs real-time. It is a window into the Elastic Stack. Provides data exploration, visualization and dashboarding.\nBeats is the platform for single-purpose data shippers. They install as lightweight agents and send data from numerous machines to Logstash of\nSoftware Versions I have used in this tutorial.\nELK Stack Server 192.168.10.10 (CentOS7) Beat Client 192.168.0.49 (CentOS7) Elasticsearch Version: 6.8 Logstash Version 6.8 Kibana Version 6.8\nSTEP 1: COMPLETE PREREQUSITES SET HOSTNAME vim /etc/hostname vim /etc/hosts CLEAR AND REMOVE YUM CACHE sudo rm /etc/yum.repos.d/$REPONAME.repo yum clean all Delete the yum cache for the repo\nsudo rm -rf /var/cache/yum/x86_64/6/$REPONAME Clearing the yum Caches\nsu -c \u0026#39;yum clean headers\u0026#39; su -c \u0026#39;yum clean packages\u0026#39; su -c \u0026#39;yum clean metadata\u0026#39; Install and Update Latest RPM Repositories rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -ivh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm rpm -ivh https://download1.rpmfusion.org/free/el/rpmfusion-free-release-7.noarch.rpm rpm -ivh http://repository.it4i.cz/mirrors/repoforge/redhat/el7/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el7.rf.x86_64.rpm STEP 2: INSTALL JAVA JDK Java is required for the Elastic stack deployment. Elasticsearch requires Java 8, it is recommended to use the Oracle JDK 1.8. I will install Java 8 from the official Oracle rpm package. ELK requires the Oracle Java JDK package has to be installed. The same JVM version should be installed on all Elasticsearch nodes and clients.\nINSTALL JDK RPM rpm -Uvh jdk-8u211-linux-x64.rpm SET DEFAULT JAVA VERSION alternatives --config java alternatives --set jar /usr/java/jdk1.8.0_211-amd64/bin/jar alternatives --set javac /usr/java/jdk1.8.0_211-amd64/bin/javac SET JAVA ENVIRONMENT VARIABLES SET JAVAC AND JAR PATHS\nexport JAVA_HOME=/usr/java/jdk1.8.0_211-amd64/ export JRE_HOME=/usr/java/jdk1.8.0_211-amd64/jre/ export PATH=$PATH:/usr/java/jdk1.8.0_211-amd64/bin/:/usr/java/jdk1.8.0_211-amd64/jre/bin/ vim ~/.bashrc\nexport JAVA_HOME=/usr/java/jdk1.8.0_211-amd64/ export JRE_HOME=/usr/java/jdk1.8.0_211-amd64/jre/ export PATH=$PATH:/usr/java/jdk1.8.0_211-amd64/bin/:/usr/java/jdk1.8.0_211-amd64/jre/bin/ CHECK JAVA VERSION java -version STEP 3: INSTALL AND CONFIGURE ELASTICSEARCH In this step, I will install and configure Elasticsearch version 6.8\nIMPORT PUBLIC GPG KEY TO THE ELK-STACK SERVER rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch CREATE YUM REPO FILE FOR ELASTICSEARCH vim /etc/yum.repos.d/elasticsearch.repo\n[logstash-6.x] name=Elastic repository for 6.x packages baseurl=https://artifacts.elastic.co/packages/6.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md INSTALL ELASTICSEARCH YUM PACKAGES sudo yum -y install elasticsearch CONFIGURE ELASTICSEARCH Do the following changes\nvim /etc/elasticsearch/elasticsearch.yml\ncluster.name: elk node.name: node-1 path.data: /var/lib/elasticsearch path.logs: /var/log/elasticsearch network.host: 192.168.10.10 http.port: 9200 vim /etc/elasticsearch/jvm.options\n-Xms4g -Xmx4g FIREWALL CONFIGURATION\nAllow traffic through the TCP port 9200 in the firewall.\nfirewall-cmd --permanent --add-port=9200/tcp firewall-cmd --permanent --add-port=9300/tcp firewall-cmd --reload START \u0026amp; ENABLE ELASTICSEARCH AT SYSTEM BOOT systemctl daemon-reload systemctl enable elasticsearch.service systemctl restart elasticsearch.service systemctl status -l elasticsearch.service TEST ELASTICSEARCH Check Elasticsearch port “9200” state as “LISTEN”\nnetstat -plntu OPEN IN BROWSER http://192.168.10.10:9200/?pretty OPEN IN TERMINAL curl -XGET \u0026#39;192.168.10.10:9200/?pretty\u0026#39; STEP 4: INSTALL AND CONFIGURE LOGSTASH In this step I will install Logstash version 6.8 and configure it as a central log server, receives logs from clients with Filebeat, then filter and transform the syslog data and move it into the stash (Elasticsearch)\nIMPORT PUBLIC GPG KEY TO THE ELK-STACK SERVER rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch CREATE YUM REPO FILE FOR ELASTICSEARCH vim /etc/yum.repos.d/logstash.repo\n[logstash-6.x] name=Elastic repository for 6.x packages baseurl=https://artifacts.elastic.co/packages/6.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md INSTALL LOGSTASH YUM PACKAGES sudo yum -y install logstash\nNOTE: Need to genarate SSL Certificate if you using SSL.\nGENERATE A NEW SSL CERTIFICATE Create new ssl certificate for securing communication between Logstash \u0026amp; Filebeat (clients). SSL Certificate file use clients to identify the elastic server.\nDo the following changes under the “[ V3_ca ]” section for the server identification.\nvim /etc/pki/tls/openssl.cnf\n[ v3_ca ] #Server IP Address subjectAltName = IP: 192.168.10.10 Generate the certificate file with the openssl command.\nopenssl req -config /etc/pki/tls/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout /etc/pki/tls/private/logstash-forwarder.key -out /etc/pki/tls/certs/logstash-forwarder.crt Once ssl certificate is ready, this certificate should be copied to all the clients using scp command.\nCONFIGURE LOGSTASH vim /etc/logstash/logstash.yml\npath.data: /var/lib/logstash http.host: \u0026#34;192.168.10.10\u0026#34; path.logs: /var/log/logstash JVM configuration vim /etc/logstash/jvm.options\n-Xms2g -Xmx2g Create Following Files Under /etc/logstash/conf.d/ Directory. vim /etc/logstash/conf.d/auditbeat.conf\n### INPUT SECTION ### ### This section make Logstash to listen on port 5044 for incoming logs \u0026amp; provides SSL certificate for secure connection. input { beats { port =\u0026gt; 5044 # ssl =\u0026gt; true # ssl_certificate =\u0026gt; \u0026#34;/etc/pki/tls/certs/logstash-forwarder.crt\u0026#34; # ssl_key =\u0026gt; \u0026#34;/etc/pki/tls/private/logstash-forwarder.key\u0026#34; } } ### FILTER SECTION ### ### This section parse the logs before sending them to Elasticsearch. filter { if [type] == \u0026#34;syslog\u0026#34; { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{SYSLOGLINE}\u0026#34; } } date { match =\u0026gt; [ \u0026#34;timestamp\u0026#34;, \u0026#34;MMM d HH:mm:ss\u0026#34;, \u0026#34;MMM dd HH:mm:ss\u0026#34; ] } } } ### OUTPUT SECTION ### ### This section defines the storage for the logs to be stored. output { elasticsearch { hosts =\u0026gt; [\u0026#34;http://192.168.10.10:9200\u0026#34;] manage_template =\u0026gt; false index =\u0026gt; \u0026#34;%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.DD}\u0026#34; document_type =\u0026gt; \u0026#34;%{[@metadata][type]}\u0026#34; } } FIREWALL CONFIGURATION Allow traffic through the TCP port 5044 in the firewall.\nfirewall-cmd --permanent --add-port=5044/tcp firewall-cmd --reload ENABLE \u0026amp; START LOGSTASH SERVICE systemctl enable logstash.service systemctl restart logstash.service systemctl status -l logstash.service STEP 5: INSTALL AND CONFIGURE KIBANA IMPORT PUBLIC GPG KEY TO THE ELK-STACK SERVER rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch CREATE YUM REPO FILE FOR KIBANA vim /etc/yum.repos.d/kibana.repo\n[kibana-6.x] name=Kibana repository for 6.x packages baseurl=https://artifacts.elastic.co/packages/6.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md INSTALL KIBANA YUM PACKAGES sudo yum install kibana CONFIGURE KIBANA vim /etc/kibana/kibana.yml\nserver.port: 5601 server.host: \u0026#34;0.0.0.0\u0026#34; server.name: \u0026#34;elk\u0026#34; elasticsearch.hosts: [\u0026#34;http://192.168.10.10:9200\u0026#34;] FIREWALL CONFIGURATION Allow traffic through the TCP port 5044 in the firewall.\nfirewall-cmd --permanent --add-port=5601/tcp firewall-cmd --reload ENABLE \u0026amp; START LOGSTASH SERVICE systemctl daemon-reload systemctl enable kibana.service systemctl restart kibana.service STEP 6: INSTALL AND CONFIGURE NGINX INSATLL EPEL REPOSITORY yum install epel-release INSTALL NGINX \u0026amp; HTTPD-TOOLS yum install nginx httpd-tools CREATE USERNAME “ADMIN” AND PASSWORD “123456” FOR KIBANA WEB INTERFACE\nhtpasswd -c /etc/nginx/htpasswd.kibana admin CONFIGURE NGINX Edit the Nginx configuration file and remove the ‘server { }’ block, so we can add a new virtual host configuration.\nvim /etc/nginx/nginx.conf\nCOMMENT {Server} Block:\nCreate new virtual host configuration file named “kibana.conf” under the conf.d directory.\nCreate VHOST FOr KIBANA: vim /etc/nginx/conf.d/kibana.conf\nserver { listen 80; server_name elk-stack.co; auth_basic \u0026#34;Restricted Access\u0026#34;; auth_basic_user_file /etc/nginx/htpasswd.kibana; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Check Nginx Configuration nginx -t FIREWALL CONFIGURATION Allow traffic through the TCP port 80 in the firewall.\nfirewall-cmd --permanent --add-port=80/tcp firewall-cmd --permanent --add-service=http firewall-cmd --reload ENABLE \u0026amp; START NGINX SERVICE systemctl enable nginx.service systemctl restart nginx.service SELINUX CONFGURATION setsebool -P httpd_can_network_connect 1 STEP 07: Connect Kibana Frontend With Elasticsearch You need assign Kibana to which Elasticsearch indeces you want yo explore. Configure the Elasticsearch Indices what you want to access with Kibana.\nOpen Web Browser and Point To\u0026hellip; (Only via Kibana) http://YOURIP.com:5601\nOR\n(If nginx/apache proxy redirect with VHOST) http://YOURIP.com:80\nNavigate To\nManagemnt \u0026gt;\u0026gt; Kibana \u0026gt;\u0026gt; Create Index Pattern\nhttps://digitalavenue.dev//img/post-imgs/elk-stack68/6_N.jpg does not exist\rNow Navigate To\nDiscover \u0026gt;\u0026gt; (Now You Search For Logs By Available Fields)\nBottom Line: Hope you guys get some idea about how to install ELK Stack on CentOS7 step by step. And monitor system logs in a effective manner. In the next lesson I will teach you following points. How to Install Beat Log Collectors on Client Systems. How to Search Log Data and Narrow down them into your Requirement How To Save Search Data How To Visualize Data How To Create Dashboards How To Use Dev Tools Log Rotate Effectively Optimize Logstash (Increase Index Search Performance \u0026amp; Lower Hardware Requirements) Video Tutorial on YouTube Will Be Available Soon.\n","date":"July 3, 2019","image":"https://digitalavenue.dev/img/post-imgs/elk-stack/ELK_huebddfe80cefaa5dacd0646e45c091998_46954_650x0_resize_box_3.png","permalink":"/blog/2019-07-03-install__configure_elk_stack_7_on_centos7/","title":"Install and Configure ELK Stack 6.8 On CentOS7"},{"categories":["Monitoring"],"contents":" Introduction: Zabbix is an open source monitoring solution which allows you to real-time monitor millions of matrices collected from thousands of servers, virtual machines and network devices.\nZabbix able to collect matrices from any devices, systems and applications using Zabbix agent, SNMP protocol and IPMI agents. Which can monitor Services, Hardware, Virtual Machines, Databases and Applications. Zabbix provides web interface with widget-based dashboards, graphs, network maps, slide shows.\nZabbix is a powerful open source monitoring solution used to monitor server applications, systems, Network devices, Hardware appliances, IoT devices.\nZabbix Server \u0026amp; Client Architecture The server communicates with the native software agents which available for Linux, Windows operating systems.\nZabbix can communicate without an agent via Simple Network Management Protocol (SNMP) or Intelligent Platform Management Interface (IPMI)\nZabbix Server Depends on 3 Software Applications Apache Web Server\nPHP with required extensions\nMySQL/MariaDB Database Server\nEnvironment:\nHostname = zabbix.digitalavenue.com IP Address = 192.168.100.150 OS = CentOS 7 / RHEL 7\nSTEP 1: Configure Prerequisites 1. Install EPEL Release\n[root@zabbix ~]# yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 2. Update an Operating System\n3. Set Resolvable Hostname\n4. Set SELINUX to Permissive Mode\n[root@zabbix /]# vim /etc/selinux/config SELINUX=permissive [root@zabbix /]# reboot STEP 02: Install Apache HTTPD Web Server Install “httpd” yum package\n[root@zabbix ~]# yum -y install httpd Start and enable apache service on reboot\n[root@zabbix ~]# systemctl enable httpd.service [root@zabbix ~]# systemctl start httpd.service Allow apache service through firewall\n[root@zabbix ~]# firewall-cmd --permanent --add-service=http [root@zabbix ~]# firewall-cmd --permanent --add-service=https [root@zabbix ~]# firewall-cmd --reload Test apache web service is up and running HTTP will start to listen on port 80\n[root@zabbix ~]# netstat -tulpen | grep httpd Access webpage through a web browser\nhttp://your_server_ip_address/\nSTEP 03: Install and Configure PHP Install PHP Dependencies for Zabbix Server\nPHP used to gather matrices from MariaDB/Mysql database and process to display dynamic content via apache web server.\nsudo yum -y install php php-pear php-cgi php-common php-mbstring php-snmp php-gd php-xml php-mysql php-gettext php-bcmath Configure PHP:\n[root@zabbix ~]# vim /etc/php.ini max_execution_time = 600 max_input_time = 600 memory_limit = 1024M post_max_size = 32M upload_max_filesize = 32M date.timezone = Asia/Colombo [root@zabbix ~]# vim /var/www/html/info.php \u0026lt;?php phpinfo(); ?\u0026gt; [root@zabbix ~]# systemctl restart httpd.service Open web browser and access following web page. And you should see page like below.\nhttp://your_server_IP_adress/info.php\nSTEP 04: Install MariaDB Database Server Add MariaDB YUM repository to CentOS 7 server\nGenarete Repositery using this URL:\nhttps://downloads.mariadb.org/mariadb/repositories/#mirror=truenetwork\u0026distro=CentOS\u0026distro_release=centos7-amd64--centos7\u0026version=10.3 # MariaDB 10.3 CentOS repository list - created 2019-04-01 15:07 UTC # http://downloads.mariadb.org/mariadb/repositories/ [mariadb] name = MariaDB baseurl = http://yum.mariadb.org/10.3/centos7-amd64 gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB gpgcheck=1 Run Following Commands optinally\u0026hellip;\nInstall MariaDB\nsudo yum -y install MariaDB-server MariaDB-client Start and enable Mariadb/MySQL service on reboot.\n[root@zabbix ~]# systemctl start mariadb.service [root@zabbix ~]# systemctl enable mariadb.service Allow MariaDB/MySQL service through firewall.\n[root@zabbix ~]# firewall-cmd --permanent --add-service=mysql [root@zabbix ~]# firewall-cmd –reload MariaDB Initial configuration:\n[root@zabbix ~]# mysql_secure_installation\n`Set root password? [Y/n] Y\nNew password:\nRe-enter new password:\nRemove anonymous users? [Y/n] Y\nDisallow root login remotely? [Y/n] Y\nReload privilege tables now? [Y/n] Y`\nLog into MariaDB Server:\n[root@zabbix ~]# mysql -uroot -p MariaDB [(none)]\u0026gt; select version(); +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | version() | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | 10.4.3-MariaDB |\n[root@zabbix ~]# mysql -V mysql Ver 15.1 Distrib 10.4.3-MariaDB, for Linux (x86_64) using readline 5.1 STEP 05: Create \u0026ldquo;zabbixdb\u0026rdquo; Database and \u0026ldquo;zabbixuser\u0026rdquo; User Create a Zabbix user and Zabbix database for Zabbix Installation. In this case I have used “zabbixuser” as a user and “zabbixdb” as a database.\n[root@zabbix ~]# mysql -uroot -p Enter password: MariaDB [(none)]\u0026gt; CREATE DATABASE zabbixdb CHARACTER SET UTF8 COLLATE UTF8_BIN; MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON zabbixdb.* to zabbixuser@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;PASSWORD\u0026#39;; MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON zabbixdb.* to zabbixuser@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;PASSWORD\u0026#39;; MariaDB [(none)]\u0026gt; FLUSH PRIVILEGES; MariaDB [(none)]\u0026gt; QUIT; STEP 06: INSTALL ZABBIX 4.0 Import GPG-KEY\nrpm --import http://repo.zabbix.com/RPM-GPG-KEY-ZABBIX\n**Install zabbix-release repository** rpm -ivh https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm Install Zabbix Server\nyum install zabbix-server-mysql zabbix-web-mysql zabbix-agent zabbix-get zabbix-sender zabbix-java-gateway -y Import Zabbix Server Databas Schema\n[root@zabbix ~]# zcat /usr/share/doc/zabbix-server-mysql-4.0.5/create.sql.gz | mysql -uzabbixuser -p zabbixdb This may take while to import the database schema. Please wait\u0026hellip;.\nSTEP 07: CONFIGURE ZABBIX SERVER Configure Zabbix Server Main Configuration File\n[root@zabbix ~]# vim /etc/zabbix/zabbix_server.conf ## DATABASE NAME DBName=zabbixdb ## DATABASE USERNAME DBUser=zabbixuser ## DATABASE PASSWORD DBPassword=PASSWORD sudo systemctl restart httpd zabbix-server sudo systemctl enable zabbix-server SET ZABBIX SERVER NAME:\n[root@zabbix ~]# vim /etc/httpd/conf/httpd.conf ServerName zabbix.digitalavenue.com:80 ServerAdmin zabbix@digitalavenue.com [root@zabbix ~]# systemctl restart httpd.service CONFIGURE ZABBIX FRONTEND:\nApache configuration file forZabbix server located in\n/etc/httpd/conf.d/zabbix.conf\nSome PHP settings have to be configured.\n[root@zabbix ~]# vim /etc/httpd/conf.d/zabbix.conf php_value max_execution_time 600 php_value memory_limit 1024M php_value post_max_size 32M php_value upload_max_filesize 32M php_value max_input_time 600 php_value max_input_vars 10000 php_value always_populate_raw_post_data -1 php_value date.timezone Asia/Colombo Allow MariaDB/MySQL service through firewall.\n[root@zabbix ~]# firewall-cmd --permanent --add-port={10050/tcp,10051/tcp} [root@zabbix ~]# firewall-cmd --reload [root@zabbix ~]# systemctl restart httpd.service Now, Zabbix Server Installation completed. Move onto you web browser and access Zabbix frontend and follow the on display guidelines.\nAccess Zabbix URL using web browser. And follow the instruction as seen on installation wizard.\nhttp://zabbix_server_IP/zabbix/setup.php\nDefault Zabbix Username: Admin\nDefault Zabbix Password: zabbix\nProvide the database name and username.\nLittle Request:\nI appreciate you guys taking the time in reading my post. Please check out my YouTube channel and please subscribe for more as it\u0026rsquo;ll help me loads.\n","date":"March 20, 2019","image":"https://digitalavenue.dev/img/post-imgs/zabbix/zabbix_N_huf533bad23e3d54432403286ad6fb974b_170843_650x0_resize_q90_box.jpg","permalink":"/blog/2019-03-20-how-to-install-zabbix-server-4.0-on-centos-7/","title":"How to Install Zabbix Server 4.0 on CentOS 7"},{"categories":["Monitoring"],"contents":"\rINSTALL AND CONFIGURE LibreNMS SERVER ON CENTOS 7 What is LibreNMS ? LibreNMS is an opensource monitoring tool for servers and network hardware. Based on Nginx or Apache, MySQL, and PHP. LibreNMS can discover network using UDP, FDP, LLDP, OSPF, BGP, SNMP and ARP portocols.Collects monitoring metrices via SNMP protocol.\nPrerequisite:- 1.Root login\n2.CenstOS 7\n3.EPEL repository\nSTEPS STEP 1:-Install Required Packages.\nSTEP 2:-Install NginX Web Server.\nSTEP 3:-Install and Configure MariaDB.\nSTEP 4:-Download and Configure LibreNMS.\nSTEP 5:-LibreNMS Web-based Installation.\nSTEP 6:-Firewall and SELinux Configuration.\nSTEP 7:-Install FPing and SNMP.\nSTEP 8:-Set Permission.\nSTEP 9:-LibreNMS Web Installation.\nSTEP 1:- Install Required Packages Configure EPEL Repository.\n[root@LibreNMS ~]# rpm -Uvh https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm Configure Webtatic Repository.\n[root@LibreNMS ~]# rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm Install some packages including ImageMagic, rrdtool, git, snmp, nmap, fping and more.\n[root@LibreNMS ~]# yum install -y cronie fping git ImageMagick jwhois mtr net-snmp net-snmp-utils nmap rrdtool MySQL-python python-memcached STEP 2:- Install Nginx Web Server [root@LibreNMS ~]# yum install -y nginx [root@LibreNMS ~]# systemctl enable nginx [root@LibreNMS ~]# systemctl start nginx STEP 3:- Install and Configure PHP-FPM Install PHP-FPM version 7 for CentOS7 from Webtatic repository. Webtatic repository has been installed on previous step.\n[root@LibreNMS ~]# yum install -y composer php72w php72w-cli php72w-common php72w-curl php72w-fpm php72w-gd php72w-mbstring php72w-mysqlnd php72w-process php72w-snmp php72w-xml php72w-zip Update the PEAR (PHP Extension and Application Repository) repository.\n[root@LibreNMS ~]# yum install php72w-pear.noarch [root@LibreNMS ~]# pear channel-update pear.php.net [root@LibreNMS ~]# pear install Net_IPv4-1.3.4 [root@LibreNMS ~]# pear install Net_IPv6-1.2.2b2 Set default timezone in the php.ini file. un-comment and edit look like below.\ndate.timezone = Asia/Colombo\ncgi.fix_pathinfo=0\nNow, Set PHP-FPM to running under the ‘sock’ file instead of the server port.\n[root@LibreNMS ~]# vim /etc/php-fpm.d/www.conf Change ‘listen’ port line to the sock file below.\ntouch /var/run/php-fpm/php7.2-fpm.sock chmod 777 /var/run/php-fpm/php7.2-fpm.sock listen = /var/run/php-fpm/php7.2-fpm.sock Ucomment the ‘listen’ owner, group and the permission of the sock file.\nlisten.owner = nginx listen.group = nginx listen.mode = 0660 [root@LibreNMS ~]# systemctl start php-fpm.service [root@LibreNMS ~]# systemctl enable php-fpm.service Now, PHP-FPM should running under the sock file.\n[root@LibreNMS ~]# netstat -pl | grep php STEP 3:- Install and Configure MariaDB Install MariaDB\n[root@LibreNMS ~]# yum install -y mariadb mariadb-server [root@LibreNMS ~]# systemctl start mariadb.service [root@LibreNMS ~]# systemctl enable mariadb.service Set root password? [Y/n] Y\nRemove anonymous users? [Y/n] Y\nDisallow root login remotely? [Y/n] Y\nRemove test database and access to it? [Y/n] Y\nReload privilege tables now? [Y/n] Y\nCreate a new database and a user for LibreNMS.\nCreate database named “librenms” and a user named “librenms” with password **\n[root@LibreNMS ~]# mysql -u root –p MariaDB [(none)]\u0026gt; CREATE DATABASE librenms CHARACTER SET utf8 COLLATE utf8_unicode_ci; MariaDB [(none)]\u0026gt; CREATE USER \u0026#39;librenms\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;\u0026lt;PASSWORD\u0026gt;\u0026#39;; MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON librenms.* TO \u0026#39;librenms\u0026#39;@\u0026#39;localhost\u0026#39;; MariaDB [(none)]\u0026gt; FLUSH PRIVILEGES; MariaDB [(none)]\u0026gt; quit; Edit /etc/my.cnf file and add new configuration.\nPaste below additional configuration under the ‘[mysqld]’ section.\ninnodb_file_per_table=1\nsql-mode=””\nlower_case_table_names=0\n[root@LibreNMS ~]# systemctl restart mariadb.service STEP 4:- Download and Configure LibreNMS Create new system user named ‘librenms’ and define the home directory under ‘/opt/librenms’ and add ‘librenms’ user to ‘nginx’ group.\n[root@LibreNMS ~]# useradd librenms -d /opt/librenms -M -r [root@LibreNMS ~]# usermod -a -G librenms nginx Clone LibreNMS into /opt/librenms directory.\n[root@LibreNMS opt]# cd /opt/ [root@LibreNMS opt]# git clone https://github.com/librenms/librenms.git librenms This Step is optional…\nCreate new directories for LibreNMS logs and rrd files.\n[root@LibreNMS opt]# mkdir -p /opt/librenms/{logs,rrd} [root@LibreNMS opt]# chmod 755 /opt/librenms/rrd/ Change ownership of all files and directories under ‘/opt/librenms’ to the ‘librenms’ user and group.\n[root@LibreNMS opt]# chown -R librenms:librenms /opt/librenms/ Configure LibreNMS virtual host\nLibreNMS is a Web-based application and we using Nginx web server to host it. Create a new virtual host file user ‘librenms.conf’ under Nginx ‘conf.d’ directory.\n[root@LibreNMS ~]# vim /etc/nginx/conf.d/librenms.conf Paste configuration below.\nserver { listen 80; server_name librenms.orelit.com; server_name 192.168.100.10; root /opt/librenms/html; index index.php; charset utf-8; gzip on; gzip_types text/css application/javascript text/javascript application/x-javascript image/svg+xml text/plain text/xsd text/xsl text/xml image/x-icon; location / { try_files $uri $uri/ /index.php?$query_string; } location /api/v0 { try_files $uri $uri/ /api_v0.php?$query_string; } location ~ \\.php { include fastcgi.conf; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php-fpm/php7.2-fpm.sock; } location ~ /\\.ht { deny all; } } ","date":"March 18, 2019","image":"https://digitalavenue.dev/img/post-imgs/librenms/LibreNMS_hu890c5bcb69812436a38dd120abdd3172_30880_650x0_resize_box_3.png","permalink":"/blog/2019-03-18-install_and_configure_librenms_server_on_centos_7/","title":"INSTALL AND CONFIGURE LibreNMS SERVER ON CENTOS 7"},{"categories":["DevOps"],"contents":" “ELK” is the acronym for the three open source projects call Elasticsearch, Logstash and Kibana. ELK stack made easier to analyze logs to system administrators. ELK stack collect logs from clients using Beats protocol.\nELK Stack Main Components:- Elasticsearch is an open source, distributed, RESTful, JSON based search and analytic engine. Easy to use and flexible. Elasticsearch is the heart of ELK stack. Elasticsearch is a No-SQL database.\nLogstash is a open source, server-side data processing pipeline that pull events data from multitude of sources simultaneously, transform it, and then sends it to Elasticsearch. Easily pull data from logs, metrics, web applications, data sources and various AWS services. Logstash dynamically transforms and prepares data regardless of format or complexity. Derive structure from unstructured data with grock.\nKibana provides GUI for users visualize data with charts and graphs real-time. It is a window into the Elastic Stack. Provides data exploration, visualization and dashboarding.\nBeats is the platform for single-purpose data shippers. They install as lightweight agents and send data from numerous machines to Logstash of Elasticsearch.\nFileBeat will send logs to Logstash, Logstash process incoming logs and stores into Elasticsearch, and then we can visualize through the Kibana web interface.\nELK Stack Architecture Software Versions I have used in this tutorial.\nELK Stack Server 192.168.100.10 (CentOS7) Beat Client 192.168.100.50 (CentOS7) Elasticsearch Version: 6.3.1 Logstash Version 6.3.1 Kibana Version 6.3.1\nSTEP 1 - Install Java 8 Java is required for the Elastic stack deployment. Elasticsearch requires Java 8, it is recommended to use the Oracle JDK 1.8. I will install Java 8 from the official Oracle rpm package. ELK requires the Oracle Java JDK package has to be installed. The same JVM version should be installed on all Elasticsearch nodes and clients.\nInstall JDK RPM [root@elk-stack /]# rpm -ivh jdk-8u171-linux-x64.rpm Set Default JAVA Version [root@elk-stack /]# alternatives --config java Set JAVAC and JAR Paths [root@elk-stack /]# alternatives --set jar /usr/java/jdk1.8.0_171-amd64/bin/jar [root@elk-stack /]# alternatives --set javac /usr/java/jdk1.8.0_171-amd64/bin/javac Check JAVA Version [root@elk-stack /]# java -version Set JAVA Environment Variables [root@elk-stack /]# export JAVA_HOME=/usr/java/jdk1.8.0_171-amd64/ [root@elk-stack /]# export JRE_HOME=/usr/java/jdk1.8.0_171-amd64/jre/ [root@elk-stack /]# export PATH=$PATH:/usr/java/jdk1.8.0_171-amd64/bin/:/usr/java/jdk1.8.0_171-amd64/jre/bin/ Set Environment Variables on /etc/bashrc to Auto Set at System Boot\n[root@elk-stack /]# vim ~/.bashrc JAVA_HOME=/usr/java/jdk1.8.0_171-amd64/ JRE_HOME=/usr/java/jdk1.8.0_171-amd64/jre/ PATH=$PATH:/usr/java/jdk1.8.0_171-amd64/bin/:/usr/java/jdk1.8.0_171-amd64/jre/bin/ STEP 2 – INSTALL AND CONFIGURE ELASTICSEARCH In this step, I will install and configure Elasticsearch version 6.3.1\nA. Import Public GPG Key to the ELK-Stack Server [root@elk-stack /]# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch B. Download Elasticsearch RPM Package [root@elk-stack /]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.1.rpm C. Install Elasticsearch RPM Package [root@elk-stack /]# rpm -ivh elasticsearch-6.3.1.rpm D. Firewall Configuration Allow traffic through the TCP port 9200 in the firewall.\n[root@elk-stack /]# firewall-cmd --permanent --add-port=9200/tcp [root@elk-stack ~]# firewall-cmd --permanent --add-port=9300/tcp [root@elk-stack /]# firewall-cmd –reload E. Configure Elasticsearch Do the following changes\n[root@elk-stack /]# vim /etc/elasticsearch/elasticsearch.yml cluster.name: my-application node.name: node-1 path.data: /var/lib/elasticsearch path.logs: /var/log/elasticsearch network.host: 127.0.0.1 #network.host: 192.168.100.10 http.port: 9200 F. Start \u0026amp; Enable Elasticsearch at System Boot [root@elk-stack /]# systemctl start elasticsearch.service [root@elk-stack /]# systemctl enable elasticsearch.service G. Test Elasticsearch Check Elasticsearch port “9200” state as “LISTEN”\n[root@elk-stack /]# netstat -plntu [root@elk-stack /]# curl -XGET \u0026#39;192.168.100.10:9200/?pretty\u0026#39; Output from the external web browser\nSTEP 3 – INSTALL AND CONFIGURE LOGSTASH In this step I will install Logstash version 6.3.1 and configure it as a central log server, receives logs from clients with Filebeat, then filter and transform the syslog data and move it into the stash (Elasticsearch)\nA. Import Public GPG Key to the ELK-Stack Server\n[root@elk-stack /]# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch B. Download Logstash RPM Package [root@elk-stack /]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.3.1.rpm C. Install Logstash RPM Package [root@elk-stack /]# rpm -ivh logstash-6.3.1.rpm D. Firewall Configuration Allow traffic through the TCP port 5044 in the firewall.\n[root@elk-stack /]# firewall-cmd --permanent --add-port=5044/tcp [root@elk-stack /]# firewall-cmd --reload E. Generate a New SSL Certificate Create new ssl certificate for securing communication between Logstash \u0026amp; Filebeat (clients). SSL Certificate file use clients to identify the elastic server.\nDo the following changes under the “[ V3_ca ]” section for the server identification.\n[ v3_ca ] #Server IP Address subjectAltName = IP: 192.168.100.10 Generate the certificate file with the openssl command.\n[root@elk-stack /]# openssl req -config /etc/pki/tls/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout /etc/pki/tls/private/logstash-forwarder.key -out /etc/pki/tls/certs/logstash-forwarder.crt Once ssl certificate is ready, this certificate should be copied to all the clients using scp command.\nF. Configure Logstash Create new configuration file named “logstash.conf” under the “/etc/logstash/conf.d” directory.\n[root@elk-stack /]# vim /etc/logstash/conf.d/logstash.conf There are three sections in “logstash.conf” file.\n1. Input Section This section make Logstash to listen on port 5044 for incoming logs \u0026amp; provides SSL certificate for secure connection.\n# input section input { beats { port =\u0026gt; 5044 ssl =\u0026gt; true ssl_certificate =\u0026gt; \u0026#34;/etc/pki/tls/certs/logstash-forwarder.crt\u0026#34; ssl_key =\u0026gt; \u0026#34;/etc/pki/tls/private/logstash-forwarder.key\u0026#34; congestion_threshold =\u0026gt; \u0026#34;40\u0026#34; } } 2. Filter Section This section parse the logs before sending them to Elasticsearch.\n# Filter section filter { if [type] == \u0026#34;syslog\u0026#34; { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{SYSLOGLINE}\u0026#34; } } date { match =\u0026gt; [ \u0026#34;timestamp\u0026#34;, \u0026#34;MMM d HH:mm:ss\u0026#34;, \u0026#34;MMM dd HH:mm:ss\u0026#34; ] } } } 3. Output Section This section defines the storage for the logs to be stored.\n# output section output { elasticsearch { hosts =\u0026gt; localhost index =\u0026gt; \u0026#34;%{[@metadata][beat]}-%{+YYYY.MM.dd}\u0026#34; } stdout { codec =\u0026gt; rubydebug } } G. Enable \u0026amp; Start Logstash Service [root@elk-stack /]# systemctl daemon-reload [root@elk-stack /]# systemctl enable logstash.service [root@elk-stack /]# systemctl start logstash.service STEP 4 – INSTALL AND CONFIGURE KIBANA A. Import Public GPG Key to the ELK-Stack Server [root@elk-stack /]# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch B. Download Logstash RPM Package [root@elk-stack /]# rpm -ivh kibana-6.3.1-x86_64.rpm C. Firewall Configuration Allow traffic through the TCP port 5601 in the firewall.\n[root@elk-stack /]# firewall-cmd --permanent --add-port=5601/tcp [root@elk-stack /]# firewall-cmd –reload D. Configure Kibana [root@elk-stack /]# vim /etc/kibana/kibana.yml server.port: 5601 server.host: \u0026#34;127.0.0.1\u0026#34; server.name: \u0026#34;Kibana Monitor\u0026#34; elasticsearch.url: “http://localhost:9200” E. Enable \u0026amp; Start Logstash Service [root@elk-stack /]# systemctl daemon-reload [root@elk-stack /]# systemctl enable kibana.service [root@elk-stack /]# systemctl start kibana.service STEP 5 – INSTALL AND CONFIGURE Nginx A. Insatll EPEL Repository [root@elk-stack /]# yum -y install epel-release B. Install Nginx \u0026amp; httpd-tools [root@elk-stack /]# yum -y install nginx httpd-tools C. Create Username “admin” and Password “123456” for Kibana Web Interface [root@elk-stack /]# htpasswd -c /etc/nginx/htpasswd.users admin D. Configure Nginx Edit the Nginx configuration file and remove the ‘server { }’ block, so we can add a new virtual host configuration.\n[root@elk-stack /]# vim /etc/nginx/nginx.conf Create new virtual host configuration file named “kibana.conf” under the conf.d directory.\n[root@elk-stack /]# vim /etc/nginx/conf.d/kibana.conf server { listen 80; server_name elk-stack.co; auth_basic \u0026#34;Restricted Access\u0026#34;; auth_basic_user_file /etc/nginx/.kibana-user; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } Check Nginx Configuration\nE. Enable \u0026amp; Start Nginx Service [root@elk-stack /]# systemctl enable nginx.service [root@elk-stack /]# systemctl start nginx.service F. Firewall Configuration Allow traffic through the TCP port 80 in the firewall.\n[root@elk-stack ~]# firewall-cmd --permanent --add-port=80/tcp [root@elk-stack ~]# firewall-cmd --permanent --add-service=http [root@elk-stack ~]# firewall-cmd --reload G. SELinux Confguration [root@elk-stack kibana]# setsebool -P httpd_can_network_connect 1 STEP 6 – Install \u0026amp; Configure Filebeat on the Client A. Copy SSL Certificate from ELK Server to Client [root@elk-stack ~]# scp /etc/pki/tls/certs/logstash-forwarder.crt root@192.168.100.50:/etc/pki/tls/certs/ B. Import Public GPG Key to the ELK-Stack Server [root@elk-stack /]# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch C. Download \u0026amp; Install Filebeat RPM Package [root@cl1 /]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.1-x86_64.rpm [root@cl1 /]# rpm -ivh filebeat-6.3.1-x86_64.rpm Configure File Beat\n[root@cl1 /]#vim /etc/filebeat/filebeat.yml In the paths: section paths:\n/var/log/secure /var/log/messages OR\n/var/log/*.log\nBy default, Filebeat is using Elasticsearch as the output target by default. Disable Elasticsearch output by commenting the configuration.\n142 #-------------------------- Elasticsearch output ------------------------------ 143 #output.elasticsearch: 144 # Array of hosts to connect to. 145 # hosts: [\u0026#34;localhost:9200\u0026#34;] Now add the Logstash as the new output configuration and do the changes as show below.\n152 #----------------------------- Logstash output -------------------------------- 153 output.logstash: 154 # The Logstash hosts 155 hosts: [\u0026#34;localhost:5044\u0026#34;] 156 bulk_max_size: 1024 157 ssl.certificate_authorities: [\u0026#34;/etc/pki/tls/certs/logstash-forwarder.crt\u0026#34;] 158 template.name: \u0026#34;filebeat\u0026#34; 159 template.path: \u0026#34;filebeat.template.json\u0026#34; 160 template.overwrite: false After that configurations will look like below.\nfilebeat.inputs: - type: log enabled: false paths: - /var/log/*.log output.logstash: hosts: [\u0026#34;localhost:5044\u0026#34;] bulk_max_size: 1024 ssl.certificate_authorities: [\u0026#34;/etc/pki/tls/certs/logstash-forwarder.crt\u0026#34;] template.name: \u0026#34;filebeat\u0026#34; template.path: \u0026#34;filebeat.template.json\u0026#34; template.overwrite: false E. Enable \u0026amp; Start Filebeat Service [root@cl1 /]# systemctl enable filebeat.service [root@cl1 /]# systemctl start filebeat.service STEP 7 – Using Elastic Stack Login to Kibana dashboard using the browser with password that you used at Nginx configuration.\nNow ELK Initial Steps are completed. We\u0026rsquo;ll discuss regarding Log Monitoring in a next tutorial.\nLittle Request:\nI appreciate you guys taking the time in reading my post. Please check out my YouTube channel and please subscribe for more as it\u0026rsquo;ll help me loads.\n","date":"March 16, 2019","image":"https://digitalavenue.dev/img/post-imgs/elk-stack/ELK_huebddfe80cefaa5dacd0646e45c091998_46954_650x0_resize_box_3.png","permalink":"/blog/2019-03-16-install--configure-elk-stack-on-centos7/","title":"Install and Configure ELK Stack On CentOS7"},{"categories":["IT Security"],"contents":" How to install \u0026amp; Configure Captive Portal with FreeRadius with Pfsense Firewall Table of contents What is Captive Portal? STEP 1:- Install FreeRADIUS3 Package STEP 2:- Create Server certificate STEP 3:- Configure FreeRadius Server STEP 4:- Configure Captive Portal What is Captive Portal? Captive portal is use for authenticated users to grant internet access. Firewall automatically captive portal authentication login page which users must use their credentials to enter the portal. User can use Username/Password or a voucher code.\nThis setup is commonly used throughout the hospitality industries like Airports, Hotels, Restaurants and corporate environments.\nThe Captive Portal function in Pfsense securing a network by requiring a username and password via portal access web page.\nPfsense built-in user management, LADP, RADIUS can be used as an authentication server.\nIn this tutorial I’m using FreeRADIUS2 as an authentication server.\nSTEP 1:- Install FreeRADIUS3 Package Navigate to System \u0026gt; Package Manager, Available Packages tab\nClick at the end of the row for FreeRADIUS3\nConfirm the installation\nSystem \u0026gt; Package Manager \u0026gt; Available Packages\n[Search Item] = freeradius3\nSTEP 2:- Create Server certificate Navigate to System \u0026gt; Cert. Manager\nCreate a Certificate Authority (CA) Create a Server Certificate\nHow to create CA and Server certificate is available at… STEP 3:- Configure FreeRadius Server Navigate to System \u0026gt; FreeRadius, EAP Tab \u0026gt; “Certificates for TLS” section\nProvide CA and server certificate that we have generated at previous step.\nSave the changes.\nAdd a new interface on which the RADIUS server should listen on.\nNavigate to System \u0026gt; Services \u0026gt; FreeRADIUS, Interfaces tab\nClick button\nIn this case I’m using my LAN interface (192.168.100.1) for RADIUS server to listening on.\nSave and exit.\nConfigure the NAS: Configure the NAS/client(s) from which the RADIUS server should accept packets.\nIn this step you need to give the IP address of the device which you want to authenticate from radius server like a firewall, access point, switch and router.\nIn this step I give my Pfsense box’s IP address because I will use the Pfsense captive portal.\nClick “+” button to add the NAS/Clients. Client IP Address : 192.168.100.1\nClient Shortname: captiveportal\nClient Shared Secret: 12345\nReset of the settings can be leave default.\nhttps://digitalavenue.dev//img/post-imgs/pfsense_captive_portal/image0010.png does not exist\rSTEP 4:- Configure Captive Portal Navigate to Services \u0026gt; Captive Portal\nClick “\r“ button to add new zone.\nSTEP 04:- Create FreeRadius Users Navigate to Services \u0026gt; FreeRadius, Users tab.\nAll the other settings can be change upon to your requirements.\nSTEP 05:- Login to Captive Portal User Account ","date":"March 15, 2019","image":"https://digitalavenue.dev/img/post-imgs/pfsense_captive_portal/captive_portal_N_huf31e91f27752b310ad98ffb4cb5b8717_167416_650x0_resize_q90_box.jpg","permalink":"/blog/2019-03-15-install-and-configure-captive-portal-with-freeradius-on-pfsense/","title":"Install and Configure Captive Portal with FreeRADIUS on pfSense"},{"categories":["Monitoring"],"contents":"INSTALL AND CONFIGURE GrayLog2 SERVER ON CENTOS 7 Graylog is an open source log management tool. It can use for collect, index and analyze remote machine logs centrally.\nComponents: - MongoDB - Stores the configuration and meta information.\nElasticsearch - Store the log messages and offers searching facility which are coming from Graylog server. Elasticsearch does indexing of data.\nGraylog Server - Collect logs coming from various inputs and provide Web based interface to manage those logs.\nPre-requisites: - Elasticsearch is based on Java Install Oracle Java / OpenJDK\n[root@graylog /]# rpm -Uvh jdk-8u161-linux-x64.rpm Install Elasticsearch: - Elasticsearch is an open source tool. Which provides distributed search, indexing and analytics using RESTful web interface. Elasticsearch stores all the log sent by Graylog server inputs and displays the messages.\nDownload and install public singing key.\n[root@graylog /]# rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch [root@graylog /]# vim /etc/yum.repos.d/elasitcsearch.repo [elasticsearch-5.x] name=Elasticsearch repository for 5.x packages baseurl=https://artifacts.elastic.co/packages/5.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md [root@graylog /]# yum install -y elasticsearch [root@graylog /]# systemctl enable elasticsearch [root@graylog /]# systemctl start elasticsearch [root@graylog /]# systemctl daemon-reload Configure Elasticsearch: - Elasticsearch configuration files can be found in /etc/elasticsearch/ directory.\nlogging.yml – manages the logging of elasticsearch\nelasticsearch.yml – main configuration file\nLog files stores in /var/log/elasticsearch/\nBy default\nBind to all network interfaces 0.0.0.0 HTTP traffic Listen on port 9200 – 9300 Internal node to node communication on port 9300 – 9400\nDo the following changes to listen on specific IP.\n[root@graylog /]# vim /etc/elasticsearch/elasticsearch.yml network.host: 192.168.100.10 The cluster.name is used to discover and auto-join other nodes. Use unique cluster name to avoid auto-join with other Elasticsearch server clusters.\ncluster.name: graylog Disable dynamic scripts to avoid remote execution\nscript.inline: false script.indexed: false script.file: false [root@graylog /]# systemctl restart elasticsearch.service Elasticsearch now starts to listen on port 9200 for HTTP requests. Use this command to check whether it is working.\n[root@graylog /]# curl -X GET \u0026lsquo;http://192.168.100.10:9200\u0026rsquo;\nTo check the Elasticsearch server’s health. Status should be as “green” to work properly.\n[root@graylog ~]# curl -XGET \u0026#39;http://192.168.100.10:9200/_cluster/health?pretty=true\u0026#39; Install MongoDB: -\nCreate MongoDB yum repository.\n[root@graylog /]# vi /etc/yum.repos.d/mongodb-org-3.2.repo [mongodb-org-3.2] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.2/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.2.asc root@graylog ~]# yum install -y mongodb-org ` SELinux Configuration for MongoDB\nroot@graylog ~]# yum -y install policycoreutils-python SELinux to allow MongoDB to Start.\n[root@graylog ~]# semanage port -a -t mongod_port_t -p tcp 27017 Enable and Start MongoDB Service\n[root@graylog /]# systemctl enable mongod.service [root@graylog /]# systemctl start mongod.service Install Graylog2: - Graylog-server accepts and process the log messages receiving from various inputs and display data through Graylog web interface\n[root@graylog /]# rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-2.4-repository_latest.rpm [root@graylog /]# yum install graylog-server ","date":"March 15, 2019","image":"https://digitalavenue.dev/img/post-imgs/graylog_ins/graylog_hucae44d9910f05a50354fc2c965bf5a6c_13409_650x0_resize_box_3.png","permalink":"/blog/2019-03-15-install-and-configure-graylog2-on-centos7/","title":"INSTALL AND CONFIGURE GrayLog2 SERVER ON CENTOS 7"},{"categories":["IT Security"],"contents":" Install PfSense 2.4.4 on Virtual Box pfSense is network firewall based on FreeBSD operating system with a custom kernel and includes free third-party packages for additional features. It provides same functionality or more of common commercial firewalls. In this tutorial, I will install pfSense in VirtualBox it will work as a firewall for our virtual lab environment for future articles as well.\nSTEP 1:- Download Pfsense ISO Image. Download most recent stable release from https://www.pfsense.org/download/ STEP 2: - Install Pfsense on Virtual box A. Create a Virtual Machine for Pfsense\nFollow The Guideline as seen on GIF image.\nHere we have assigned two interfaces with one Bridge Interface (em0) and one Internal Interface Named (em1) LAN2. Assigned IP as follows…\nEm0 - WAN Interface - 192.168.1.100/24 Em1 – LAN Interface – 192.168.100.1/24\nDHCP Range – 192.168.100.100 – 192.168.100.200\nSTEP 2: - Logging in to Web Interface Pfsense default username and password is “admin” and “Pfsense”\nComplete Pfsense initial setup wizard.\n","date":"March 15, 2019","image":"https://digitalavenue.dev/img/post-imgs/pf-install/pfSense-Installation_N_hu201bf2e936c37c986369636be1e6642e_176039_650x0_resize_q90_box.jpg","permalink":"/blog/2019-03-15-install_pfsense_2.4.4_on_virtual_box/","title":"Install PfSense 2.4.4 on Virtual Box"},{"categories":["IT Security"],"contents":" Pfsense is a great firewall solution. Very reliable and comes with built in VLAN and VPN support. In this tutorial I’m going to demonstrate how to setup a user authenticated OpenVPN server in PfSense. In this guide I assume you already have a functional pfSense firewall running.\nSTEP 1: - Open OpenVPN Wizard A. Create a Virtual Machine for Pfsense\nSelect OpenVPN Authentication Backed Type\nIn this tutorial I have used “Local User Access” as the authenticated backed type.\nSTEP 2:- Create New CA Create a Certificate Authority to generate certificates for the OpenVPN server.\nFill out the following fields to create a new CA.\nSTEP 3:- Create Server Certificate Create a Server Certificate from the CA for OpenVPN.\nSTEP 4:- OpenVPN Genaral Settings Configuration In this case OpenVPN interface will listen on external facing WAN interface which is connected to the internet.\nInterface: WAN\nProtocol: UDP on IPv4 Only\nLocalPort: 1194\nDescription: VPN\nCryptographic Settings Configuration\nThis section can be left default or change it upon your security needs.\nSTEP 5:- OpenVPN Tunnel Configuration There are two important sections.\nTunnel Network\nThe tunnel networl should be a new network that does not currently exist on the network or the Pfsense firewall routing table.\nWhen client connect to the VPN they will receive an address in this network.\nEx: 172.25.0.10/24\nLocal Network\nEnter the network address of that client will connect to local network. Network address that Pfsense box resides.\nRest of the settings can be change according to your requirement.\nSTEP 6:- OpenVPN Client Settings The settings in the client settings section will be assigned to OpenVPN clients when they connect to the network.\nIf you are also using pfSense as your local DNS server, you would enter them here. Separate DNS servers also can enter here.\nOptionally DNS, NTP server can be provided to the VPN clients from here.\nSTEP 7:- Firewall Rule creation for OpnVPN Traffic from client to server: - If this section enabled, OpenVPN wizard will automatically generate the necessary firewall rules to permit the incoming connection to Pfsense OpenVPN server from clients anywhere on the internet.\nTraffic from clients through VPN:- If this connection enabled, OpenVPN wizard will automatically generate firewall rules which allow traffic from clients connected to the VPN to anywhere on the local network.\nFinally finish the wizard.\nSTEP 8:- Create VPN Users with Certificates If you selected the “local user access” option during the VPN wizard then users can be added through the pfSense user manger.\nCreate new user.\nSTEP 9:- Install OpnVPN Client Export Package Install OpenVPN Client Export package using Pfsense package manager.\nAfter the installation there will be a new tab named with “Client Export” in OpenVPN menu.\nModify “Hostname Resolution” field. By default this is set to the IP address of the interface running OpenVPN.\nAfter any changes made, click the “Save as default” button to store the settings.\nSTEP 10:- Download the OpenVPN Client Packages. Download and install OpenVPN client application.\nhttps://openvpn.net/index.php/open-source/downloads.html https://swupdate.openvpn.org/community/releases/openvpn-install-2.4.6-I602.exe Install downloaded OpenVPN profile.\n","date":"March 15, 2019","image":"https://digitalavenue.dev/img/post-imgs/pf-vpn/pf-openvpn_N_hu8ebb7a6c4e65b8eaf668acb093f69fd4_173922_650x0_resize_q90_box.jpg","permalink":"/blog/2019-03-15-setup-remote-vpn-access-using-pfsense-and-openvpn/","title":"Setup Remote VPN Access Using PfSense and OpenVPN"},{"categories":["pfSense","Linux","IT Security"],"contents":" Hi Guys today I\u0026rsquo;m goning to demonstrate how to install and configure dhcp server and dns reslover on pfsense 2.4.\nSTEP 01: GENARAL CONFIGURATION Systemc \u0026gt; Genaral Setup\nGoto \u0026ldquo;System\u0026rdquo; tab and select \u0026ldquo;Genaral Setup\u0026rdquo; from the drop down menu.\nChange following fields as seen below.\nHostname : Define a meaningfull hostname for pfSense.\nDomain Name : Define your domain name which pfsense router used.\nDNS Server : Define public authoritative DNS servers for user pfSense itself.\nSTEP 02: SETUP DHCP SERVER Goto Services tab and select DHCP Server from the drop down menu.\nServices \u0026gt; DHCP Server\nSelect the interface which you want to DHCP server runing on.\nFill the relevent fields with \u0026ldquo;Subnet\u0026rdquo;, \u0026ldquo;Subnet Mask\u0026rdquo;, \u0026ldquo;Range\u0026rdquo;, \u0026ldquo;DNS Servers\u0026rdquo;, \u0026ldquo;Gateway\u0026rdquo; and \u0026ldquo;Domain Name\u0026rdquo;\nChange all other options according to your requirement.\nSTEP 02: SETUP DNS SERVER Unbound is integrated into pfSense. Unbound is use as the DNS server.\nGo to \u0026ldquo;Services\u0026rdquo; tab and select \u0026ldquo;DNS Resolver\u0026rdquo; from the drop down menu.\nServices \u0026gt; DNS Resolver\nEnable DNS Resolver: Enable/Disable DNS Resolver\nNetwork Interfaces: Network interfaces which are listening from DNS queries from clients.\nOutgoing Network Interfaces: Specific interfaces which are passing outbound DNS queries to higer level DNS server such as Google DNS. Most cases WAN interfaces used.\nEnable DNSSEC Support: DNSSEC provides added security to DNS queries which validate DNS queries.\nEnable Forwarding Mode: Unbound DNS queries forwarding to upstream DNS server which are defined under System \u0026gt; General\nRegister DHCP leases in the DNS Resolver: DHCP static mappings can be registered in Unbound which enables the resolving of hostnames that have been assigned addresses by the DHCP server in pfSense\nHost Overrides: Allows creation of custom DNS responses/records to create new entries that do not exist in DNS outside the firewall, or to override DNS responses for other hosts.\nDomain Override: For domains that should be queried by a specific remote server. At this point I want to redirect all DNS quest for .youtube.com and .facebook.com redirect to localhost/127.0.0.1 itself.\nSTEP 02: Verifing DNS Queries. ","date":"September 18, 2016","image":"https://digitalavenue.dev/img/post-imgs/pfsense-dns-dhcp/pf-DNS\u0026DHCP_hu7da9a1f29c86b6245ccd5cea61689352_945137_650x0_resize_box_3.png","permalink":"/blog/2019-09-18-configure_dns_dhcp_with_pfsense/","title":"Configure Local DHCP Server and DNS Resolver on pfSense"}]